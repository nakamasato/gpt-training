{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"GPT Training","text":""},{"location":"#project-layout","title":"Project layout","text":"<pre><code>experiment/ # experiment for new tools\n\u251c\u2500\u2500 gpt-engineer\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 README.md\nsrc/ # src for runnable scripts and apps\n\u251c\u2500\u2500 examples/ # one-time scripts\n\u251c\u2500\u2500 libs/ # common libs\n\u251c\u2500\u2500 projects/apps/ # runnable apps\ndocs/ # md files for mkdocs\n\u251c\u2500\u2500 index.md\nREADME.md\nmkdocs.yml\n</code></pre>"},{"location":"#versions","title":"Versions","text":"<pre><code>python 3.12.7\npoetry 1.8.3\n</code></pre>"},{"location":"#tools","title":"Tools","text":"<ol> <li> <p>asdf</p> <pre><code>asdf local python &lt;version&gt;\nasdf local poetry &lt;version&gt;\n</code></pre> </li> <li> <p>poetry</p> <pre><code>poetry install\n</code></pre> </li> </ol>"},{"location":"#environment-variables","title":"Environment Variables","text":".env<pre><code>OPENAI_API_KEY=\nOPENAI_ORGANIZATION=\nGOOGLE_CSE_ID=\nGOOGLE_API_KEY=\nSERPAPI_API_KEY=\nTAVILY_API_KEY=\n</code></pre> <pre><code>poetry add poetry-dotenv-plugin\n</code></pre>"},{"location":"#config","title":"Config","text":"pyproject.toml pyproject.toml<pre><code>[tool.poetry]\nname = \"gpt-training\"\nversion = \"0.1.0\"\ndescription = \"GPT Training\"\nauthors = [\"Masato Naka &lt;masatonaka1989@gmail.com&gt;\"]\nreadme = \"README.md\"\npackages = [{include = \"src\"}]\n\n[tool.poetry.dependencies]\npython = \"^3.12\"\nmkdocs-material = \"^9.5.3\"\nstreamlit = \"^1.30.0\"\nopenai = \"^2.0.0\"\nlangchain = \"^0.3.0\"\nlangchain-openai = \"^0.3.0\"\ngoogle-api-python-client = \"^2.115.0\"\nlangchain-google-genai = \"^2.0.0\"\npillow = \"^12.0.0\"\nlanggraph = \"^1.0.0\"\nwikipedia = \"^1.4.0\"\ngoogle-search-results = \"^2.4.2\"\ntabulate = \"^0.9.0\"\nlangchain-community = \"^0.3.0\"\nlangchain-experimental = \"^0.3.0\"\nlangchain-google-community = \"^2.0.0\"\nbrowser-use = \"^0.12.0\"\n\n\n[tool.poetry.group.dev.dependencies]\npytest = \"^9.0.0\"\npytest-cov = \"^7.0.0\"\npytest-env = \"^1.1.3\"\npoetry-dotenv-plugin = \"^0.2.0\"\nmkdocs-awesome-pages-plugin = \"^2.9.2\"\nruff = \"^0.15.0\"\nlangchain-cli = \"^0.0.37\"\n\n[tool.ruff]\nexclude = [\n    \".venv\",\n    \"venv\",\n    \"__pycache__\",\n    \".git\",\n]\n\nline-length = 200\nindent-width = 4\n\n\n[build-system]\nrequires = [\"poetry-core\"]\nbuild-backend = \"poetry.core.masonry.api\"\n\n# From https://github.com/googleapis/google-cloud-python/issues/11184\n# https://docs.pytest.org/en/stable/reference/customize.html\n[tool.pytest.ini_options]\ntestpaths = [\n    \"tests\",\n]\npythonpath = [\n    \"src\",\n]\naddopts = \"--junitxml=pytest.xml --cov-report=term-missing --cov=src tests/ -vvv -s\"\nenv = [\n    \"ENV=test\",\n    \"OPENAI_API_KEY=dummy\",\n    \"OPENAI_ORGANIZATION=dummy\",\n    \"GOOGLE_CSE_ID=dummy\",\n    \"GOOGLE_API_KEY=dummy\",\n]\n</code></pre>"},{"location":"#run","title":"Run","text":"<p>Run a script</p> <pre><code>poetry run python src/examples/agent_react.py\n</code></pre> <p>Run an app</p> <pre><code>poetry run streamlit run src/projects/apps/streamlit/app.py\n</code></pre>"},{"location":"#testing","title":"Testing","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>poetry add pytest pytest-cov pytest-env --group dev\n</code></pre>"},{"location":"#run_1","title":"Run","text":"<pre><code>poetry run pytest\n</code></pre>"},{"location":"#patch","title":"patch","text":"<p>Source code:</p> <pre><code>from langchain_community.utilities import GoogleSearchAPIWrapper\ngoogle = GoogleSearchAPIWrapper()\n</code></pre> <p>Test:</p> <pre><code>@patch(\"src.libs.tools.google\")\ndef test_xxx(mock_google):\n    google_mock.results.return_value = [\n        {\n            \"title\": \"\u5185\u95a3\u5236\u5ea6\u3068\u6b74\u4ee3\u5185\u95a3\",\n            \"link\": \"https://www.kantei.go.jp/jp/rekidai/ichiran.html\",\n            \"snippet\": \"(\u5916\u52d9\u5927\u81e3 \u5185\u7530\u5eb7\u54c9\u304c\u5185\u95a3\u7dcf\u7406\u5927\u81e3\u81e8\u6642\u517c\u4efb\uff09. 22, \uff08\u7b2c2\u6b21\uff09 \u5c71\u672c\u6b0a\u5175\u885e, \u5927\u6b6312.9.2 \uff0d\u5927\u6b6313.1.7, 128, 70\u6b73, \u5609\u6c385.10.15, \u662d\u548c8.12.8 (81\u6b73), \u9e7f\u5150\u5cf6\u770c, 549. 23(13)\\xa0...\",\n        },\n    ]\n</code></pre>"},{"location":"#fakelistllm","title":"FakeListLLM","text":"<p>langchain/testing.md</p>"},{"location":"references/","title":"References","text":""},{"location":"references/#rag","title":"RAG","text":"<ol> <li>\u30d9\u30af\u30c8\u30eb\u691c\u7d22\u3067\u6b32\u3057\u3044\u60c5\u5831\u304c\u5f97\u3089\u308c\u306a\u3044\u3068\u304d\u306e\u554f\u984c\u70b9\u3068\u6539\u826f\u65b9\u6cd5\u3092\u8003\u3048\u3066\u307f\u305f</li> <li>Enhancing LangChain\u2019s RetrievalQA for Real Source Links</li> <li>RAG\u306b\u304a\u3051\u308b\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u691c\u7d22\u7cbe\u5ea6\u5411\u4e0a\u306b\u3064\u3044\u3066(\u6982\u8981\u7de8)</li> <li>Best Practices for LLM Evaluation of RAG Applications</li> <li>RAG\u306e\u5b9f\u6848\u4ef6\u306b\u53d6\u308a\u7d44\u3093\u3067\u304d\u305f\u4eca\u307e\u3067\u306e\u77e5\u898b\u3092\u307e\u3068\u3081\u3066\u307f\u305f</li> </ol>"},{"location":"references/#langchain","title":"LangChain","text":"<ol> <li>Official<ol> <li>https://python.langchain.com/en/latest (Official)</li> <li>Twitter Finetune -&gt; https://elon-twitter-clone.streamlit.app/</li> <li>Streamlit Agent</li> <li>LCEL (LangChain Expression Language) sequential chain. e.g. <code>prompt | llm | output_parser</code></li> <li>Human as a tool</li> <li>Template &lt;- Must</li> </ol> </li> <li>Blog<ol> <li>LLM\u9023\u643a\u30a2\u30d7\u30ea\u306e\u958b\u767a\u3092\u652f\u63f4\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea LangChain \u306e\u4f7f\u3044\u65b9 (1) - LLM\u3068\u30d7\u30ed\u30f3\u30d7\u30c8\u30fb\u30c1\u30a7\u30fc\u30f3</li> <li>LLM\u9023\u643a\u30a2\u30d7\u30ea\u306e\u958b\u767a\u3092\u652f\u63f4\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea LangChain \u306e\u4f7f\u3044\u65b9 (2) - \u30c7\u30fc\u30bf\u62e1\u5f35\u751f\u6210</li> <li>LLM\u9023\u643a\u30a2\u30d7\u30ea\u306e\u958b\u767a\u3092\u652f\u63f4\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea LangChain \u306e\u4f7f\u3044\u65b9 (3) - \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8</li> <li>LLM\u9023\u643a\u30a2\u30d7\u30ea\u306e\u958b\u767a\u3092\u652f\u63f4\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea LangChain \u306e\u4f7f\u3044\u65b9 (4) - \u30e1\u30e2\u30ea</li> <li>LangChain \u306e HOW-TO EXAMPLES (1) - LLM</li> <li>LangChain \u306e HOW-TO EXAMPLES (2) - \u30d7\u30ed\u30f3\u30d7\u30c8</li> <li>LangChain \u306e HOW-TO EXAMPLES (3) - \u30c1\u30a7\u30fc\u30f3</li> <li>LangChain \u306e HOW-TO EXAMPLES (4) - \u30c7\u30fc\u30bf\u62e1\u5f35\u751f\u6210</li> <li>LangChain \u306e HOW-TO EXAMPLES (5) - \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8</li> <li>LangChain \u306e Google\u30ab\u30b9\u30bf\u30e0\u691c\u7d22 \u9023\u643a\u3092\u8a66\u3059</li> <li>MRKL(Modular Reasoning, Knowledge and Language)</li> <li>Build a Chatbot on Your CSV Data With LangChain and OpenAI</li> <li>LangChain Indexes\u3068\u306f\uff1f\u3010Document Loaders\u30fbText Splitters\u30fbVectorstores\u3011</li> <li>LangChain\u30ab\u30b9\u30bf\u30e0\u30c4\u30fc\u30eb\u3092\u4f5c\u3063\u3066AI Agent\u306b\u6e21\u3057\u3066\u307f\u305f &lt;- \u3081\u3061\u3083\u304f\u3061\u3083\u3044\u3044Example</li> <li>\u7b2c29\u56de LangChain \u3067\u30de\u30eb\u30c1\u30bf\u30fc\u30f3\uff06\u30bf\u30b9\u30af\u306e Agent \u3092\u4f5c\u308b</li> <li>Zod\u30b9\u30ad\u30fc\u30de\u3067\u30d7\u30ed\u30f3\u30d7\u30c8\u751f\u6210\u3092\u884c\u3044\u69cb\u9020\u5316\u30c7\u30fc\u30bf\u3092\u81ea\u7531\u81ea\u5728\u306b\u6271\u3048\u3066\u3001LLM\u30d7\u30ed\u30c0\u30af\u30c8\u958b\u767a\u304c\u5727\u5012\u7684\u306b\u52b9\u7387\u5316\u3057\u305f\u8a71</li> <li>LangChain <code>with_structured_output</code> \u30e1\u30bd\u30c3\u30c9\u306b\u3088\u308b\u69cb\u9020\u5316\u30c7\u30fc\u30bf\u62bd\u51fa</li> </ol> </li> </ol>"},{"location":"references/#slack-chatgpt","title":"Slack ChatGPT\u9023\u643a","text":"<ol> <li>https://tech.visasq.com/chatgpt-with-zapier</li> <li>https://qiita.com/Yuki_Oshima/items/112e69df63df9958709f</li> <li>https://techblog.gmo-ap.jp/2023/04/17/connecting-slack-app-with-chatgpt/</li> <li>https://dev.classmethod.jp/articles/slack-chat-gpt-bot/</li> </ol>"},{"location":"references/#vector-db","title":"Vector DB","text":"<ol> <li>\u3010LLM\u3011\u30d9\u30af\u30c8\u30eb\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3063\u3066\u591a\u304f\u3066\u3069\u308c\u3092\u4f7f\u3063\u305f\u3089\u826f\u3044\u304b\u5206\u304b\u3089\u306a\u3044\u3068\u3044\u3046\u3042\u306a\u305f\u306e\u305f\u3081\u306e\u8a18\u4e8b\uff086\u3064\u306e\u30c4\u30fc\u30eb\u3092\u6bd4\u8f03\uff09</li> </ol>"},{"location":"references/#app","title":"App","text":"<ol> <li>\u3064\u304f\u308a\u306a\u304c\u3089\u5b66\u3076\uff01AI\u30a2\u30d7\u30ea\u958b\u767a\u5165\u9580 - LangChain &amp; Streamlit \u306b\u3088\u308b ChatGPT API \u5fb9\u5e95\u6d3b\u7528</li> <li>\u30b5\u30af\u30c3\u3068\u59cb\u3081\u308b\u30d7\u30ed\u30f3\u30d7\u30c8\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\u3010LangChain / ChatGPT\u3011</li> <li>LangChain <code>with_structured_output</code> \u30e1\u30bd\u30c3\u30c9\u306b\u3088\u308b\u69cb\u9020\u5316\u30c7\u30fc\u30bf\u62bd\u51fa</li> <li>\u793e\u5185\u306e\u30c7\u30fc\u30bf\u30a2\u30d7\u30ea\u30b1\u30fc\u30b7\u30e7\u30f3\u3092Streamlit + Cloud Run\u3067\u5b9f\u73fe\u3059\u308b</li> <li>Streamlit with Google Cloud: Hello, world!</li> <li>Streamlit with Google Cloud: Firebase \u8a8d\u8a3c</li> <li>Streamlit with Google Cloud: GitHub\u3001GitLab\u3001Cloud Build \u3067\u306e CI/CD</li> <li>streamlit\u3067LangGraph\u306b\u3088\u308b\u81ea\u5df1\u4fee\u6b63RAG\u3092\u5b9f\u88c5\u3057\u3066\u307f\u3088\u3046\uff01</li> <li>LangGraph\u306b\u3088\u308b\u81ea\u5df1\u4fee\u6b63RAG\u3092\u5b9f\u884c\u3057\u3066\u307f\u3088\u3046\uff01</li> </ol>"},{"location":"references/#chatgpt","title":"ChatGPT","text":"<ol> <li>Account Blocked</li> </ol>"},{"location":"references/#others","title":"Others","text":"<ol> <li>gpt-index(0.2.5)\u3092OpenAI API\u306a\u3057\uff06\u65e5\u672c\u8a9e\u3067\u52d5\u304b\u3059</li> <li>\u30b5\u30af\u30c3\u3068\u59cb\u3081\u308b\u30d7\u30ed\u30f3\u30d7\u30c8\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\u3010LangChain / ChatGPT\u3011</li> <li>Streaming Generator with LangChain</li> <li>\u667a\u80fd\u9605\u8bfb\u52a9\u624b ReaderGPT \u5f00\u53d1\u8bb0\u5f55</li> <li>\u5927\u6a21\u578b\u65f6\u4ee3\uff0c\u7a0b\u5e8f\u5458\u5982\u4f55\u5b9e\u73b0\u81ea\u6211\u6210\u957f\uff1f\u2014\u2014\u4e00\u540d\u666e\u901a\u5f00\u53d1\u8005\u7684 ChatGPT \u4e00\u5468\u5e74\u8bb0</li> <li>LangChain\u6a21\u5757\u4e4b Chains</li> <li>LlamaIndex vs. Langchain: If you are building a general-purpose application that needs to be flexible and extensible, then Langchain is a good choice. If you are building a search and retrieval application that needs to be efficient and simple, then LlamaIndex is a better choice.</li> </ol>"},{"location":"references/#projects","title":"Projects","text":"<ol> <li>https://docs.llamaindex.ai/en/stable/</li> <li>CassIO: Cassandra for LLM (with LangChain, LlamaIndex, )</li> <li>Poetry<ol> <li>upgrade python version</li> </ol> </li> <li>pytest<ol> <li>pytest-env: environment variable</li> </ol> </li> <li>gpt copilot<ol> <li>https://github.com/Pythagora-io/gpt-pilot</li> <li>https://github.com/gpt-engineer-org/gpt-engineer</li> </ol> </li> <li>LangServe</li> <li>Greg\u306e\u8cea\u554f: Who's doing Slack as a knowledge base + LLMs?<ol> <li>Emerging Architectures for LLM Applications</li> <li>Ask Astro</li> </ol> </li> <li>https://topai.tools/s/Slack-knowledge-base-tool</li> <li>\u3010\u5229\u7528\u4e8b\u4f8b\u3011miibo\u3067\u4f5c\u308b\u4f1a\u8a71\u578bAI</li> <li>Langsearch</li> <li>Cloud Natural Language API (Bard API)</li> </ol>"},{"location":"langchain/01_twitter/","title":"LangChain: Twitter algorithm code","text":"<p>embeddings -&gt; DeepLake (Vector Store) -&gt; Question Answering</p>"},{"location":"langchain/01_twitter/#components","title":"Components","text":"<ol> <li>Splitter</li> <li>Embedding</li> <li>VectorStore<ol> <li>DeepLake</li> <li>Chroma</li> <li>Qdrant</li> </ol> </li> <li>Retriever</li> </ol>"},{"location":"langchain/01_twitter/#run","title":"Run","text":"<pre><code>poetry example --example langchain-twitter-algorithm\n</code></pre> <pre><code>-&gt; **Question**: What does favCountParams do?\n\n**Answer**: It is not clear from the given context what exactly `favCountParams` does. It seems to be an optional parameter of `ThriftLinearFeatureRankingParams` in a larger codebase that includes many other parameters and modules. Without more context or documentation, it is impossible to say what this parameter specifically does.\n\n-&gt; **Question**: is it Likes + Bookmarks, or not clear from the code?\n\n**Answer**: Based on the provided context, `favCountParams` is one of the optional `ThriftLinearFeatureRankingParams` in the codebase. However, without further context, it is unclear what its specific function is within the larger codebase.\n\n-&gt; **Question**: What are the major negative modifiers that lower your linear ranking parameters?\n\n**Answer**: I'm sorry, but I can't provide a specific answer to your question without more context. The code you provided includes multiple references to different linear ranking parameters and modifiers, so I would need to know which specific parameters and modifiers you are referring to. Could you please provide more specific context or clarify your question?\n\n-&gt; **Question**: How do you get assigned to SimClusters?\n\n**Answer**: Unfortunately, the given context does not provide information about how one gets assigned to SimClusters. The paper describes SimClusters as a general-purpose representation layer based on overlapping communities into which users as well as heterogeneous content can be captured as sparse, interpretable vectors to support a multitude of recommendation tasks. However, it does not provide information on the assignment process.\n\nRetrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.&lt;locals&gt;._completion_with_retry in 1.0 seconds as it raised RateLimitError: That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4b9f981cfd37f488b2dcfd71a14a5792 in your message.).\n-&gt; **Question**: What is needed to migrate from one SimClusters to another SimClusters?\n\n**Answer**: Can you explain the difference between SimClusters Scalding Jobs and SimClusters GCP Jobs?\n\n-&gt; **Question**: How much do I get boosted within my cluster?\n\n**Answer**: I'm sorry, but I couldn't find enough information to answer your question about the amount of boost that one receives within their SimCluster. Could you please provide me with more context or clarify your question?\n\n-&gt; **Question**: How does Heavy ranker work. what are it\u2019s main inputs?\n\n**Answer**: The Heavy Ranker is part of the Follow Recommendations Service (FRS), which is a service that generates a list of candidate accounts for a user to follow. The Heavy Ranker employs both Machine Learning (ML) and heuristic rule-based candidate ranking. The main inputs to the Heavy Ranker are the scoring algorithm, source embedding ID, candidate embedding type, minimum score, and candidates. The candidates are pairs of &lt;user, candidate&gt; and a DataRecord is constructed for each pair. The ML features are fetched beforehand (i.e., feature hydration) and then sent to a separate ML prediction service, which houses the ML model trained offline. The ML prediction service returns a prediction score, representing the probability that a user will follow and engage with the candidate. This score is a weighted sum of p(follow|recommendation) and p(positive engagement|follow), and FRS uses this score to rank the candidates.\n\n-&gt; **Question**: How can one influence Heavy ranker?\n\n**Answer**: Unfortunately, the context provided does not mention any specific methods to influence the Heavy Ranker in the Follow Recommendations Service. The Heavy Ranker employs Machine Learning and heuristic rule-based candidate ranking to rank the candidates. The Machine Learning model is trained offline, and its prediction score represents the probability that a user will follow and engage with the candidate. If there are any specific methods to influence the Heavy Ranker, they are not mentioned in the provided context.\n\n-&gt; **Question**: why threads and long tweets do so well on the platform?\n\n**Answer**: I'm sorry, based on the context provided, I cannot find any information about why threads and long tweets perform well on the platform. The context only provides information about the high-level architecture and main components of the Tweet Search System (Earlybird) and how it works to retrieve in-network tweets.\n\n-&gt; **Question**: Are thread and long tweet creators building a following that reacts to only threads?\n\n**Answer**: I'm sorry, but I cannot determine an answer to your question based on the given context. The code snippets provided are related to conversation controls and following features, and don't provide any information about creators who post long threads on the platform and their following interactions.\n\n-&gt; **Question**: Do you need to follow different strategies to get most followers vs to get most likes and bookmarks per tweet?\n\n**Answer**: Unfortunately, the given context does not provide information on the different strategies to get the most followers on the platform versus getting the most likes and bookmarks per tweet. The provided context mainly discusses the algorithms and services used by Twitter to rank users and tweets, and how these services are used to enhance the user experience.\n\n-&gt; **Question**: Content meta data and how it impacts virality (e.g. ALT in images).\n\n**Answer**: I'm sorry, but the given context does not provide any information on how content metadata, such as ALT in images, affects virality on the Twitter platform. The context mainly discusses the architecture of the Tweet Search System (Earlybird) and various rules used by the visibility library for tweet and user ranking.\n\n-&gt; **Question**: What are some unexpected fingerprints for spam factors?\n\n**Answer**: I'm sorry, but I don't see any context or information related to \"unexpected fingerprints for spam factors\". Can you please provide more context or clarify your question?\n\n-&gt; **Question**: Is there any difference between company verified checkmarks and blue verified individual checkmarks?\n\n**Answer**: Based on the code provided, it seems that the system is checking if a Twitter user is \"blue verified\", which means that they have been personally verified by Twitter as an individual. Company verified checkmarks, on the other hand, indicate that a business or organization's Twitter account has been verified by Twitter. So, the difference is that blue verified checkmarks are for individuals, while company verified checkmarks are for businesses or organizations.\n</code></pre>"},{"location":"langchain/01_twitter/#ref","title":"Ref","text":"<ul> <li>https://python.langchain.com/en/latest/reference/modules/chat_models.html#langchain.chat_models.ChatOpenAI</li> <li>https://github.com/langchain-ai/langchain/blob/master/cookbook/twitter-the-algorithm-analysis-deeplake.ipynb</li> </ul>"},{"location":"langchain/06_output_parser/","title":"Output Parser","text":""},{"location":"langchain/06_output_parser/#prompt","title":"Prompt","text":"<ol> <li>Prepare <code>ResponseSchema</code> <pre><code>gift_schema = ResponseSchema(name=\"gift\",\n                            description=\"Was the item purchased\\\n                            as a gift for someone else? \\\n                            Answer True if yes,\\\n                            False if not or unknown.\")\n</code></pre></li> <li> <p>Craete OutputParser</p> <p><pre><code>output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n</code></pre> 1. Generate format instructions from the OutputParser <pre><code>format_instructions = output_parser.get_format_instructions()\n</code></pre> 1. Chat 1. Parse <pre><code>output_dict = output_parser.parse(response.content)\n</code></pre></p> </li> </ol> <pre><code>poetry run python src/langchain/output_parser.py\n</code></pre> <pre><code>{'gift': True, 'delivery_days': '2', 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}\n</code></pre>"},{"location":"langchain/06_output_parser/#run","title":"Run","text":""},{"location":"langchain/09_token_count/","title":"09 token count","text":""},{"location":"langchain/09_token_count/#token-count","title":"Token Count","text":"<pre><code>from langchain_community.callbacks import get_openai_callback\n\nwith get_openai_callback() as cb:\n    result = llm(\"Tell me a joke\")\n    print(cb)\n</code></pre> <pre><code>Tokens Used: 6659\n        Prompt Tokens: 6321\n        Completion Tokens: 338\nSuccessful Requests: 4\nTotal Cost (USD): $0.13318\n</code></pre> <p>\u2191 \u3053\u308c\u306fstreaming\u306b\u306f\u5bfe\u5fdc\u3057\u3066\u3044\u306a\u3044 (https://github.com/langchain-ai/langchain/issues/4583) \u306e\u3067\u3001Streaming\u3067\u4f7f\u3046\u5834\u5408\u306b\u306f\u3001</p> <pre><code>from typing import Any, Dict, List\n\nimport tiktoken\nfrom langchain.callbacks.base import BaseCallbackHandler\nfrom langchain.callbacks.openai_info import (\n    MODEL_COST_PER_1K_TOKENS,\n    get_openai_token_cost_for_model,\n)\nfrom langchain.schema import LLMResult\n\n\nclass Cost:\n    def __init__(\n        self, total_cost=0, total_tokens=0, prompt_tokens=0, completion_tokens=0\n    ):\n        self.total_cost = total_cost\n        self.total_tokens = total_tokens\n        self.prompt_tokens = prompt_tokens\n        self.completion_tokens = completion_tokens\n\n    def __str__(self):\n        return (\n            f\"Tokens Used: {self.total_tokens}\\n\"\n            f\"\\tPrompt Tokens: {self.prompt_tokens}\\n\"\n            f\"\\tCompletion Tokens: {self.completion_tokens}\\n\"\n            f\"Total Cost (USD): ${self.total_cost}\"\n        )\n\n    def add_prompt_tokens(self, prompt_tokens):\n        self.prompt_tokens += prompt_tokens\n        self.total_tokens += prompt_tokens\n\n    def add_completion_tokens(self, completion_tokens):\n        self.completion_tokens += completion_tokens\n        self.total_tokens += completion_tokens\n\n\nclass CostCalcCallbackHandler(BaseCallbackHandler):\n    def __init__(self, model_name, cost, *args, **kwargs):\n        self.model_name = model_name\n        self.encoding = tiktoken.encoding_for_model(model_name)\n        self.cost = cost\n        super().__init__(*args, **kwargs)\n\n    def on_llm_start(\n        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n    ) -&gt; None:\n        for prompt in prompts:\n            self.cost.add_prompt_tokens(len(self.encoding.encode(prompt)))\n\n    def on_llm_end(self, response: LLMResult, **kwargs: Any) -&gt; None:\n        for generation_list in response.generations:\n            for generation in generation_list:\n                self.cost.add_completion_tokens(\n                    len(self.encoding.encode(generation.text))\n                )\n        if self.model_name in MODEL_COST_PER_1K_TOKENS:\n            prompt_cost = get_openai_token_cost_for_model(\n                self.model_name, self.cost.prompt_tokens\n            )\n            completion_cost = get_openai_token_cost_for_model(\n                self.model_name, self.cost.completion_tokens, is_completion=True\n            )\n            self.cost.total_cost = prompt_cost + completion_cost\n</code></pre> <p>\u3092\u5b9a\u7fa9\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u4f7f\u3046\u3053\u3068\u304c\u3067\u304d\u308b\u3002</p> <pre><code>cost = Cost()\nqa = ConversationalRetrievalChain.from_llm(\n    ChatOpenAI(\n        model_name=MODEL_NAME,\n        streaming=True,\n        callbacks=[handler, CostCalcCallbackHandler(MODEL_NAME, cost)],\n    ),\n    retriever=retriever,\n    return_source_documents=True,\n)\n\n# \u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306e\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u8868\u793a\nresult = qa(\n    {\"question\": user_msg, \"chat_history\": st.session_state.chat_history}\n)\n\nprint(result)\nst.session_state[SESSION_KEY_FOR_COST].append(\n    {\"usage\": \"normal\", \"amount\": cost.total_cost}\n)\n</code></pre>"},{"location":"langchain/10_callbacks/","title":"Callbacks","text":""},{"location":"langchain/10_callbacks/#overview","title":"Overview","text":"<p>LLM\u3092\u547c\u3076\u30bf\u30a4\u30df\u30f3\u30b0\u306a\u3069\u306b\u51e6\u7406\u3092\u631f\u3080\u3053\u3068\u304c\u3067\u304d\u308b</p> <ol> <li><code>on_llm_start</code></li> <li><code>on_llm_end</code></li> <li><code>on_llm_new_token</code></li> <li><code>on_llm_error</code></li> <li><code>on_chain_start</code></li> <li><code>on_chain_end</code></li> <li><code>on_agent_action</code></li> <li><code>on_tool_end</code></li> <li><code>on_tool_error</code></li> <li><code>on_agent_finish</code></li> </ol>"},{"location":"langchain/10_callbacks/#examples","title":"Examples","text":"<ol> <li>StdOutCallbackHandler &lt;- <code>verbose=True</code> \u3068\u3059\u308b\u3068\u30bb\u30c3\u30c8\u3055\u308c\u308bCallbackHandler</li> <li>CostCalcCallbackHandler: CostStreaming\u7528\u306eCallback</li> <li>SimpleStreamlitCallbackHandler: Streamlit\u3067\u30c1\u30e3\u30c3\u30c8\u304cStreaming\u3067\u51fa\u308b\u3088\u3046\u306b\u3059\u308b\u305f\u3081\u306eCallback</li> </ol>"},{"location":"langchain/10_callbacks/#run","title":"Run","text":"<pre><code>poetry run pytest -k test_callback -s\n</code></pre> <pre><code>platform darwin -- Python 3.9.9, pytest-7.4.0, pluggy-1.2.0\nrootdir: /Users/m.naka/repos/nakamasato/gpt-poc\nplugins: cov-4.1.0\ncollected 4 items / 3 deselected / 1 selected\n\ntests/test_callbacks.py run_id=UUID('a49fbd93-2745-4b73-807b-7ae668fcc9fd')\nprompts=['Tell me a joke']\nserialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain', 'llms', 'fake', 'FakeListLLM']}\n.\n</code></pre> <p>prompts\u3092\u30c1\u30a7\u30c3\u30af\u3057\u305f\u308a\u3067\u304d\u308b\u3002</p>"},{"location":"langchain/11_chat_model/","title":"ChatModels","text":"<p>Chat Models\u306fText in, Text out\u3067\u306f\u306a\u304f\u3001Message\u3068\u3044\u3046Interface\u3092\u4f7f\u3063\u3066\u3084\u308a\u53d6\u308a\u3059\u308b\u3002 Langchain\u3067\u306f\u3001 <code>gpt-3.5-turbo</code>\u3084<code>gpt-4</code>\u3067<code>OpenAI()</code>\u3092\u521d\u671f\u5316\u3059\u308b\u5834\u5408\u306f\u5185\u90e8\u7684\u306b<code>ChatOpenAI()</code>\u304c\u521d\u671f\u5316\u3055\u308c\u3066\u3044\u305f\u304c\u3001\u76f4\u63a5<code>ChatOpenAI()</code>\u3067\u521d\u671f\u5316\u3059\u308b\u3088\u3046\u306bWarning\u304c\u51fa\u3066\u3044\u308b</p> <p>Message\u306e\u7a2e\u985e: 1. <code>AIMessage</code>: A chat message representing information coming from the AI system. \u4eba\u304c\u64cd\u4f5c\u3059\u308b\u3082\u306e\u3067\u306f\u306a\u304fAI\u304b\u3089\u8fd4\u3063\u3066\u304f\u308b\u30e1\u30c3\u30bb\u30fc\u30b8 1. <code>HumanMessage</code>: A chat message representing information coming from a human interacting with the AI system. \u805e\u304d\u305f\u3044\u5185\u5bb9\u3092\u3044\u308c\u308b e.g. <code>\"\"</code> 1. <code>SystemMessage</code>: A chat message representing information that should be instructions to the AI system. System\u306b\u4f55\u304b\u3092\u8a00\u3044\u6e21\u3059\u3002 e.g. <code>\"You are a helpful assistant.\"</code> 1. <code>ChatMessage</code>: \u3053\u3044\u3064\u306f\u57fa\u672c\u4f7f\u308f\u306a\u3044\u304b\u3082\u3002</p> <p>Message\u306e\u4f7f\u3044\u65b9\uff1a<code>SystemMessage</code>\u3067\u3001AI\u306e\u30b9\u30bf\u30f3\u30b9\u3092\u6c7a\u3081\u3001<code>HumanMessage</code>\u3068<code>AIMessage</code>\u306e\u7d44\u307f\u5408\u308f\u305b\u3092\u6e21\u3057\u3066\u3001\u305d\u308c\u307e\u3067\u306eChat\u306e\u5c65\u6b74\u3092\u6a21\u5023\u3057\u3066\u3001\u6700\u7d42\u7684\u306b\u7d50\u679c\u3092\u51fa\u3057\u3066\u3082\u3089\u3046</p> <p>\u4f8b:</p> <pre><code>messages = [\n    SystemMessage(content=\"\u3042\u306a\u305f\u306f\u89aa\u5207\u306a\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059\u3002\"),\n    HumanMessage(content=\"\u6625\u306e\u5b63\u8a9e\u3092\u7d61\u3081\u305f\u5197\u8ac7\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002\"),\n    AIMessage(content=\"\u300c\u6625\u7720\uff08\u3057\u3085\u3093\u307f\u3093\uff09\u6681\uff08\u304e\u3087\u3046\uff09\u3092\u899a\uff08\u3055\uff09\u3048\u305a\u300d\u3068\u3044\u3046\u8a00\u8449\u304c\u3042\u308a\u307e\u3059\u304c\u3001\u300c\u6625\u306f\u7720\u304f\u3066\u3082\u3001\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u306f\u899a\u3048\u3066\u308b\u3088\uff01\u300d\u3068\u8a00\u3063\u3066\u3001\u30c4\u30c3\u30b3\u30df\u3092\u5165\u308c\u308b\u306e\u306f\u3044\u304b\u304c\u3067\u3057\u3087\u3046\u304b\uff1f\u7b11\"),\n    HumanMessage(content=\"\u9762\u767d\u304f\u306a\u3044\u3002\u3082\u3046\u4e00\u5ea6\u3002\"),\n]\n</code></pre> <p>Ref: ChatMessages</p> <p>\u4f7f\u3044\u65b9:</p> <pre><code>from langchain_openai import ChatOpenAI\n\nchat = ChatOpenAI()\n</code></pre> <pre><code># \u76f4\u63a5\u30e1\u30c3\u30bb\u30fc\u30b8\u3092\u5165\u308c\u308b\nchat([HumanMessage(content=\"Translate this sentence from English to French: I love programming.\")])\n\n# \u8907\u6570\u30e1\u30c3\u30bb\u30fc\u30b8\nmessages = [\n    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n    HumanMessage(content=\"I love programming.\")\n]\nchat(messages)\n\n# generate\nbatch_messages = [\n    [\n        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n        HumanMessage(content=\"I love programming.\")\n    ],\n    [\n        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n        HumanMessage(content=\"I love artificial intelligence.\")\n    ],\n]\nresult = chat.generate(batch_messages)\n</code></pre>"},{"location":"langchain/12_summarize/","title":"Summarize","text":""},{"location":"langchain/12_summarize/#1-chain-types","title":"1. Chain Types","text":"<ol> <li><code>stuff</code>: \u3059\u3079\u3066\u306eDocument\u3092combine\u3057\u3066LLM\u306b\u9001\u308b</li> <li><code>map_reduce</code>: \u305d\u308c\u305e\u308c\u306e\u8981\u7d04\u3092\u3064\u304f\u3063\u3066\u304b\u3089\u307e\u3068\u3081\u308b</li> <li><code>refine</code>:\u6700\u521d\u306b\u3088\u3046\u3084\u304f\u3092\u4f5c\u308a\u3001\u6b21\u306eDoc\u3068\u5408\u308f\u305b\u3066\u3088\u3046\u3084\u304f\u3092\u4f5c\u3063\u3066\u3044\u304f (Incremental)</li> </ol>"},{"location":"langchain/12_summarize/#2-implementaion","title":"2. Implementaion","text":"<ol> <li> <p>load_summarize_chain: llm\u3068Chain Type\u3092\u6307\u5b9a\u3057\u3066chain\u3092\u8fd4\u3059\u3002</p> <p>\u5f15\u6570: 1. <code>llm</code>: 1. <code>chain_type</code>: <code>stuff</code>\u304c\u30c7\u30d5\u30a9\u30eb\u30c8 1. <code>verbose</code>: bool 1. <code>**kwargs</code>:</p> <p>\u8fd4\u308a\u5024: <code>BaseCombineDocumentsChain</code></p> <p>\u5185\u90e8\u3067\u306f\u3001chain_type\u306b\u3088\u3063\u3066\u305d\u308c\u305e\u308c\u5bfe\u8c61\u3068\u306a\u308bChain\u3092Load\u3057\u3066\u3044\u308b</p> <p><pre><code>loader_mapping: Mapping[str, LoadingCallable] = {\n    \"stuff\": _load_stuff_chain,\n    \"map_reduce\": _load_map_reduce_chain,\n    \"refine\": _load_refine_chain,\n}\n</code></pre> \u305d\u308c\u305e\u308c\u306f\u3001\u305d\u308c\u305e\u308c\u306eChain\u3092\u521d\u671f\u5316\u3059\u308b\u3002 1. _load_stuff_chain: \u30c7\u30d5\u30a9\u30eb\u30c8\u3067<code>stuff_prompt.PROMPT</code>\u6307\u5b9a\u3055\u308c\u308b\u3002\u81ea\u5206\u3067\u6307\u5b9a\u3057\u305f\u3044\u5834\u5408\u306b\u306f\u3001<code>load_summarize_chain</code>\u306b\u5f15\u6570\u3067<code>prompt</code>\u3092\u6307\u5b9a\u3059\u308c\u3070\u826f\u3044\u3002prompt\u5185\u306evariable\u3082<code>document_variable_name</code>\u3067\u6307\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b     <pre><code>def _load_stuff_chain(\n    llm: BaseLanguageModel,\n    prompt: BasePromptTemplate = stuff_prompt.PROMPT,\n    document_variable_name: str = \"text\",\n    verbose: Optional[bool] = None,\n    **kwargs: Any,\n) -&gt; StuffDocumentsChain:\n    llm_chain = LLMChain(llm=llm, prompt=prompt, verbose=verbose)\n    # TODO: document prompt\n    return StuffDocumentsChain(\n        llm_chain=llm_chain,\n        document_variable_name=document_variable_name,\n        verbose=verbose,\n        **kwargs,\n    )\n</code></pre> 1. _load_map_reduce_chain: \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f<code>map_reduce_prompt.PROMPT</code>\u304c<code>map_prompt</code>\u3068<code>combine_prompt</code>\u306b\u6307\u5b9a\u3055\u308c\u308b\u304c\u3001\u81ea\u5206\u3067\u305d\u308c\u305e\u308c\u6307\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\u307e\u305f\u3001variable\u3082<code>combine_document_variable_name</code>\u3068<code>map_reduce_document_variable_name</code>\u3067\u6307\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002     <pre><code>def _load_map_reduce_chain(\n    llm: BaseLanguageModel,\n    map_prompt: BasePromptTemplate = map_reduce_prompt.PROMPT,\n    combine_prompt: BasePromptTemplate = map_reduce_prompt.PROMPT,\n    combine_document_variable_name: str = \"text\",\n    map_reduce_document_variable_name: str = \"text\",\n    collapse_prompt: Optional[BasePromptTemplate] = None,\n    reduce_llm: Optional[BaseLanguageModel] = None,\n    collapse_llm: Optional[BaseLanguageModel] = None,\n    verbose: Optional[bool] = None,\n    token_max: int = 3000,\n    **kwargs: Any,\n) -&gt; MapReduceDocumentsChain:\n    map_chain = LLMChain(llm=llm, prompt=map_prompt, verbose=verbose)\n    _reduce_llm = reduce_llm or llm\n    reduce_chain = LLMChain(llm=_reduce_llm, prompt=combine_prompt, verbose=verbose)\n    ...\n</code></pre> 1. _load_refine_chain: \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u3001<code>question_prompt</code>\u306b\u306f\u3001<code>refine_prompts.REFINE_PROMPT</code>\u304c\u3001<code>refine_prompt</code>\u306b\u306f\u3001<code>refine_prompts.REFINE_PROMPT</code>\u304c\u6307\u5b9a\u3055\u308c\u308b\u304c\u81ea\u5206\u3067\u6307\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002variable\u3082<code>document_variable_name</code>\u3067\u6307\u5b9a\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002     <pre><code>def _load_refine_chain(\n    llm: BaseLanguageModel,\n    question_prompt: BasePromptTemplate = refine_prompts.PROMPT,\n    refine_prompt: BasePromptTemplate = refine_prompts.REFINE_PROMPT,\n    document_variable_name: str = \"text\",\n    initial_response_name: str = \"existing_answer\",\n    refine_llm: Optional[BaseLanguageModel] = None,\n    verbose: Optional[bool] = None,\n    **kwargs: Any,\n) -&gt; RefineDocumentsChain:\n    initial_chain = LLMChain(llm=llm, prompt=question_prompt, verbose=verbose)\n    _refine_llm = refine_llm or llm\n    refine_chain = LLMChain(llm=_refine_llm, prompt=refine_prompt, verbose=verbose)\n    return RefineDocumentsChain(\n        initial_llm_chain=initial_chain,\n        refine_llm_chain=refine_chain,\n        document_variable_name=document_variable_name,\n        initial_response_name=initial_response_name,\n        verbose=verbose,\n        **kwargs,\n    )\n</code></pre> 1. BaseCombineDocumentsChain: Stuff, MapReduce, Refine Documents Chain\u306f\u3059\u3079\u3066BaseCombineDocumentsChain\u3092\u5b9f\u88c5\u3057\u3066\u3044\u308b 1. <code>combine_docs</code>: Documents\u3092\u4e00\u3064\u306estring\u306b\u5909\u63db\u3059\u308b 1. <code>prompt_length</code>: \u4e0e\u3048\u3089\u308c\u305fDocuments\u306b\u5bfe\u3057\u3066\u306ePrompt\u306e\u9577\u3055\u3092\u8fd4\u3059</p> </li> <li> <p>StuffDocumentsChain:     combine_docs: function to stuff all documents into one prompt and pass to LLM.</p> <p>prompt:</p> <pre><code>\"\"\"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:\"\"\"\n</code></pre> </li> <li> <p><code>MapReduceDocumentsChain</code>:     prompt     <pre><code>\"\"\"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:\"\"\"\n</code></pre></p> </li> <li><code>RefineDocumentsChain</code>:     init prompt:     <pre><code>\"\"\"Write a concise summary of the following:\n\n\n\"{text}\"\n\n\nCONCISE SUMMARY:\"\"\"\n</code></pre>     refine prompt:     <pre><code>\"Your job is to produce a final summary\\n\"\n\"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n\"We have the opportunity to refine the existing summary\"\n\"(only if needed) with some more context below.\\n\"\n\"------------\\n\"\n\"{text}\\n\"\n\"------------\\n\"\n\"Given the new context, refine the original summary\"\n\"If the context isn't useful, return the original summary.\"\n</code></pre></li> </ol>"},{"location":"langchain/12_summarize/#basic-usage","title":"Basic Usage","text":"<ol> <li> <p>Import     <pre><code>from langchain import OpenAI, PromptTemplate, LLMChain\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.chains.mapreduce import MapReduceChain\nfrom langchain.prompts import PromptTemplate\n\nllm = OpenAI(temperature=0)\n</code></pre></p> </li> <li> <p>doc\u306e\u6e96\u5099 (state_of_the_union.txt (723\u884c))     <pre><code>text_splitter = CharacterTextSplitter()\n\nwith open(\"../../state_of_the_union.txt\") as f:\n    state_of_the_union = f.read()\ntexts = text_splitter.split_text(state_of_the_union)\n\nfrom langchain.docstore.document import Document\n\ndocs = [Document(page_content=t) for t in texts[:3]]\n</code></pre></p> </li> <li> <p>Summarize</p> <p><pre><code>from langchain.chains.summarize import load_summarize_chain\n\nchain = load_summarize_chain(llm, chain_type=\"map_reduce\")\nchain.run(docs)\n</code></pre> 1. \u7d50\u679c <code>In response to Russian aggression in Ukraine, the US and its allies are taking action to hold Putin accountable, including economic sanctions, cutting off access to technology, and a dedicated task force to go after the crimes of Russian oligarchs. The US is also providing economic and military assistance to Ukraine and mobilizing ground forces, air squadrons, and ship deployments to protect NATO countries. President Biden's American Rescue Plan and Bipartisan Infrastructure Law have provided economic relief and created jobs for millions of Americans.</code></p> </li> </ol>"},{"location":"langchain/12_summarize/#advanced","title":"Advanced","text":""},{"location":"langchain/12_summarize/#tip1-prompt","title":"Tip1: Prompt","text":"<p><code>{text}</code>\u304c\u5fc5\u8981\u306b\u306a\u308b</p> <pre><code>prompt_template = \"\"\"Write a concise summary of the following:\n\n\n{text}\n\n\nCONCISE SUMMARY IN JAPANESE:\"\"\"\nPROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])\nchain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\nchain.run(docs)\n</code></pre> <pre><code>{\n    'intermediate_steps':\n        [\n            '\\n\u4eca\u591c\u3001\u79c1\u305f\u3061\u306f\u30a2\u30e1\u30ea\u30ab\u4eba\u3068\u3057\u3066\u4e00\u7dd2\u306b\u96c6\u307e\u308a\u307e\u3057\u305f\u3002\u30ed\u30b7\u30a2\u306e\u30d7\u30fc\u30c1\u30f3\u304c\u30a6\u30af\u30e9\u30a4\u30ca\u306b\u4fb5\u653b\u3057\u3088\u3046\u3068\u3057\u305f\u3068\u304d\u3001\u30a6\u30af\u30e9\u30a4\u30ca\u306e\u4eba\u3005\u306f\u52c7\u6562\u306b\u7acb\u3061\u5411\u304b\u3044\u307e\u3057\u305f\u3002NATO\u52a0\u76df\u56fd\u306f\u30ed\u30b7\u30a2\u3092\u5236\u88c1\u3057\u3001\u72af\u7f6a\u3092\u8ffd\u6c42\u3057\u3066\u3044\u307e\u3059\u3002\u30d7\u30fc\u30c1\u30f3\u306f\u4eca\u3088\u308a\u3082\u3063\u3068\u5b64\u7acb\u3057\u3066\u3044\u307e\u3059\u3002\u79c1\u305f\u3061\u306f\u30d7\u30fc\u30c1\u30f3\u306e\u8c4a\u304b\u306a\u8cc7\u7523\u3092\u53d6\u308a\u4e0a\u3052\u308b\u305f\u3081\u306b\u6e96\u5099\u3057\u3066\u3044\u307e\u3059\u3002',\n            '\\n\u30a2\u30e1\u30ea\u30ab\u306f\u30e8\u30fc\u30ed\u30c3\u30d1\u306e\u4ef2\u9593\u3068\u5171\u306b\u3001\u30ed\u30b7\u30a2\u306e\u8c6a\u83ef\u306a\u30e8\u30c3\u30c8\u3001\u30a2\u30d1\u30fc\u30c8\u3001\u30d7\u30e9\u30a4\u30d9\u30fc\u30c8\u30b8\u30a7\u30c3\u30c8\u3092\u62bc\u53ce\u3059\u308b\u305f\u3081\u306b\u53d6\u308a\u7d44\u3093\u3067\u3044\u307e\u3059\u3002\u7a7a\u6e2f\u3092\u9589\u9396\u3057\u3001\u30ed\u30b7\u30a2\u306e\u7d4c\u6e08\u3092\u62bc\u3057\u3064\u3076\u3059\u305f\u3081\u306b\u3001\u7c73\u56fd\u306f\u4ef2\u9593\u3068\u5171\u306b\u3001\u30a6\u30af\u30e9\u30a4\u30ca\u306b\u8ecd\u4e8b\u652f\u63f4\u3001\u7d4c\u6e08\u652f\u63f4\u3001\u4eba\u9053\u652f\u63f4\u3092\u63d0\u4f9b\u3057\u3066\u3044\u307e\u3059\u3002\u7c73\u56fd\u306f\u30a6\u30af\u30e9\u30a4\u30ca\u306b10\u5104\u30c9\u30eb\u4ee5\u4e0a\u306e\u652f\u63f4\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\u7c73\u56fd\u306fNATO\u52a0\u76df\u56fd\u3092\u5b88\u308b\u305f\u3081\u306b\u3001\u5730\u4e0a\u90e8\u968a\u3001\u822a\u7a7a\u90e8\u968a',\n            '\\n\\n\u30a2\u30e1\u30ea\u30ab\u306f2\u5e74\u9593\u3067\u6700\u3082\u53b3\u3057\u3044\u6642\u671f\u3092\u8fce\u3048\u307e\u3057\u305f\u3002\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306f\u53b3\u3057\u304f\u3001\u591a\u304f\u306e\u5bb6\u65cf\u304c\u98df\u6599\u3001\u30ac\u30bd\u30ea\u30f3\u3001\u4f4f\u5b85\u306a\u3069\u306e\u8cbb\u7528\u3092\u652f\u6255\u3046\u306e\u306b\u82e6\u52b4\u3057\u3066\u3044\u307e\u3059\u3002\u79c1\u306f\u7406\u89e3\u3057\u3066\u3044\u307e\u3059\u3002\u79c1\u306f\u7236\u304c\u30b9\u30b3\u30fc\u30c8\u30f3\u3001\u30da\u30f3\u30b7\u30eb\u30d9\u30cb\u30a2\u304b\u3089\u4ed5\u4e8b\u3092\u63a2\u3059\u305f\u3081\u306b\u5bb6\u3092\u51fa\u305f\u306e\u3092\u899a\u3048\u3066\u3044\u307e\u3059\u3002\u79c1\u304c\u5927\u7d71\u9818\u306b\u306a\u3063\u305f\u6700\u521d\u306e\u3053\u3068\u306e1\u3064\u306f\u3001\u30a2\u30e1\u30ea\u30ab\u6551\u6e08\u8a08\u753b\u3092\u901a\u3059\u305f\u3081\u306b\u6226\u3046\u3053\u3068\u3067\u3057\u305f\u3002\u3053\u306e\u8a08\u753b\u306f\u3001\u56fd\u6c11\u3092\u6551\u6e08\u3057\u3001COVID-19\u3068\u6226\u3046\u305f\u3081\u306b'\n        ],\n    'output_text': '\\n\\n\u30a2\u30e1\u30ea\u30ab\u306fNATO\u52a0\u76df\u56fd\u3092\u5b88\u308b\u305f\u3081\u306b\u3001\u30a6\u30af\u30e9\u30a4\u30ca\u3078\u306e\u8ecd\u4e8b\u652f\u63f4\u3001\u7d4c\u6e08\u652f\u63f4\u3001\u4eba\u9053\u652f\u63f4\u3092\u884c\u3044\u3001\u30d7\u30fc\u30c1\u30f3\u306e\u8c4a\u304b\u306a\u8cc7\u7523\u3092\u53d6\u308a\u4e0a\u3052\u308b\u6e96\u5099\u3092\u3057\u3066\u3044\u307e\u3059\u3002\u307e\u305f\u3001\u30d1\u30f3\u30c7\u30df\u30c3\u30af\u306b\u3088\u308a\u591a\u304f\u306e\u5bb6\u65cf\u304c\u82e6\u52b4\u3057\u3066\u3044\u308b\u3053\u3068\u3092\u7406\u89e3\u3057\u3001\u56fd\u6c11\u3092\u6551\u6e08\u3059\u308b\u305f\u3081\u306b\u6226\u3063\u3066\u3044\u307e\u3059\u3002'\n}\n</code></pre>"},{"location":"langchain/12_summarize/#_1","title":"\u8ab2\u984c","text":"<ul> <li>[ ] \u65e5\u672c\u8a9e\u304c\u5207\u308c\u3066\u3057\u307e\u3046     </li> </ul>"},{"location":"langchain/12_summarize/#ref","title":"Ref","text":"<ol> <li>LangChain\u306eSummarization\u306b\u3064\u3044\u3066\u8abf\u3079\u305f\u307e\u3068\u3081</li> </ol>"},{"location":"langchain/13_text_splitter/","title":"Text Splitter","text":""},{"location":"langchain/13_text_splitter/#overview","title":"Overview","text":"<p>Embedding\u3059\u308b\u307e\u3048\u306b\u3061\u3087\u3046\u3069\u3044\u3044\u9577\u3055\u3084Chunk\u306b\u5206\u5272\u3059\u308b\u305f\u3081\u306b\u4f7f\u308f\u308c\u308b\u3002</p> <p>https://langchain-text-splitter.streamlit.app/ \u3067\u3044\u308d\u3093\u306a\u30b1\u30fc\u30b9\u3092\u8a66\u3057\u3066\u307f\u308b\u3068\u3044\u3044\u3002</p>"},{"location":"langchain/13_text_splitter/#types","title":"Types","text":"<ol> <li>TextSplitter: Abstract class</li> <li>CharacterTextSplitter</li> <li>RecursiveCharacterTextSplitter: <code>from_language</code>\u3092\u4f7f\u3046\u3053\u3068\u3067\u3001\u5bfe\u5fdc\u3059\u308bseparators\u306b\u3088\u3063\u3066Split\u3059\u308b\u3002 <code>split_documents</code>\u3092\u4f7f\u3063\u3066document\u3092\u518d\u5206\u5272\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u308b</li> <li>MarkdownHeaderTextSplitter: Markdown\u306e\u30d8\u30c3\u30c0\u30fc\u306b\u3088\u3063\u3066\u5206\u5272\u3059\u308b\u3002 metadata\u306b\u305d\u306e\u30d8\u30c3\u30c0\u30fc\u60c5\u5831\u3082\u5165\u308b\u306e\u3067\u3044\u3044\u3002</li> <li>TokenTextSplitter</li> <li>SentenceTransformersTokenTextSplitter</li> </ol>"},{"location":"langchain/13_text_splitter/#sample","title":"Sample","text":"<pre><code>from langchain.text_splitter import RecursiveCharacterTextSplitter\n\n\ndef text_split(texts):\n    splitter = RecursiveCharacterTextSplitter(\n        separators=None,\n        chunk_size = 20,\n        chunk_overlap = 0,\n    )\n    docs = splitter.create_documents(\n        texts=texts,\n    )\n    return docs\n\n\nif __name__ == \"__main__\":\n    texts = [\n        \"\"\"\n\u3053\u306e\u304f\u3089\u3044\u9577\u3044\u30c6\u30ad\u30b9\u30c8\u3092\u5207\u3063\u3066\u307f\u307e\u3059\u3002\n\u3055\u3089\u306b\u305f\u304f\u3055\u3093\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u5165\u308c\u307e\u3059\u3002\n\"\"\"\n    ]\n    print(text_split(texts=texts))\n</code></pre> <p>Result:</p> <pre><code>[Document(page_content='\u3053\u306e\u304f\u3089\u3044\u9577\u3044\u30c6\u30ad\u30b9\u30c8\u3092\u5207\u3063\u3066\u307f\u307e\u3059\u3002', metadata={}), Document(page_content='\u3055\u3089\u306b\u305f\u304f\u3055\u3093\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u5165\u308c\u307e\u3059\u3002', metadata={})]\n</code></pre>"},{"location":"langchain/14_vector/","title":"Vectorstore","text":"<p>https://python.langchain.com/docs/integrations/vectorstores</p>"},{"location":"langchain/14_vector/#overview","title":"Overview","text":"<ol> <li>Emebdding\u3057\u305fVector\u3092\u683c\u7d0d\u3059\u308bDatastore</li> <li>LangChain\u306eVectorStore\u306e\u4e2d\u306evectorstore\u306b\u5b9f\u969b\u306evectorstore\u304c\u683c\u7d0d\u3055\u308c\u3066\u3044\u308b</li> <li>DeepLake\u306a\u3089deeplake_vectorstore.py\u304c\u672c\u4f53\u306evectorstore.<ol> <li> <p>dataset</p> <ol> <li>delete_and_commit</li> </ol> </li> </ol> </li> </ol>"},{"location":"langchain/14_vector/#types-of-vector-store","title":"Types of Vector Store","text":"<ol> <li>Qdrant</li> <li>Deeplake</li> </ol>"},{"location":"langchain/14_vector/#usage-embedding-vector-vectorstore-qdrant-retrievalqa","title":"Usage: Embedding -&gt; Vector -&gt; VectorStore (Qdrant) -&gt; RetrievalQA","text":"<ol> <li>Qdrant \u306eClient\u306e\u521d\u671f\u5316(local file\u306e\u5834\u5408)     <pre><code>client = QdrantClient(path=QDRANT_PATH)\n</code></pre>     Qdrant     <pre><code>Qdrant(client=client, collection_name=COLLECTION_NAME, embeddings=OpenAIEmbeddings())\n</code></pre></li> <li>Qdrant\u306b<code>str</code>\u307e\u305f\u306f<code>Document</code>\u3092\u8ffd\u52a0     <pre><code>qdrant.add_texts(texts_or_docs)\nqdrant.add_documents(texts_or_docs)\n</code></pre></li> <li>Retriever\u306e\u521d\u671f\u5316     <pre><code>retriever = qdrant.as_retriever(\n    # \"mmr\",  \"similarity_score_threshold\" \u306a\u3069\u3082\u3042\u308b\n    search_type=\"similarity\",\n    # \u6587\u66f8\u3092\u4f55\u500b\u53d6\u5f97\u3059\u308b\u304b (default: 4)\n    search_kwargs={\"k\": 5},\n)\nreturn RetrievalQA.from_chain_type(\n    llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True, verbose=True\n)\n</code></pre></li> <li>Question\uff06Answer: \u4e0a\u3067\u4f5c\u6210\u3057\u305fretriver\u306b\u8cea\u554f\u3059\u308b     <pre><code>qa(\"\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u4e0a\u3067\u30e6\u30fc\u30b6\u306b\u554f\u984c\u304c\u3042\u3063\u305f\u30b1\u30fc\u30b9\u3092\u6559\u3048\u3066\u304f\u3060\u3055\u3044\u3002\")\n</code></pre></li> </ol>"},{"location":"langchain/14_vector/#vectorstore-interface-for-vector-store","title":"VectorStore (Interface for vector store)","text":"<ol> <li> <p>VectorStore.from_documents\u306f texts \u3068 metadatas\u3092from_texts\u306b\u6e21\u3057\u3066\u3044\u308b\u3060\u3051:</p> <pre><code>@classmethod\ndef from_documents(\n    cls: Type[VST],\n    documents: List[Document],\n    embedding: Embeddings,\n    **kwargs: Any,\n) -&gt; VST:\n    \"\"\"Return VectorStore initialized from documents and embeddings.\"\"\"\n    texts = [d.page_content for d in documents]\n    metadatas = [d.metadata for d in documents]\n    return cls.from_texts(texts, embedding, metadatas=metadatas, **kwargs)\n</code></pre> </li> <li> <p>DeepLake.from_texts\u306f \u7d50\u5c40\u306f<code>add_texts</code>\u3092\u3057\u3066\u308b\u3060\u3051     <pre><code>    @classmethod\n    def from_texts(\n        cls,\n        texts: List[str],\n        embedding: Optional[Embeddings] = None,\n        metadatas: Optional[List[dict]] = None,\n        ids: Optional[List[str]] = None,\n        dataset_path: str = _LANGCHAIN_DEFAULT_DEEPLAKE_PATH,\n        **kwargs: Any,\n    ) -&gt; DeepLake:\n        deeplake_dataset = cls(dataset_path=dataset_path, embedding=embedding, **kwargs)\n        deeplake_dataset.add_texts(\n            texts=texts,\n            metadatas=metadatas,\n            ids=ids,\n        )\n        return deeplake_dataset\n</code></pre></p> </li> <li> <p>DeepLake.add_texts\u306f\u3001DeepLakeVectorStore.add (https://github.com/activeloopai/deeplake)\u3057\u3066\u3044\u308b (\u5177\u4f53\u7684\u306b\u3084\u3063\u3066\u3044\u308b\u3053\u3068\u306fEmbedding Function (\u3053\u306e\u30b1\u30fc\u30b9\u3060\u3068OpenAI)\u3092\u53e9\u3044\u3066\u4e0e\u3048\u3089\u308c\u305f\u30c6\u30ad\u30b9\u30c8\u3092Embedding\u3057\u3066VectorStore\u306b\u683c\u7d0d\u3059\u308b)</p> <pre><code>    return self.vectorstore.add(\n        text=texts,\n        metadata=metadatas,\n        embedding_data=texts,\n        embedding_tensor=\"embedding\",\n        embedding_function=self._embedding_function.embed_documents,  # type: ignore\n        return_ids=True,\n        **kwargs,\n    )\n</code></pre> </li> </ol>"},{"location":"langchain/14_vector/#tips","title":"Tips","text":"<pre><code>vector_store = DeepLake(\n    dataset_path=DATASET_PATH,\n    read_only=False,\n    embedding=cached_embedder,\n)\n</code></pre> <ol> <li>dataset\u30b5\u30de\u30ea\u30fc     <pre><code>vector_store.vectorstore.dataset.summary()\nDataset(path='./datasets/v1', tensors=['embedding', 'id', 'metadata', 'text'])\n\n  tensor      htype     shape    dtype  compression\n  -------    -------   -------  -------  -------\n embedding  embedding   (0,)    float32   None\n    id        text      (0,)      str     None\n metadata     json      (0,)      str     None\n   text       text      (0,)      str     None\n</code></pre></li> <li> <p>dataset\u3092\u30ea\u30bb\u30c3\u30c8\u3059\u308b (Tensor\u3082\u6d88\u3048\u308b\u306e\u3067\u6ce8\u610f)     <pre><code>import deeplake\ndeeplake.empty(vector_store.vectorstore.dataset.path, overwrite=True)\n</code></pre></p> <p>with delete_all_samples_if_specified <pre><code>from deeplake.core.vectorstore.vector_search import dataset as dataset_utils\ndataset_utils.delete_all_samples_if_specified(vector_store.vectorstore.dataset, True)\n</code></pre></p> <p><pre><code>from deeplake.core.vectorstore.vector_search import dataset as dataset_utils\n</code></pre> 1. dataset\u306etensor\u3092Clear\u3059\u308b (<code>id</code>\u306fTensor\u540d) <pre><code>vector_store.vectorstore.dataset.id.clear()\n</code></pre></p> </li> </ol>"},{"location":"langchain/15_conversational_retrieval/","title":"Conversational Retrieval","text":""},{"location":"langchain/15_conversational_retrieval/#overview","title":"Overview","text":"<p>Retrieval uses following components:</p> <ol> <li>Document loaders</li> <li>Document transformers</li> <li>Text embedding models</li> <li>Vector stores</li> <li>Retrievers</li> </ol> <p>RetrievalChain \u306f\u3001<code>question_generator</code> (LLMChain) \u3092\u4f7f\u3063\u3066\u4e0e\u3048\u3089\u308c\u305f<code>question</code>\u3068<code>chat_history</code>\u304b\u3089\u8cea\u554f\u3092\u751f\u6210\u3057\u3066\u6295\u3052\u308b\u3002\u3053\u308c\u306b\u3088\u3063\u3066\u904e\u53bb\u306echat_history\u306e\u6587\u8108\u3092\u542b\u3093\u3060\u8cea\u554f\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308b\u3002\u4e00\u65b9\u3067\u3001\u8cea\u554f\u306etoken\u4e0a\u9650\u306b\u9054\u3057\u3066\u3057\u307e\u3046\u53ef\u80fd\u6027\u304c\u3042\u308b\u3002</p>"},{"location":"langchain/15_conversational_retrieval/#types","title":"Types","text":"<ol> <li>BaseConversationalRetrievalChain: Chain for chatting with an index \u3053\u308c\u3092\u57fa\u672c\u306f\u7d99\u627f\u3059\u308b</li> <li>ConversationalRetrievalChain(BaseConversationalRetrievalChain): Chain for having a conversation based on retrieved documents.     <pre><code>This chain takes in chat history (a list of messages) and new questions,\nand then returns an answer to that question.\nThe algorithm for this chain consists of three parts:\n 1. Use the chat history and the new question to create a \"standalone question\".\nThis is done so that this question can be passed into the retrieval step to fetch\nrelevant documents. If only the new question was passed in, then relevant context\nmay be lacking. If the whole conversation was passed into retrieval, there may\nbe unnecessary information there that would distract from retrieval.\n 2. This new question is passed to the retriever and relevant documents are\nreturned.\n 3. The retrieved documents are passed to an LLM along with either the new question\n(default behavior) or the original question and chat history to generate a final\nresponse.\n</code></pre></li> <li>ChatVectorDBChain(BaseConversationalRetrievalChain): Chain for chatting with a vector database.</li> </ol>"},{"location":"langchain/15_conversational_retrieval/#implementation","title":"Implementation","text":"<ul> <li> <p>_call: \u3053\u308c\u304c\u30e1\u30a4\u30f3\u3067\u5b9f\u884c\u3055\u308c\u308b\u95a2\u6570</p> <pre><code>qa =\n</code></pre> </li> </ul>"},{"location":"langchain/15_conversational_retrieval/#components","title":"Components","text":""},{"location":"langchain/15_conversational_retrieval/#1-retrievalqa","title":"1. RetrievalQA","text":"<ol> <li>RetrievalQA(BaseRetrievalQA)<ol> <li>\u521d\u671f\u5316: from_llm\u3067\u3001retriever\u3092\u6e21\u3057\u3066\u521d\u671f\u5316\u3059\u308b (RetrievalQA\u306f\u3001<code>retriever: BaseRetriever = Field(exclude=True)</code> \u3092\u6301\u3063\u3066\u3044\u308b\u3002)</li> <li>Query: <code>qa(query)</code> \u3067\u5b9f\u884c\u3055\u308c\u308b\u3002 BaseRetrievalQA._call\u304c\u547c\u3070\u308c\u308b</li> <li>self._get_docs(question)\u3067Document\u3092<code>retriever.get_relevant_documents</code>\u3067\u53d6\u5f97\u3059\u308b     <pre><code>def _get_docs(\n    self,\n    question: str,\n    *,\n    run_manager: CallbackManagerForChainRun,\n) -&gt; List[Document]:\n    \"\"\"Get docs.\"\"\"\n    return self.retriever.get_relevant_documents(\n        question, callbacks=run_manager.get_child()\n    )\n</code></pre></li> <li><code>_call</code>\u5185\u3067<code>_get_docs</code>\u306e\u5f8c\u306bdocs\u3092<code>combine_documents_chain</code>\u306b\u6e21\u3057\u3066\u3001Docs\u3068Question\u3067\u6700\u7d42\u7684\u306a\u7b54\u3048\u3092\u51fa\u3059\u3002     <pre><code>answer = self.combine_documents_chain.run(\n    input_documents=docs, question=question, callbacks=_run_manager.get_child()\n)\n</code></pre></li> <li>PromptTemplate</li> </ol> </li> <li>ConversationalRetrievalChain<ol> <li>step:<ol> <li>standalone questions\u3092Chat History\u304b\u3089\u4f5c\u6210</li> <li>question\u3092\u4f7f\u3063\u3066\u95a2\u9023Document\u3092VectorStore\u304b\u3089\u53d6\u5f97\u3059\u308b</li> <li>\u53d6\u5f97\u3057\u305fDocument\u3092LLM\u306b(\u65b0\u3057\u3044question\u307e\u305f\u306f\u5143\u3005\u306eQuestion\u3068ChatHistory\u3068\u5171\u306b)\u6e21\u3057\u3066\u6700\u7d42\u7684\u306a\u7b54\u3048\u3092\u4f5c\u6210\u3059\u308b</li> </ol> </li> <li>template: CONDENSE_QUESTION_PROMPT <pre><code>_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n\nChat History:\n{chat_history}\nFollow Up Input: {question}\nStandalone question:\"\"\"\n</code></pre></li> </ol> </li> </ol>"},{"location":"langchain/15_conversational_retrieval/#2-retriever","title":"2. Retriever","text":"<p>RetrievalQA\u306fRetriever\u3092\u4f7f\u3063\u3066Document\u3092\u691c\u7d22\u3059\u308b\u3002</p> <ol> <li>VectorStore\u306f <code>as_retriever()</code>\u3068\u3044\u3046\u95a2\u6570\u3067VectorStoreRetriever\u3092\u53d6\u5f97\u3067\u304d\u308b</li> <li> <p>retriever\u306f\u3001\u691c\u7d22\u306e\u7a2e\u985e\u304c <code>similarity</code>, <code>similarity_score_threshold</code>, <code>mmr</code>\u306e3\u7a2e\u985e\u304c\u3042\u308b (TODO: \u3069\u3046\u3044\u3046\u3068\u304d\u306b\u3069\u308c\u3092\u4f7f\u3046\u304b\u306e\u6bd4\u8f03)</p> </li> <li> <p>\u691c\u7d22\u6642\u306f\u3001RetrievalQA._get_docs\u304c<code>retriever.get_relevant_documents</code>\u3092\u547c\u3076</p> </li> <li> <p>retriever._get_relevant_documents \u304c\u547c\u3070\u308c\u308b</p> <ol> <li>\u5185\u90e8\u3067\u306f <code>self.vectorstore</code>\u306e<ol> <li><code>similarity_search(query, **self.search_kwargs)</code>,</li> <li><code>similarity_search_with_relevance_scores(query, **self.search_kwargs)</code></li> <li><code>max_marginal_relevance_search(query, **self.search_kwargs)</code>\u306e\u3044\u3065\u308c\u304b\u304c\u547c\u3070\u308c\u308b\u3002</li> </ol> </li> </ol> <pre><code>def _get_relevant_documents(\n    self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n) -&gt; List[Document]:\n    if self.search_type == \"similarity\":\n        docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\n    elif self.search_type == \"similarity_score_threshold\":\n        docs_and_similarities = (\n            self.vectorstore.similarity_search_with_relevance_scores(\n                query, **self.search_kwargs\n            )\n        )\n        docs = [doc for doc, _ in docs_and_similarities]\n    elif self.search_type == \"mmr\":\n        docs = self.vectorstore.max_marginal_relevance_search(\n            query, **self.search_kwargs\n        )\n    else:\n        raise ValueError(f\"search_type of {self.search_type} not allowed.\")\n    return docs\n</code></pre> </li> </ol> <p>vectorstore\u304b\u3089\u53d6\u5f97\u3059\u308b\u90e8\u5206\u306f\u3001\u6b21\u306eVectorstore\u3092\u53c2\u7167\u3002</p>"},{"location":"langchain/15_conversational_retrieval/#2-vectorstore","title":"2. Vectorstore","text":"<p>Vectorstore\u306fLangChain\u5185\u3067\u30e9\u30c3\u30d7\u3055\u308c\u305fDeepLake\u3092\u4f7f\u3063\u3066\u3044\u308b\u3002\u304c\u3001DeepLake\u306e\u6a5f\u80fd\u3092\u3059\u3079\u3066\u4f7f\u3044\u305f\u3044\u5834\u5408\u306b\u306f\u3001\u4e2d\u8eab\u3092\u7406\u89e3\u3057\u3066\u9069\u5b9c\u62e1\u5f35\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002 \u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u306f\u3001Score\u304c\u8fd4\u3063\u3066\u3053\u306a\u3044\u306e\u3067\u3001DeepLake\u306eVectorStore\u3092\u76f4\u63a5\u3044\u3058\u308c\u308b\u3088\u3046\u306b\u5909\u66f4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002</p> <ol> <li> <p>LangChain</p> <ol> <li> <p>DeepLake.similarity_search <pre><code>def similarity_search(\n    self,\n    query: str,\n    k: int = 4,\n    **kwargs: Any,\n) -&gt; List[Document]:\n    return self._search(\n        query=query,\n        k=k,\n        use_maximal_marginal_relevance=False,\n        return_score=False,\n        **kwargs,\n    )\n</code></pre></p> <p>_search\u306f\u3001query\u304b\u3089embedding_function\u3092\u4f7f\u3063\u3066embedding\u3092\u8a08\u7b97\u3057\u3066 self.vectorstore.search\u3092\u547c\u3076\u3002 <pre><code>result = self.vectorstore.search(\n    embedding=embedding,\n    k=fetch_k if use_maximal_marginal_relevance else k,\n    distance_metric=distance_metric,\n    filter=filter,\n    exec_option=exec_option,\n    return_tensors=[\"embedding\", \"metadata\", \"text\", \"id\"],\n    deep_memory=deep_memory,\n)\n</code></pre>     1. VectorStore.similarity_search_with_relevance_scores <pre><code>def similarity_search_with_relevance_scores(\n    self,\n    query: str,\n    k: int = 4,\n    **kwargs: Any,\n) -&gt; List[Tuple[Document, float]]:\n    docs_and_similarities = self._similarity_search_with_relevance_scores(\n        query, k=k, **kwargs\n    )\n    ...\n    return docs_and_similarities\n</code></pre> _similarity_search_with_relevance_scores\u3067\u5b9f\u969b\u306e\u8a08\u7b97\u306f\u3059\u308b\u304c\u3001\u8a08\u7b97\u5f0f\u306fSubclass\u3067<code>self._select_relevance_score_fn()</code>\u3067\u5909\u66f4\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002</p> </li> <li> <p>DeepLake.max_marginal_relevance_search <pre><code>return self._search(\n    query=query,\n    k=k,\n    fetch_k=fetch_k,\n    use_maximal_marginal_relevance=True,\n    lambda_mult=lambda_mult,\n    exec_option=exec_option,\n    embedding_function=embedding_function,  # type: ignore\n    **kwargs,\n)\n</code></pre></p> <p>maximum_marginal_relevance\u3067\u3001fetch_k\u53d6\u5f97\u3057\u305f\u5206\u306b\u305f\u3044\u3057\u3066\u8a08\u7b97\u3059\u308b\u3002(TODO:\u8a73\u7d30\u78ba\u8a8d)</p> </li> </ol> </li> </ol> <p>Maximal Marginal Relevance: The idea behind using MMR is that it tries to reduce redundancy and increase diversity in the result and is used in text summarization. (Maximal Marginal Relevance to Re-rank results in Unsupervised KeyPhrase Extraction)</p> <ol> <li>DeepLake: libs/langchain/langchain/vectorstores/deeplake.py<ol> <li>Update dataset</li> <li>Offcial Docs</li> <li>Deep Lake Vector Store API</li> </ol> </li> </ol>"},{"location":"langchain/15_conversational_retrieval/#3-datastore","title":"3. Datastore","text":"<ol> <li>SQLite: For permanent data (Chat History, document source data (Confluence, Directory, Text, etc.), imported document.)</li> </ol>"},{"location":"langchain/15_conversational_retrieval/#4-text-splitter","title":"4. Text Splitter","text":"<ol> <li>Check: https://langchain-text-splitter.streamlit.app/</li> <li>MarkdownHeaderTextSplitter</li> <li>RecursiveCharacterTextSplitter</li> </ol>"},{"location":"langchain/15_conversational_retrieval/#5-document-loader","title":"5. Document Loader","text":"<ol> <li>TextLoader for Directory import</li> <li>ConfluenceLoader for Confluence (Source)<ol> <li>https://developer.atlassian.com/server/confluence/confluence-server-rest-api/</li> <li>https://python.langchain.com/docs/integrations/document_loaders/confluence</li> </ol> </li> <li>GoogleDriveLoader<ol> <li>google.auth package added to langchain in langchain#6035</li> </ol> </li> </ol>"},{"location":"langchain/15_conversational_retrieval/#faq","title":"FAQ","text":"<ol> <li><code>ConversationalRetrievalChain</code> vs. <code>ChatVectorDBChain</code>?</li> <li>retrievalqa\u3092agent\u306etool\u3068\u3057\u3066\u4f7f\u3046\u6642\u306bsource_document\u3092\u8868\u793a\u3059\u308b\u65b9\u6cd5\u306f\uff1f<ol> <li>Tool\u306eInput\u3068Output\u306fString\u3067\u3042\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u3067\u4ee5\u4e0b\u306e\u3088\u3046\u306b source\u3092\u307e\u3068\u3081\u3066\u8fd4\u3059\u95a2\u6570\u3067wrap\u3059\u308b (<code>return_direct=True</code>)     <pre><code>def run_qa_chain(question):\n    results = qa_chain({\"question\":question},return_only_outputs=True)\n    return str(results)\n</code></pre></li> <li><code>_run</code> \u3092\u62e1\u5f35\u3057\u3066Source\u3092\u542b\u3081\u308b (<code>return_direct=True</code>)     <pre><code>def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -&gt; str:\n    \"\"\"Use the tool.\"\"\"\n    retrieval_chain = self.create_retrieval_chain()\n    answer = retrieval_chain(query)['answer']\n    sources = '\\n'.join(retrieval_chain(query)['sources'].split(', '))\n\n    return f'{answer}\\nSources:\\n{sources}'\n</code></pre></li> </ol> </li> </ol>"},{"location":"langchain/15_conversational_retrieval/#ref","title":"Ref","text":"<ol> <li>RetrievalQA chain return source_documents when using it as a Tool for an Agent #5097</li> <li>Vector store agent with sources #4187</li> </ol>"},{"location":"langchain/16_documents_loader/","title":"Documents Loader","text":"<ol> <li>TextLoader</li> <li>UnstructuredMarkdownLoader<ol> <li>https://unstructured-io.github.io/unstructured/bricks.html#partition-md</li> <li>https://unstructured-io.github.io/unstructured/bricks/cleaning.html</li> </ol> </li> <li>ConfluenceLoader: 1Page\u30921Document\u3068\u3057\u3066\u8aad\u307f\u8fbc\u3080\u306e\u3067\u3001Split\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b</li> </ol>"},{"location":"langchain/19_testing/","title":"Testing","text":"<ol> <li>FakeLLMList <pre><code>from langchain.llms.fake import FakeListLLM\n</code></pre></li> <li>FakeChatModel     \u81ea\u5206\u3067\u5b9a\u7fa9\u3057\u3066\u4f7f\u3046     tests/fake_chat_model.py\u3092\u53c2\u7167</li> <li> <p>FakeEmbeddings <pre><code>from langchain.embeddings import FakeEmbeddings\nembeddings = FakeEmbeddings(size=1352)\n</code></pre></p> <p>doc</p> </li> </ol>"},{"location":"langchain/README_21_memory/","title":"Memory","text":"<p>Langchain\u306eMemory\u6a5f\u80fd\u306e\u899a\u3048\u66f8\u304d &lt;- \u5168\u90e8\u306e\u7a2e\u985e\u306e\u30b5\u30de\u30ea\u30fc\u3068\u3057\u3066\u306f\u3068\u3066\u3082\u307e\u3068\u307e\u3063\u3066\u3044\u3066\u7d20\u6674\u3089\u3057\u3044</p> <ol> <li>ConversationBufferMemory<ol> <li>\u4f1a\u8a71\u306e\u5c65\u6b74\u3092\u4fdd\u6301\u3059\u308b</li> <li>\u4f1a\u8a71\u306e\u5c65\u6b74\u3092\u5168\u3066\u4fdd\u6301\u3059\u308b\u305f\u3081\u3001\u4f1a\u8a71\u304c\u9577\u304f\u306a\u308b\u3068\u30d7\u30ed\u30f3\u30d7\u30c8\u304c\u81a8\u5927\u306b\u306a\u308b\u3002</li> </ol> </li> <li>ConversationBufferWindowMemory<ol> <li>\u76f4\u8fd1k\u500b\u306e\u4f1a\u8a71\u5c65\u6b74\u306e\u307f\u4fdd\u6301\u3059\u308b</li> <li>\u53e4\u3044\u4f1a\u8a71\u304c\u524a\u9664\u3055\u308c\u308b\u305f\u3081\u3001\u30d7\u30ed\u30f3\u30d7\u30c8\u304c\u81a8\u5927\u306b\u306a\u308b\u554f\u984c\u304c\u89e3\u6c7a\u3059\u308b\u3002</li> </ol> </li> <li>Entity Memory<ol> <li>\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u306b\u95a2\u3057\u3066\u306e\u307f\u30e1\u30e2\u30ea\u3092\u4fdd\u6301\u3059\u308b</li> </ol> </li> <li>Conversation Knowledge Graph Memory<ol> <li>\u30ca\u30ec\u30c3\u30b8\u30b0\u30e9\u30d5\u3092\u4f5c\u3063\u3066\u4f1a\u8a71\u306b\u3064\u3044\u3066\u306e\u60c5\u5831\u3092\u4fdd\u6301\u3059\u308b</li> </ol> </li> <li>ConversationSummaryMemory<ol> <li>\u904e\u53bb\u4f1a\u8a71\u3092\u8981\u7d04\u3057\u3066\u4fdd\u6301\u3059\u308b</li> <li>\u8981\u7d04\u306f\u82f1\u8a9e\u3067\u884c\u308f\u308c\u308b</li> </ol> </li> <li>ConversationSummaryBufferMemory<ol> <li>\u904e\u53bb\u4f1a\u8a71\u306e\u8981\u7d04\u3068\u3001\u76f4\u8fd1\u306e\u4f1a\u8a71\uff08\u30c8\u30fc\u30af\u30f3\u6570\u3067\u6307\u5b9a\uff09\u3092\u4fdd\u6301\u3059\u308b</li> </ol> </li> <li>ConversationTokenBufferMemory</li> <li>ConversationBufferWindowMemory\u306e\u30c8\u30fc\u30af\u30f3\u7248\u3002\u76f4\u8fd1\u306ek\u4f1a\u8a71\u3067\u306f\u306a\u304f\u3001\u76f4\u8fd1\u306en\u30c8\u30fc\u30af\u30f3\u3092\u4fdd\u6301\u3059\u308b\u3002</li> <li>VectorStore-Backed Memory<ol> <li>\u904e\u53bb\u4f1a\u8a71\u3092\u30d9\u30af\u30bf\u30fc\u30b9\u30c8\u30a2\u306b\u4fdd\u6301\u3057\u3001\u95a2\u9023\u3059\u308b\u4f1a\u8a71\u306e\u307f\u63a2\u7d22\u3057\u3066\u7528\u3044\u308b</li> <li>\u4f3c\u3066\u3044\u308b\u4f1a\u8a71\u304c\u7121\u3044\u5834\u5408\u306f\u3001\u540c\u3058\u3082\u306e\u3092\u8907\u6570\u8868\u793a\u3059\u308b\u306a\u3069\u6319\u52d5\u304c\u602a\u3057\u3044\u70b9\u304c\u3042\u308b</li> </ol> </li> <li><code>ReadOnlySharedMemory</code>: Read-only\u3067\u8ffd\u52a0\u306f\u3057\u306a\u3044</li> </ol>"},{"location":"langchain/README_21_memory/#usage","title":"Usage","text":"<pre><code>from langchain.memory import ConversationBufferMemory\nmemory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n\nagent_kwargs = {\n    \"extra_prompt_messages\": [MessagesPlaceholder(variable_name=\"history\")],\n}\nmrkl = initialize_agent(\n    tools, llm, agent=AgentType.OPENAI_FUNCTIONS, agent_kwargs=agent_kwargs, memory=memory, verbose=True\n)\n</code></pre> <p>\u3053\u308c\u3067Agent\u304c\u547c\u3070\u308c\u308b\u305f\u3073\u306b\u3001<code>memory_key</code>\u306b\u6307\u5b9a\u3057\u305f\u540d\u524d\u306ekey\u306b\u3001\u4f1a\u8a71\u306e\u5c65\u6b74\u304c\u8ffd\u52a0\u3055\u308c\u308b\u3002\u6bce\u56deLLM\u306b\u6e21\u3059prompt\u306b\u306f\u3001<code>extra_prompt_messages</code>\u3067\u6307\u5b9a\u3057\u305f<code>MessagesPlaceholder</code>\u304c\u8ffd\u52a0\u3055\u308c\u308b\u3002</p> <p>Prompt\u304c\u80a5\u5927\u5316\u3057\u3084\u3059\u3044\u30c7\u30e1\u30ea\u30c3\u30c8\u3082\u3042\u308b\u306e\u3067\u6ce8\u610f\u3002</p>"},{"location":"langchain/README_21_memory/#implementation","title":"Implementation","text":"<p>Chain.__call__\u5185\u3067\u3001</p> <ol> <li> <p>prep_inputs\u3067memory\u304b\u3089\u306emessage\u3092prompt\u306b\u6ce8\u5165\u3057\u3066\u3044\u308b<code>self.memory.load_memory_variables(inputs)</code></p> <pre><code>def prep_inputs(self, inputs: Union[Dict[str, Any], Any]) -&gt; Dict[str, str]:\n    \"\"\"Validate and prepare chain inputs, including adding inputs from memory.\n\n    Args:\n        inputs: Dictionary of raw inputs, or single input if chain expects\n            only one param. Should contain all inputs specified in\n            `Chain.input_keys` except for inputs that will be set by the chain's\n            memory.\n\n    Returns:\n        A dictionary of all inputs, including those added by the chain's memory.\n    \"\"\"\n    if not isinstance(inputs, dict):\n        _input_keys = set(self.input_keys)\n        if self.memory is not None:\n            # If there are multiple input keys, but some get set by memory so that\n            # only one is not set, we can still figure out which key it is.\n            _input_keys = _input_keys.difference(self.memory.memory_variables)\n        if len(_input_keys) != 1:\n            raise ValueError(\n                f\"A single string input was passed in, but this chain expects \"\n                f\"multiple inputs ({_input_keys}). When a chain expects \"\n                f\"multiple inputs, please call it by passing in a dictionary, \"\n                \"eg `chain({'foo': 1, 'bar': 2})`\"\n            )\n        inputs = {list(_input_keys)[0]: inputs}\n    if self.memory is not None:\n        external_context = self.memory.load_memory_variables(inputs)\n        inputs = dict(inputs, **external_context)\n    self._validate_inputs(inputs)\n    return inputs\n</code></pre> </li> <li> <p>prep_output\u3067\u30e1\u30e2\u30ea\u30fc\u306b\u4fdd\u5b58\u3055\u308c\u308b</p> <pre><code>def prep_outputs(\n    self,\n    inputs: Dict[str, str],\n    outputs: Dict[str, str],\n    return_only_outputs: bool = False,\n) -&gt; Dict[str, str]:\n    \"\"\"Validate and prepare chain outputs, and save info about this run to memory.\n\n    Args:\n        inputs: Dictionary of chain inputs, including any inputs added by chain\n            memory.\n        outputs: Dictionary of initial chain outputs.\n        return_only_outputs: Whether to only return the chain outputs. If False,\n            inputs are also added to the final outputs.\n\n    Returns:\n        A dict of the final chain outputs.\n    \"\"\"\n    self._validate_outputs(outputs)\n    if self.memory is not None:\n        self.memory.save_context(inputs, outputs)\n    if return_only_outputs:\n        return outputs\n    else:\n        return {**inputs, **outputs}\n</code></pre> </li> </ol>"},{"location":"langchain/cache/","title":"Caching","text":"<pre><code>from langchain.cache import SQLiteCache\nset_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n</code></pre> <p>callback\u3092\u4f7f\u3063\u3066\u3044\u308b\u3068\u304d\u306b\u306f\u3001BaseCallbackHandler.on_llm_end\u3067\u5024\u3092\u53d6\u5f97\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002</p> <p>streamlit\u3067Streaming\u304b\u3064Cache\u3092\u4f7f\u3063\u3066\u3044\u308b\u5834\u5408 StreamlitCallbackHandler</p>"},{"location":"langchain/documentschain/","title":"DocumentsChain","text":"<p>Use given Documents as context for the LLM chain to answer a question.</p>"},{"location":"langchain/documentschain/#stuffdocumentschain","title":"StuffDocumentsChain","text":"<p>StuffDocumentsChain just stuffs the documents into the prompt as context without modifying the contents of the documents.</p> <p>default prompt</p> <pre><code>Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n\n{context}\n\nQuestion: {question}\nHelpful Answer:\n</code></pre> <p>Notes</p> <p>Actual contents of <code>{context}</code> would be generated in the following way:</p> <ol> <li>The retrieved Documents are joined together with a separator after formatting with <code>format_document</code>.</li> <li>Return a Dict with the key <code>context</code> and the value as the joined Documents.</li> </ol> <pre><code># Format each document according to the prompt\ndoc_strings = [format_document(doc, self.document_prompt) for doc in docs]\n# Join the documents together to put them in the prompt.\ninputs = {\n    k: v\n    for k, v in kwargs.items()\n    if k in self.llm_chain.prompt.input_variables\n}\ninputs[self.document_variable_name] = self.document_separator.join(doc_strings)\n</code></pre> <p>Ref: StuffDocumentsChain</p> <pre><code>document_prompt = PromptTemplate(\n    input_variables=[\"page_content\"],\n    template=\"{page_content}\",\n)\n\ncombine_documents_chain = StuffDocumentsChain(\n    llm_chain=llm_chain,\n    document_variable_name=\"context\",\n    document_prompt=document_prompt,\n    callbacks=None,\n)\n</code></pre>"},{"location":"langchain/documentschain/#refinedocumentschain","title":"RefineDocumentsChain","text":""},{"location":"langchain/documentschain/#mapreducedocumentschain","title":"MapReduceDocumentsChain","text":""},{"location":"langchain/documentschain/#ref","title":"Ref","text":"<ol> <li>https://nakamasato.medium.com/enhancing-langchains-retrievalqa-for-real-source-links-53713c7d802a</li> <li>Retrieval</li> </ol>"},{"location":"langchain/gemini/","title":"Gemini","text":"<p>ChatGoogleGenerativeAI</p>"},{"location":"langchain/gemini/#pricing","title":"Pricing","text":"<p>60 QPM (queries per minute)</p> <p>Details: https://ai.google.dev/pricing</p>"},{"location":"langchain/gemini/#set-up-api-key","title":"Set up API Key","text":"<ol> <li> <p>Set project id     <pre><code>PROJECT=xxx\nKEY_ID=gemini-api-key\n</code></pre></p> </li> <li> <p>Enable service <code>generativelanguage.googleapis.com</code></p> <pre><code>gcloud services list --available --project $PROJECT --filter 'generativelanguage'\nNAME                               TITLE\ngenerativelanguage.googleapis.com  Generative Language API\n</code></pre> <pre><code>gcloud services enable generativelanguage.googleapis.com --project $PROJECT\n</code></pre> </li> <li> <p>Set alias (ref)</p> <pre><code>alias gcurl='curl -H \"Authorization: Bearer $(gcloud auth print-access-token)\" -H \"Content-Type: application/json\"'\n</code></pre> </li> <li> <p>Create API key with API target restriction.</p> <pre><code>DISPLAY_NAME=\"Gemini API Key\"\ngcloud services api-keys create --display-name \"$DISPLAY_NAME\" --project $PROJECT --api-target=service=generativelanguage.googleapis.com\n</code></pre> </li> <li> <p>Check API Key     <pre><code>gcloud services api-keys list --project $PROJECT --format json | jq \".[] | select(.displayName == \\\"$DISPLAY_NAME\\\")\"\n{\n  \"createTime\": \"2024-02-23T12:58:04.334290Z\",\n  \"displayName\": \"Gemini API Key\",\n  \"etag\": \"W/\\\"dMMHgHoJFaTkh6z89v139Q==\\\"\",\n  \"name\": \"projects/xxxxx/locations/global/keys/xxxxxxx\",\n  \"restrictions\": {\n    \"apiTargets\": [\n      {\n        \"service\": \"generativelanguage.googleapis.com\"\n      }\n    ]\n  },\n  \"uid\": \"xxxxxxxxxxxxxxx\",\n  \"updateTime\": \"2024-02-23T12:58:04.358444Z\"\n}\n</code></pre></p> </li> <li> <p>Get Key string</p> <pre><code>gcloud services api-keys get-key-string projects/xxxxxxx/locations/global/keys/xxxxxxxxxxxxxxxx --project $PROJECT\nkeyString: XXXXXXXXXXXXXXXXXXXXX\n</code></pre> <p>Set this value in <code>.env</code></p> </li> </ol>"},{"location":"langchain/gemini/#ref","title":"Ref","text":"<ol> <li>https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini</li> <li>https://cloud.google.com/api-keys/docs/add-restrictions-api-keys</li> <li>https://ai.google.dev/pricing</li> <li>https://makersuite.google.com/app/</li> <li>[langchain] ChatGoogleGenerativeAI</li> <li>https://qiita.com/nakamasato/items/b8d76da6751df8d7bddc</li> </ol>"},{"location":"langchain/retrieval/","title":"Retrieval","text":""},{"location":"langchain/retrieval/#retrievalqa","title":"RetrievalQA","text":"<p><code>question</code> -&gt; <code>DocumentsChain</code> -&gt; <code>answer</code></p>"},{"location":"langchain/retrieval/#conversationalretrieval","title":"ConversationalRetrieval","text":"<p><code>question</code> -&gt; <code>chat_history</code> -&gt; <code>generated_question</code> -&gt; <code>DocumentsChain</code> -&gt; <code>answer</code></p>"},{"location":"langchain/retrieval/#ref","title":"Ref","text":"<ol> <li>DocumentsChain</li> </ol>"},{"location":"langchain/testing/","title":"Testing","text":""},{"location":"langchain/testing/#fakelistllm","title":"FakeListLLM","text":"<pre><code>from langchain.llms.fake import FakeListLLM\n\nresponses = [\n    '{\"action\": \"google-search\",\"action_input\": \"current Japan\\'s prime minister\"}',\n    '{\"action\": \"Final Answer\", \"action_input\": \"\u73fe\u5728\u306e\u65e5\u672c\u306e\u7dcf\u7406\u5927\u81e3\u306f\u5cb8\u7530\u6587\u96c4\u3067\u3059\u3002\"}',\n]\nllm = FakeListLLM(responses=responses)\n</code></pre>"},{"location":"langchain/agent/","title":"Agent","text":"<ul> <li>Concepts</li> </ul> <pre><code>next_action = agent.get_action(...)\nwhile next_action != AgentFinish:\n    observation = run(next_action)\n    next_action = agent.get_action(..., next_action, observation)\nreturn next_action\n</code></pre>"},{"location":"langchain/agent/#1-getting-started","title":"1. Getting Started","text":"<ol> <li> <p>Create a tool that splits a given string with a comma and muliply them.</p> <pre><code>def multiplier(a, b):\n    return a * b\n\n\ndef parsing_multiplier(string):\n    a, b = string.split(\",\")\n    return multiplier(int(a), int(b))\n</code></pre> </li> <li> <p>Write the usage in the description.</p> <pre><code>Tool(\n    name = \"Multiplier\",\n    func=parsing_multiplier,\n    description=\"useful for when you need to multiply two numbers together. The input to this tool should be a comma separated list of numbers of length two, representing the two numbers you want to multiply together. For example, `1,2` would be the input if you wanted to multiply 1 by 2.\"\n)\n</code></pre> </li> <li> <p>Prepare Prompt</p> <pre><code>prompt = hub.pull(\"hwchase17/react-chat\")\n</code></pre> <p>langchainhub\u3092\u3084\u3081\u305f</p> </li> <li> <p>Create Agent</p> <pre><code>agent = create_react_agent(\n    llm=llm,\n    tools=tools,\n    prompt=prompt,\n)\n</code></pre> </li> <li> <p>Create AgentExecutor</p> <pre><code>agent_executor = AgentExecutor(\n    agent=agent,\n    tools=tools,\n)\n</code></pre> </li> <li> <p>Invoke</p> <pre><code>agent_executor.invoke({\"input\": \"3 multiplied by 4 is?\"})\n</code></pre> </li> </ol>"},{"location":"langchain/agent/#2-list","title":"2. List","text":"llm create agent func prompt output parser link <code>OpenAI()</code> create_react_agent hwchase17/react-chat <code>ReActSingleInputOutputParser</code> link <code>ChatOpenAI(model=\"gpt-3.5-turbo-1106\")</code> create_structured_chat_agent hwchase17/structured-chat-agent <code>JSONAgentOutputParser</code> link <code>ChatOpenAI(model=\"gpt-3.5-turbo-1106\")</code> create_openai_tools_agent hwchase17/openai-tools-agent <code>OpenAIToolsAgentOutputParser</code> link <code>gpt-3.5-turbo-1106</code>, <code>gpt-4-0613</code> create_openai_functions_agent hwchase17/openai-functions-agent <code>OpenAIFunctionsAgentOutputParser</code> link <p>For more details about prompt, please read prompt.</p> <p>Which agent type to use:</p> <ol> <li>Structured Agent: you can use tools with mutliple inputs</li> <li>OpenAI Functions vs. OpenAI Tools:</li> <li>React:</li> </ol>"},{"location":"langchain/agent/#3-examples","title":"3. Examples","text":""},{"location":"langchain/agent/#31-react-normal-create_react_agent","title":"3.1. React Normal <code>create_react_agent</code>","text":"<p>https://python.langchain.com/docs/modules/agents/agent_types/react</p> <pre><code>from langchain_community.utilities import GoogleSearchAPIWrapper\nfrom langchain_openai import OpenAI\n\nimport langchain\n\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, Tool, create_react_agent\nfrom langchain.memory import ConversationBufferMemory\n\nlangchain.debug = False\n\n\ndef multiplier(a, b):\n    return a * b\n\n\ndef parsing_multiplier(string):\n    a, b = string.split(\",\")\n    return multiplier(int(a), int(b))\n\n\ngoogle = GoogleSearchAPIWrapper()\n\n\ndef top5_results(query):\n    return google.results(query, 5)\n\n\ntools = [\n    Tool(\n        name=\"Multiplier\",\n        func=parsing_multiplier,\n        description=(\n            \"useful for when you need to multiply two numbers together. \"\n            \"The input to this tool should be a comma separated list of numbers of length two, representing the two numbers you want to multiply together. \"\n            \"For example, `1,2` would be the input if you wanted to multiply 1 by 2.\"\n        ),\n    ),\n    Tool(\n        name=\"google-search\",\n        description=\"Search Google for recent results.\",\n        func=top5_results,\n    ),\n]\n\n\ndef main():\n\n    memory = ConversationBufferMemory(\n        memory_key=\"chat_history\",\n        return_messages=True,\n    )\n\n    llm = OpenAI(temperature=0)\n\n    prompt = hub.pull(\"hwchase17/react-chat\")\n    print(prompt)\n\n    agent = create_react_agent(\n        llm=llm,\n        tools=tools,\n        prompt=prompt,\n    )\n    print(\n        agent.invoke({\"input\": \"Who is Japan's prime minister?\", \"intermediate_steps\": [], \"chat_history\": []})\n    )  # return AgentAction or AgentFinish\n\n    agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True, handle_parsing_errors=False)\n\n    print(agent_executor.invoke({\"input\": \"3 multiplied by 4 is?\"}))\n    print(agent_executor.invoke({\"input\": \"5 x 4 is ?\"}))\n\n    print(agent_executor.invoke({\"input\": \"Who is Japan's prime minister?\"}))\n    print(agent_executor.invoke({\"input\": \"\u80fd\u767b\u534a\u5cf6\u306e\u5730\u9707\u306e\u72a0\u7272\u8005\u306f\u4f55\u4eba\u3067\u3059\u304b\"}))\n\n    print(memory)\n</code></pre> <pre><code>poetry run python src/examples/agent_react_normal.py\n</code></pre> <pre><code>&gt; Entering new AgentExecutor chain...\nThought: Do I need to use a tool? Yes\nAction: Multiplier\nAction Input: 3,412Do I need to use a tool? No\nFinal Answer: 3 multiplied by 4 is 12.\n\n&gt; Finished chain.\n{'input': '3 multiplied by 4 is?', 'chat_history': [HumanMessage(content='3 multiplied by 4 is?'), AIMessage(content='3 multiplied by 4 is 12.')], 'output': '3 multiplied by 4 is 12.'}\n</code></pre> <pre><code>agent_google = create_react_agent(\n    llm=llm,\n    tools=tools_google,\n    prompt=prompt_google,\n)\n</code></pre>"},{"location":"langchain/agent/#32-structured-create_structured_chat_agent","title":"3.2. Structured <code>create_structured_chat_agent</code>","text":"<ul> <li>https://python.langchain.com/docs/modules/agents/agent_types/structured_chat</li> <li>https://python.langchain.com/docs/modules/agents/tools/custom_tools#structuredtool-dataclass</li> <li>agent_structured.py</li> </ul> <pre><code>def multiplier(a, b):\n    return a * b\n\n...\n\n    tools = [\n        StructuredTool.from_function(\n            func=multiplier,\n            name=\"Multiplier\",\n            description=(\n                \"useful for when you need to multiply two numbers together. \"\n                \"The input to this tool is two numbers you want to multiply together. \"\n                \"For example, (1, 2) would be the input if you wanted to multiply 1 by 2.\"\n            ),\n        ),\n        ...\n    ]\n</code></pre> <pre><code>agent_google = create_react_agent(\n    llm=llm,\n    tools=tools_google,\n    prompt=prompt,\n)\nagent_executor_google = AgentExecutor(\n    agent=agent_google,\n    tools=tools_google,\n    # memory=memory,\n    verbose=True,\n    handle_parsing_errors=False,\n)\n\n\ndef search_google_with_agent(query):\n    return agent_executor_google.invoke({\"input\": query})[\"output\"]\n\n...\n\ntools = [\n    ...\n    Tool(\n        name=\"google\",\n        description=\"Search Google for recent results.\",\n        func=search_google_with_agent,\n    ),\n]\n\nagent = create_structured_chat_agent(llm, tools, prompt)\n</code></pre>"},{"location":"langchain/agent/#4-implementation","title":"4. Implementation","text":""},{"location":"langchain/agent/#41-create_react_agent","title":"4.1. create_react_agent","text":"<p>The function <code>create_react_agent</code> just combines the <code>llm</code>, <code>tools</code>, <code>prompt</code>, <code>output_parser</code> and <code>tools_renderer</code> with LCEL.</p> <ol> <li>An agent is just a <code>Runnable</code>.</li> <li> <p>The key part is <code>llm_with_stop = llm.bind(stop=[\"\\nObservation\"])</code>.</p> <pre><code>def create_react_agent(\n    llm: BaseLanguageModel,\n    tools: Sequence[BaseTool],\n    prompt: BasePromptTemplate,\n    output_parser: Optional[AgentOutputParser] = None,\n    tools_renderer: ToolsRenderer = render_text_description,\n) -&gt; Runnable:\n    missing_vars = {\"tools\", \"tool_names\", \"agent_scratchpad\"}.difference(\n        prompt.input_variables\n    )\n    if missing_vars:\n        raise ValueError(f\"Prompt missing required variables: {missing_vars}\")\n\n    prompt = prompt.partial(\n        tools=tools_renderer(list(tools)),\n        tool_names=\", \".join([t.name for t in tools]),\n    )\n    llm_with_stop = llm.bind(stop=[\"\\nObservation\"])\n    output_parser = output_parser or ReActSingleInputOutputParser()\n    agent = (\n        RunnablePassthrough.assign(\n            agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n        )\n        | prompt\n        | llm_with_stop\n        | output_parser\n    )\n    return agent\n</code></pre> <ol> <li>Th prompt must have <code>tools</code>, <code>tool_names</code> and <code>agent_scratchpad</code> as input variables.<ol> <li>we ususally use a promp from <code>hub.pull(\"hwchase17/react-chat\")</code> which has these variables. (see prompt)</li> </ol> </li> <li>The agent needs to be called with <code>intermediate_steps</code> and <code>chat_history</code> as inputs.     <pre><code>agent.invoke({\"input\": \"Who is Japan's prime minister?\", \"intermediate_steps\": [], \"chat_history\": []})\n</code></pre></li> <li><code>AgentExecutor</code> is used to run the agent.</li> </ol> <pre><code>agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True, handle_parsing_errors=False)\n</code></pre> <p>For more details, please read AgentExecutor.</p> </li> </ol>"},{"location":"langchain/agent/#5-ref","title":"5. Ref","text":"<ol> <li>Structured chat: capable of using multi-input tools</li> <li>Defining Custom Tools</li> <li>agent_structured.py</li> <li>AgentExecutor</li> <li>Agent</li> <li>ReadOnlySharedMemory</li> <li>Custom agent with tool retrieval: Tool\u306eDescription\u306eEmbedding\u3092\u4f5c\u3063\u3066\u3001Vector\u691c\u7d22\u3067\u4f7f\u3046\u30c4\u30fc\u30eb\u3092\u9078\u629e\u3059\u308b</li> </ol>"},{"location":"langchain/agent/02_react_docstore/","title":"02 react docstore","text":""},{"location":"langchain/agent/02_react_docstore/#react-reasonact-prompting-in-openai-gpt-and-langchain","title":"ReAct (Reason+Act) prompting in OpenAI GPT and LangChain","text":""},{"location":"langchain/agent/02_react_docstore/#run","title":"Run","text":"<pre><code>poetry run python src/langchain/react_docstore.py\n</code></pre>"},{"location":"langchain/agent/02_react_docstore/#example-author-david-chanoff-has-collaborated-with-a-us-navy-admiral-who-served-as-the-ambassador-to-the-united-kingdom-under-which-president","title":"Example: Author David Chanoff has collaborated with a U.S. Navy admiral who served as the ambassador to the United Kingdom under which President?","text":""},{"location":"langchain/agent/02_react_docstore/#example","title":"Example: \u5cb8\u7530\u7dcf\u7406\u304c\u6f14\u8aac\u4e2d\u306b\u7206\u5f3e\u3092\u6295\u3052\u8fbc\u307e\u308c\u305f\u306e\u306f\u3044\u3064?","text":"<p>\u7b54\u3048\u306f\u9593\u9055\u3063\u3066\u3057\u307e\u3063\u305f\u3002</p> <p>Wikipedia\u306b\u306f\u3001\u4ee5\u4e0b\u304c\u3042\u308b\u304b\u3089\u53d6\u3063\u3066\u304d\u3066\u307b\u3057\u304b\u3063\u305f\u3002</p> <p>On 15 April 2023, a man threw a cylindrical explosive at Kishida shortly before he was due to make a campaign speech in Wakayama.</p>"},{"location":"langchain/agent/03_react_custom/","title":"03 react custom","text":""},{"location":"langchain/agent/03_react_custom/#custom-react-reasonact","title":"Custom ReAct (Reason+Act)","text":"<ol> <li>AgentExecutor\u3067\u5b9f\u884c\u958b\u59cb\u3001\u7d42\u4e86\u6761\u4ef6\u306b\u9054\u3059\u308b\u307e\u3067Loop\u3092\u56de\u3057\u3001Loop\u5185\u3067Agent.plan\u3092\u547c\u3076\u3002</li> <li>Agent.plan\u3067\u306fLLM\u306b\u3069\u306eAction\u3092\u53d6\u308b\u3079\u304d\u304b\u805e\u304d\u5e30\u3063\u3066\u304d\u305fOutput\u3092Parse\u3057\u3066\u3001AgentFinish\u306e\u5834\u5408\u306f\u7d42\u308f\u308a\u3001AgentAction\u306e\u5834\u5408\u306f\u3001Input\u3068\u3068\u3082\u306bTool\u3092\u5b9f\u884c\u3057\u3001\u305d\u306e\u7d50\u679c\u3092\u8fd4\u3059\u3002</li> <li><code>Action</code> (Tool\u306e\u4e2d\u304b\u3089\u4f55\u3092\u4f7f\u3046\u3079\u304d\u304b\u306e\u53f8\u4ee4), <code>Thought</code> (LLM\u304c\u3069\u306eAction\u3092\u53d6\u308b\u3079\u304d\u304b\u8fd4\u3057\u3066\u304d\u305f\u3082\u306e), <code>Observation</code> (Tool\u306e\u7d50\u679c), <code>Finish</code> (\u6700\u7d42\u7684\u306a\u7b54\u3048) \u306f\u3001Prompt\u3084Example\u5185\u3067\u4f7f\u308f\u308c\u308b\u3060\u3051\u3067\u306a\u304f\u3001OutputParser\u3084Agent\u5185\u306e<code>finish_tool_name</code>, <code>observation_prefix</code>, <code>llm_prefix</code>\u3067\u5b9a\u7fa9\u3055\u308c\u308b (\u65e5\u672c\u8a9e\u306b\u3059\u3079\u3066\u5909\u3048\u308b\u3053\u3068\u3082\u53ef\u80fd #19)</li> </ol>"},{"location":"langchain/agent/03_react_custom/#1-run","title":"1. Run","text":"<pre><code>poetry run python src/langchain/react_custom.py\n</code></pre> <ol> <li>\u7dd1: LLM\u304b\u3089\u306e\u30a2\u30a6\u30c8\u30d7\u30c3\u30c8</li> <li>\u9ec4\u8272\u3001\u30d4\u30f3\u30af\uff1aTool\u306e\u7d50\u679c</li> </ol>"},{"location":"langchain/agent/03_react_custom/#2-components","title":"2. Components","text":""},{"location":"langchain/agent/03_react_custom/#21-tools","title":"2.1. Tools","text":"<p>\u30c4\u30fc\u30eb\u30ea\u30b9\u30c8\u3082\u3042\u308b: https://python.langchain.com/en/latest/modules/agents/tools/getting_started.html</p> <p>Custom Tools:</p> <p>Tool consists of several components:</p> <ol> <li><code>name (str)</code>, is required and must be unique within a set of tools provided to an agent</li> <li><code>description (str)</code>, is optional but recommended, as it is used by an agent to determine tool use</li> <li><code>return_direct (bool)</code>, defaults to False</li> <li><code>args_schema (Pydantic BaseModel)</code>, is optional but recommended, can be used to provide more information (e.g., few-shot examples) or validation for expected parameters.</li> </ol> <p>\u4f5c\u308a\u65b91: Tool:</p> <ol> <li> <p>Tool dataclass</p> <pre><code>Tool.from_function(\n    func=search.run,\n    name = \"Search\",\n    description=\"useful for when you need to answer questions about current events\"\n    # coroutine= ... &lt;- you can specify an async method if desired as well\n),\n</code></pre> </li> <li> <p>Subclassing the BaseTool class</p> <p>```py class CustomSearchTool(BaseTool):     name = \"custom_search\"     description = \"useful for when you need to answer questions about current events\"</p> <pre><code>def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -&gt; str:\n    \"\"\"Use the tool.\"\"\"\n    return search.run(query)\n\nasync def _arun(self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None) -&gt; str:\n    \"\"\"Use the tool asynchronously.\"\"\"\n    raise NotImplementedError(\"custom_search does not support async\")\n</code></pre> <p>````</p> </li> <li> <p>Using @tool decorator</p> <pre><code>from langchain.tools import tool\n\n@tool(\"search\", return_direct=True)\ndef search_api(query: str) -&gt; str:\n    \"\"\"Searches the API for the query.\"\"\"\n    return \"Results\"\n\nsearch_api\n</code></pre> </li> </ol> <p>\u4f5c\u308a\u65b92: Structured Tool:</p> <ol> <li> <p>Structured Tool</p> <pre><code>def post_message(url: str, body: dict, parameters: Optional[dict] = None) -&gt; str:\n    \"\"\"Sends a POST request to the given url with the given body and parameters.\"\"\"\n    result = requests.post(url, json=body, params=parameters)\n    return f\"Status: {result.status_code} - {result.text}\"\n\ntool = StructuredTool.from_function(post_message)\n</code></pre> </li> <li> <p>Subclassing BaseTool</p> </li> <li>Using decorator</li> </ol> <p>Others</p> <ol> <li> <p>Priorities of tools</p> <p>Description\u306b<code>Use this more than the normal search if the question is about Music, like 'who is the singer of yesterday?' or 'what is the most popular song in 2022?'</code>\u306e\u3088\u3046\u306b\u66f8\u304f\u3002Default\u3067\u306fnormal tool\u304c\u512a\u5148\u3055\u308c\u308b 1. return_direct=True: Tool\u306e\u7d50\u679c\u304c\u305d\u306e\u307e\u307e\u8fd4\u3055\u308c\u308b (LLM\u306a\u3057\u3067\uff1f)</p> </li> </ol>"},{"location":"langchain/agent/03_react_custom/#22-agents","title":"2.2. Agents","text":""},{"location":"langchain/agent/03_react_custom/#221-agent-types","title":"2.2.1. Agent Types","text":"<ol> <li>zero-shot-react-description: This agent uses the ReAct framework to determine which tool to use based solely on the tool\u2019s description. Any number of tools can be provided. This agent requires that a description is provided for each tool.</li> <li>react-docstore: This agent uses the ReAct framework to interact with a docstore. Two tools must be provided: a Search tool and a Lookup tool (REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS)</li> <li>self-ask-with-search:  utilizes a single tool that should be named <code>Intermediate Answer</code>. (MEASURING AND NARROWING THE COMPOSITIONALITY GAP IN LANGUAGE MODELS)<ul> <li>the term compositionality gap to describe the fraction of compositional questions that the model answers incorrectly out of all the compositional questions for which the model answers the sub-questions correctly</li> </ul> </li> <li>conversational-react-description:</li> </ol>"},{"location":"langchain/agent/03_react_custom/#222-agent-classes","title":"2.2.2. Agent Classes","text":"<ol> <li>BaseSingleActionAgent: tbd</li> <li>LLMSingleActionAgent: BaseSingleActionAgent\u306e\u62e1\u5f35</li> <li>Agent: BaseSingleActionAgent\u306e\u62e1\u5f35<ol> <li>use output parsers in agents#2987\u304b\u3089OutputParser\u304c\u5c0e\u5165\u3055\u308c\u305f</li> </ol> </li> </ol> <p>Agent\u306e\u4ee5\u4e0b\u306e\u95a2\u6570\u3067finish, observation, prefix\u306e\u540d\u524d\u3092\u6307\u5b9a\u3059\u308b\u3002</p> <pre><code>    @property\n    def finish_tool_name(self) -&gt; str:\n        return \"Finish\"\n\n    @property\n    def observation_prefix(self) -&gt; str:\n        return \"Observation: \"\n\n    @property\n    def llm_prefix(self) -&gt; str:\n        return \"Thought: \"\n</code></pre>"},{"location":"langchain/agent/03_react_custom/#23-toolkikt","title":"2.3. Toolkikt","text":"<ol> <li>CSV Agent: Pandas\u3092\u4f7f\u3063\u3066interactive\u306bCSV\u3068\u3084\u308a\u53d6\u308a\u3067\u304d\u308b (ref: https://python.langchain.com/docs/templates/csv-agent)</li> <li>Vectorstore agent</li> </ol>"},{"location":"langchain/agent/03_react_custom/#24-agent-executors","title":"2.4. Agent Executors","text":""},{"location":"langchain/agent/03_react_custom/#25-plan-and-execute","title":"2.5. Plan and Execute","text":""},{"location":"langchain/agent/03_react_custom/#ref","title":"Ref","text":"<ol> <li>Custom Agent</li> <li>ReAct with OpenAI GPT and LangChain</li> <li>Custom Agent (Blog from LangChain)</li> <li>\u63a8\u8ad6\u3057\u3001\u884c\u52d5\u3059\u308bChatGPT(OpenAI API) Agent\u3092\u4f5c\u308b \u2013 langchain ReAct Custom Agent\u57fa\u790e \u2013</li> <li>Change ReAct words to Japanese</li> <li>Custom ReAct\u89e3\u5256</li> </ol>"},{"location":"langchain/agent/04_self_ask_with_search/","title":"self ask with search","text":""},{"location":"langchain/agent/04_self_ask_with_search/#example","title":"Example","text":"<pre><code>from langchain_community.llms import OpenAI\nfrom langchain import SerpAPIWrapper\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.agents import AgentType\n\nllm = OpenAI(temperature=0)\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Intermediate Answer\",\n        func=search.run,\n        description=\"useful for when you need to ask with search\"\n    )\n]\n\nself_ask_with_search = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True)\nself_ask_with_search.run(\"What is the hometown of the reigning men's U.S. Open champion?\")\n</code></pre> <p>This agent utilizes a single tool that should be named <code>Intermediate Answer</code></p>"},{"location":"langchain/agent/04_self_ask_with_search/#implementation","title":"Implementation","text":"<ol> <li>SelfAskWithSearchAgent(Agent):</li> <li>SelfAskOutputParser<ol> <li>\u6700\u7d42\u884c\u306b<code>Followup:</code>\u304c\u3042\u308c\u3070\u3001<code>AgentAction(\"Intermediate Answer\", after_colon, text)</code>\u3092\u8fd4\u3059</li> <li>\u306a\u3044\u5834\u5408\u306b\u306f\u3001<code>\"So the final answer is: \"</code>\u304c\u3042\u308c\u3070\u3001<code>AgentFinish({\"output\": last_line[len(self.finish_string) :]}, text)</code>\u3092\u8fd4\u3057\u305d\u308c\u4ee5\u5916\u306f\u3001<code>OutputParserException(f\"Could not parse output: {text}\")</code></li> </ol> </li> <li>prompt: few shots\u306e\u3088\u3046\u306aprompt\u3067\u3001Example\u3092\u3044\u304f\u3064\u304b\u8f09\u305b\u3066 <code>Follow up: &lt;tool\u306b\u805e\u304f\u8cea\u554f&gt;</code> \u3092\u8fd4\u3059\u3088\u3046\u306b\u3057\u3066\u3082\u3089\u3063\u3066\u3044\u308b\u3002\u7279\u306b\u4e0d\u8981\u3067\u3042\u308c\u3070\u3001 <code>So the final answer is: &lt;\u6700\u7d42\u7684\u306a\u56de\u7b54&gt;</code>\u3092\u8fd4\u3059\u3088\u3046\u306b\u3057\u3066\u3082\u3089\u3063\u3066\u3044\u308b\u3002     <pre><code>Question: Who lived longer, Muhammad Ali or Alan Turing?\nAre follow up questions needed here: Yes.\nFollow up: How old was Muhammad Ali when he died?\nIntermediate answer: Muhammad Ali was 74 years old when he died.\nFollow up: How old was Alan Turing when he died?\nIntermediate answer: Alan Turing was 41 years old when he died.\nSo the final answer is: Muhammad Ali\n\nQuestion: When was the founder of craigslist born?\nAre follow up questions needed here: Yes.\nFollow up: Who was the founder of craigslist?\nIntermediate answer: Craigslist was founded by Craig Newmark.\nFollow up: When was Craig Newmark born?\nIntermediate answer: Craig Newmark was born on December 6, 1952.\nSo the final answer is: December 6, 1952\n\nQuestion: Who was the maternal grandfather of George Washington?\nAre follow up questions needed here: Yes.\nFollow up: Who was the mother of George Washington?\nIntermediate answer: The mother of George Washington was Mary Ball Washington.\nFollow up: Who was the father of Mary Ball Washington?\nIntermediate answer: The father of Mary Ball Washington was Joseph Ball.\nSo the final answer is: Joseph Ball\n\nQuestion: Are both the directors of Jaws and Casino Royale from the same country?\nAre follow up questions needed here: Yes.\nFollow up: Who is the director of Jaws?\nIntermediate answer: The director of Jaws is Steven Spielberg.\nFollow up: Where is Steven Spielberg from?\nIntermediate answer: The United States.\nFollow up: Who is the director of Casino Royale?\nIntermediate answer: The director of Casino Royale is Martin Campbell.\nFollow up: Where is Martin Campbell from?\nIntermediate answer: New Zealand.\nSo the final answer is: No\n\nQuestion: {input}\nAre followup questions needed here:{agent_scratchpad}\n</code></pre></li> </ol>"},{"location":"langchain/agent/04_self_ask_with_search/#search-tools","title":"Search tools","text":""},{"location":"langchain/agent/04_self_ask_with_search/#serpapiwrapper","title":"SerpAPIWrapper \u2705","text":"<p>Need to get api key from https://serpapi.com/manage-api-key and set it to <code>SERPAPI_API_KEY</code> environment variable.</p> <pre><code>poetry run python src/langchain/self_ask_with_search.py --search-tool SerpAPIWrapper\n</code></pre> <p></p>"},{"location":"langchain/agent/04_self_ask_with_search/#duckduckgosearchrun","title":"DuckDuckGoSearchRun \u274c","text":"<pre><code>poetry run python src/langchain/self_ask_with_search.py --search-tool DuckDuckGoSearchRun\n</code></pre> <p>Error</p> <pre><code>s/agent.py\", line 1077, in _iter_next_step\n    raise ValueError(\nValueError: An output parsing error occurred. In order to pass this error back to the agent and have it try again, pass `handle_parsing_errors=True` to the AgentExecutor. This is the error: Could not parse output: No.\n</code></pre>"},{"location":"langchain/agent/05_react_with_tool_and_llm/","title":"Custom ReAct with tool and llm","text":"<p>Tool\u3068LLM\u306b\u805e\u304f\u306e\u3092\u3046\u307e\u304f\u7d44\u307f\u5408\u308f\u305b\u308b\u3084\u308a\u65b9\u304c\u5206\u304b\u3089\u306a\u304b\u3063\u305f\u306e\u3067\u3001Tool\u3067\u76f4\u63a5llm\u306b\u805e\u304f\u3068\u3044\u3046\u306e\u3092\u4f5c\u3063\u305f\u3002</p> <pre><code>    tools = [\n        Tool(\n            name=\"GetBirthplace\",\n            func=get_birthplace,\n            description=\"Get birthplace of a person.\",\n        ),\n        Tool(\n            name=\"Llm\",\n            func=llm,\n            description=\"Use this tool to ask general questions\"\n        ),\n    ]\n</code></pre>"},{"location":"langchain/agent/05_react_with_tool_and_llm/#run","title":"Run","text":"<pre><code>poetry run python src/langchain/react_with_tool_and_llm.py\n</code></pre> <pre><code>&gt; Entering new AgentExecutor chain...\nThought: I need to get birthplace of \u4f50\u85e4.\nAction: GetBirthplace[\u4f50\u85e4]\nObservation: \u9752\u5cf6\nThought:  I need to get birthplace of \u7530\u4e2d.\nAction: GetBirthplace[\u7530\u4e2d]\nObservation: \u5927\u962a\nThought:  I need to get distance between \u9752\u5cf6 and \u5927\u962a.\nAction: Llm[I need to get distance between \u9752\u5cf6 and \u5927\u962a.]\nObservation:\n\nThe approximate distance between Qingdao and Osaka is 1,845 kilometers (1,145 miles).\nThought:  So the answer is 1845 km.\nAction: Finish[1845]\n\n&gt; Finished chain.\n</code></pre>"},{"location":"langchain/agent/08_react_decompose/","title":"Decompose ReAct","text":""},{"location":"langchain/agent/08_react_decompose/#1-overview","title":"1. Overview","text":"<p>Custom ReAct\u3092\u30d9\u30fc\u30b9\u306bReAct\u306e\u5185\u90e8\u3067\u8d77\u3053\u3063\u3066\u3044\u308b\u90e8\u5206\u3092\u3059\u3079\u3066\u4e00\u3064\u305a\u3064\u5206\u89e3\u3057\u3066\u898b\u308b\u3002</p> <p>\u542b\u307e\u308c\u308b\u5185\u5bb9</p> <ol> <li>LLM\u306b\u9001\u3089\u308c\u308bPrompt (Prompt Template): Agent\u306b\u30bb\u30c3\u30c8\u3055\u308c\u3066\u3044\u308b</li> <li>LLM\u304b\u3089\u5e30\u3063\u3066\u304d\u305fOutput: Agent\u5185</li> <li><code>Output Parser</code>: Agent\u5185</li> <li><code>AgentAction</code>\u3068<code>AgentFinish</code>: Agent.plan\u306e\u7d50\u679c\u304cAgentAction\u304bAgentFinish\u3067AgentFinish\u306e\u5834\u5408\u306b\u306fAgentExecutor\u5185\u306eWhile\u3092\u629c\u3051\u3066\u7d42\u4e86\u3059\u308b</li> </ol> <p>\u8a73\u7d30:</p> <ol> <li>AgentExecutor\u306fagent\u3068tools\u3067\u521d\u671f\u5316\u3055\u308c\u308b</li> <li><code>agent_executor(inputs=question, return_only_outputs=False, callbacks=None)</code> \u3067\u5b9f\u884c\u3055\u308c\u308b\u304c\u3001 <code>AgentExecutor</code>\u306f<code>Chain</code>\u3092\u7d99\u627f\u3057\u3066\u3044\u308b\u306e\u3067 <code>Chain.__call__</code>\u304c\u547c\u3070\u308c\u308b\u4e8b\u3068\u306a\u308b\u3002</li> <li><code>Chain.__call__</code>\u306e\u5185\u90e8\u304b\u3089\u306f\u3001AgentExecutor._call\u304c\u547c\u3070\u308c\u308b</li> <li><code>AgentExecutor._call</code>\u5185\u3067\u306fwhile\u306b\u3088\u3063\u3066\u7d42\u4e86\u6761\u4ef6or <code>_take_next_step</code>\u306e\u7d50\u679c\u304c<code>AgentFinish</code>\u306b\u306a\u308b\u307e\u3067<code>_take_next_step</code>\u3092\u884c\u3046</li> <li><code>_take_next_step</code>\u5185\u3067\u306f\u3001<code>agent_action = agent.plan(intermediate_steps, callbacks, inputs)</code>\u3092\u5b9f\u884c<ol> <li><code>agent_action</code> \u304c<code>AgentAction</code>\u306e\u5834\u5408\u306f\u3001\u5bfe\u5fdc\u3059\u308b\u30c4\u30fc\u30eb\u3092\u547c\u3093\u3067\u7d50\u679c\u3092\u5f97\u308b <code>observation = tool.run()</code>\u3092\u3068\u308a\u3001 <code>result.append((agent_action, observation))</code>\u3068\u3057\u3066result\u3092\u8fd4\u3059 (\u57fa\u672clen(result)\u306f1)</li> <li><code>agent_action</code>\u304c<code>AgentFinish</code>\u306e\u5834\u5408\u306f <code>agent_action</code>\u3092\u305d\u306e\u307e\u307e\u5e30\u3059</li> </ol> </li> <li><code>_take_next_step</code>\u306e\u7d50\u679c\u3092\u53d6\u308a<ol> <li>AgentFinish\u306e\u5834\u5408\u306f\u305d\u306e\u307e\u307ereturn\u3057\u3066\u7d42\u4e86 (return_values\u306bintermediate_steps\u3092\u5165\u308c\u3066\u8fd4\u3059)</li> <li>AgentAction\u306e\u5834\u5408\u306f\u3001intermediate_steps\u306b<code>intermediate_steps.extend(result)</code> \u524d\u56de\u306eresult\u3092\u8ffd\u52a0\u3057\u3066\u3001while\u30eb\u30fc\u30d7\u3092\u7d9a\u3051\u308b</li> </ol> </li> </ol>"},{"location":"langchain/agent/08_react_decompose/#2-implementation","title":"2. Implementation","text":""},{"location":"langchain/agent/08_react_decompose/#_take_next_step-step","title":"_take_next_step Step\u306e\u4e2d\u8eab","text":"<pre><code>output = self.agent.plan(\n    intermediate_steps,\n    callbacks=run_manager.get_child() if run_manager else None,\n    **inputs,\n)\n</code></pre> <ol> <li><code>intermediate_steps</code>: agent.plan\u5185\u3067\u5b9f\u884c\u3057\u305fAgentAction\u3068Result\u306e\u5c65\u6b74(tuple)\u306elist (intermediate_steps)</li> <li><code>run_manager</code> &lt;- callbackManager.on_chain_start\u306e\u8fd4\u308a\u5024 (\u7279\u306b\u4e0d\u8981\u3067\u3042\u308c\u3070None\u3067\u3044\u3044)</li> <li><code>inputs</code>\u306f Chain.call\u306e\u306a\u304b\u306einputs = self.prep_inputs(inputs)\u3067\u5909\u63db\u3055\u308c\u3066\u3044\u308b</li> </ol>"},{"location":"langchain/agent/08_react_decompose/#21-run_managercallbackmanager","title":"2.1. run_manager\u306e\u51fa\u51e6CallbackManager\u306b\u95a2\u3057\u3066","text":"<p>CallbackManager.configure:</p> <p><code>AgentExecutor(Chain).callbacks</code> \u306f<code>from_agent_and_tools</code>\u3067\u4f5c\u6210\u3059\u308b\u3068None (ref)</p> <pre><code>callback_manager = CallbackManager.configure(\n    inheritable_callbacks=None, local_callbacks=None, verbose=True, inheritable_tags=None, local_tags=None\n)\nrun_manager = callback_manager.on_chain_start( # _call\u306brun_manager\u304c\u3042\u308b\u306e\u3067run_manager\u3092\u8a2d\u5b9a\u3059\u308b\n    dumpd(self), # dumpd(obj: Any) = json.loads(dumps(obj))\n    inputs, # self.prep_inputs\u3067Dict\u306b\u306a\u3063\u305f\u3082\u306e\n)\n</code></pre>"},{"location":"langchain/agent/08_react_decompose/#22-intermediate_steps","title":"2.2. intermediate_steps\u306b\u95a2\u3057\u3066","text":"<ol> <li>\u6700\u521d\u306e\u72b6\u614b\u306f <code>[]</code> (_call()\u5185)     <pre><code>intermediate_steps: List[Tuple[AgentAction, str]] = []\n</code></pre></li> <li>\u6bce\u56deagent.plan\u3067\u3069\u306e\u30c4\u30fc\u30eb\u3092\u4f7f\u3046\u304b\u6c7a\u3081\u3001Tool\u3092\u5b9f\u884c\u3057\u305f\u5f8c\u306b <code>result.append((agent_action, observation))</code>\u304cextend\u3055\u308c\u308b</li> </ol>"},{"location":"langchain/agent/08_react_decompose/#23-inputs-input_keys","title":"2.3. inputs &amp; input_keys","text":"<ol> <li> <p><code>Chain.__call__</code>\u5185\u3067inputs\u306f\u5909\u5f62\u3055\u308c\u3066\u3044\u308b\u306e\u3067\u6ce8\u610f</p> <p><pre><code>inputs = {list(_input_keys)[0]: inputs}\n</code></pre> 1. <code>_input_keys</code>\u306f\u5404 <code>self.agent.input_keys</code> (AgentExecutor.input_keys) 1. \u4eca\u56de\u306eAgent\u306e input_keys\u306f <pre><code>agent.input_keys\n['input']\n</code></pre></p> </li> </ol>"},{"location":"langchain/agent/08_react_decompose/#24-callbacks","title":"2.4. callbacks","text":"<pre><code>agent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent,\n    tools=tools,\n    callback_manager=None, # default: None (explicitly write None)\n    verbose=True,\n)\n</code></pre> <p>\u4ee5\u4e0b\u306e\u5b9a\u7fa9\u3088\u308a<code>cls(agent=agent, tools=tools, callback_manager=callback_manager, **kwargs)</code> AgentExecutor\u3092\u521d\u671f\u5316\u3059\u308b\u3002</p> <pre><code>class AgentExecutor(Chain):\n    ...\n    @classmethod\n    def from_agent_and_tools(\n        cls,\n        agent: Union[BaseSingleActionAgent, BaseMultiActionAgent],\n        tools: Sequence[BaseTool],\n        callback_manager: Optional[BaseCallbackManager] = None,\n        **kwargs: Any,\n    ) -&gt; AgentExecutor:\n        \"\"\"Create from agent and tools.\"\"\"\n        return cls(\n            agent=agent, tools=tools, callback_manager=callback_manager, **kwargs\n        )\n</code></pre> <ol> <li><code>callback_manager</code>\u306f<code>None</code>\u3067\u3001<code>verbose</code>\u304c<code>True</code>.</li> <li>CallbackManager\u3092\u521d\u671f\u5316\u3057\u3066\u304b\u3089run_manager\u3092\u53d6\u308a\u51fa\u3059\u304ccallback manager\u81ea\u4f53\u306f\u7279\u306b\u4f7f\u308f\u308c\u3066\u3044\u306a\u3044\u3002</li> <li><code>Chain.__call__</code>\u5185\u3067 run_manager\u304ccallback_manager.on_chain_start()\u3067\u53d6\u308a\u51fa\u3055\u308c\u3001<code>self._call(inputs, run_manager=run_manager)</code>\u3067\u6e21\u3055\u308c\u308b</li> </ol>"},{"location":"langchain/agent/08_react_decompose/#3-tips","title":"3. Tips","text":""},{"location":"langchain/agent/08_react_decompose/#31","title":"3.1. \u6e96\u5099","text":"<pre><code>poetry run python\n</code></pre> <pre><code>from src.langchain.react_decompose import *\n</code></pre>"},{"location":"langchain/agent/08_react_decompose/#32","title":"3.2. \u5168\u90e8\u3092\u5b9f\u884c","text":"<pre><code>llm = OpenAI()\nagent = ReActTestAgent.from_llm_and_tools(\n    llm,\n    tools,\n)\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent,\n    tools=tools,\n    callback_manager=None, # default: None (explicitly write None)\n    verbose=True,\n)\nquestion = \"How much is the difference between the total of company C, F and the total of company A, E ?\"\nagent_executor(inputs=question, return_only_outputs=False, callbacks=None)\n</code></pre>"},{"location":"langchain/agent/08_react_decompose/#33-agentplanstep1-_take_next_step","title":"3.3. Agent.plan\u306eStep\u30921\u3064\u5b9f\u884c (_take_next_step\u524d\u534a)","text":"<ol> <li>\u5b9f\u969b\u306e\u5b9f\u884c\u3067\u306f\u3001AgentExecutor._take_next_step\u306e\u4e2d\u306e\u30eb\u30fc\u30d7\u3067\u884c\u308f\u308c\u3066\u3044\u308b</li> <li>OutputParser\u304cParse\u306b\u5931\u6557\u3059\u308b\u3053\u3068\u3082\u3042\u308b</li> <li>\u8fd4\u308a\u5024\u306f\u3001 <code>AgentAction</code> or <code>AgentFinish</code></li> </ol> <pre><code>intermediate_steps = []\nagent_action = agent.plan(intermediate_steps, callbacks=None, **{\"input\":\"How much is the difference between the total of company C, F and the total of company A, E ?\"})\nAgentAction(tool='GetInvoice', tool_input='C', log='\u601d\u8003: I need to get invoice amount of company C.\\n\u884c\u52d5: GetInvoice[C]')\n</code></pre> <p><code>AgentAction</code>\u306b\u306f<code>tool</code>, <code>tool_input</code>, <code>log</code>\u304c\u3042\u308b</p>"},{"location":"langchain/agent/08_react_decompose/#34-agentplantool-_take_next_step","title":"3.4. Agent.plan\u7d50\u679c\u304b\u3089Tool\u3092\u547c\u3076 (_take_next_step\u5f8c\u534a)","text":"<p>\u4e0a\u306eagent.plan\u306e\u7d50\u679c\u304c<code>AgentAction</code>\u3060\u3063\u305f\u5834\u5408\u3001<code>_take_next_step</code>\u306e\u7d50\u679c\u3092\u4ee5\u4e0b\u306e\u3088\u3046\u306b\u53d6\u308c\u308b</p> <pre><code>name_to_tool_map = {tool.name: tool for tool in tools} # https://github.com/hwchase17/langchain/blob/b95002289409077965d99636b15a45300d9c0b9d/langchain/agents/agent.py#L939C9-L939C25\nagent_action # AgentAction(tool='GetInvoice', tool_input='C', log='\u601d\u8003: I need to get invoice amount of company C.\\n\u884c\u52d5: GetInvoice[C]')\nresult = []\ntool = name_to_tool_map[agent_action.tool]\nreturn_direct = tool.return_direct # false\ntool_run_kwargs = agent.tool_run_logging_kwargs() # {'llm_prefix': '\u601d\u8003: ', 'observation_prefix': '\u89b3\u5bdf: '}\nif return_direct:\n    tool_run_kwargs[\"llm_prefix\"] = \"\"\nobservation = tool.run(\n    agent_action.tool_input,\n    verbose=True,\n    callbacks=None,\n    **tool_run_kwargs,\n) # observation = 20000 &lt;- tool\u304b\u3089\u8fd4\u3055\u308c\u305f\u7d50\u679c\nresult.append((agent_action, observation))\nresult # _take_next_step\u3067\u8fd4\u3055\u308c\u308b\u7d50\u679c [(AgentAction(tool='GetInvoice', tool_input='C', log='\u601d\u8003: I need to get invoice amount of company C.\\n\u884c\u52d5: GetInvoice[C]'), 20000)]\n</code></pre>"},{"location":"langchain/agent/08_react_decompose/#35-agentexecutor_callwhile_take_next_step","title":"3.5. AgentExecutor._call\u306ewhile\u5185\u3067_take_next_step\u306e\u7d50\u679c\u3092\u53d7\u3051\u53d6\u3063\u305f\u5f8c","text":"<pre><code>next_step_output = self._take_next_step(...)\n</code></pre> <p>next_step_output\u306b\u3088\u308a - AgentFinish -&gt; _return\u3092\u547c\u3093\u3067\u7d42\u4e86 - \u305d\u306e\u4ed6\u2192\u7d9a\u884c</p> <pre><code>intermediate_steps.extend(result) # result\u306f\u524d\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u306eresult\n</code></pre> <p>\u524d\u56de\u3068\u307b\u307c\u540c\u69d8\u306b <code>intermediate_steps</code>\u3060\u3051\u304c\u5909\u308f\u3063\u305f\u72b6\u614b\u3067\u3082\u3046\u4e00\u5ea6agent.plan\u3092\u547c\u3076</p> <pre><code>agent_action = agent.plan(intermediate_steps, callbacks=None, **{\"input\":\"How much is the difference between the total of company C, F and the total of company A, E ?\"})\nagent_action # AgentAction(tool='GetInvoice', tool_input='F', log=' I need to get invoice amount of company F.\\n\u884c\u52d5: GetInvoice[F]')\n</code></pre> <p>agent_action\u306e\u51e6\u7406\u30923.4.\u3068\u540c\u69d8\u306b\u3059\u308b\u3068result\u306f\u3044\u304b\u306e\u3088\u3046\u306b\u306a\u308b</p> <pre><code>result\n[(AgentAction(tool='GetInvoice', tool_input='F', log=' I need to get invoice amount of company F.\\n\u884c\u52d5: GetInvoice[F]'), 20000), (AgentAction(tool='GetInvoice', tool_input='F', log=' I need to get invoice amount of company F.\\n\u884c\u52d5: GetInvoice[F]'), 4100)]\n</code></pre> <p>\u305d\u3057\u3066intermediate_steps\u3092\u66f4\u65b0\u3059\u308b</p> <pre><code>intermediate_steps.extend(result) # result\u306f\u524d\u306e\u30b9\u30c6\u30c3\u30d7\u3067\u306eresult\n</code></pre> <p>\u3055\u3089\u306bagent.plan(intermediate_steps, callbacks, inputs) -&gt; agent_action or agent_finish -&gt; tool.run -&gt; observation -&gt; result\u306bappend -&gt; intermediate_steps\u66f4\u65b0 -&gt; \u2026\u3092\u7e70\u308a\u8fd4\u3059</p> <p>\u7d50\u5c40\u4e00\u756a\u5927\u4e8b\u306a\u306e\u306f<code>intermediate_steps</code>\u306b\u7d50\u679c\u3092\u8caf\u3081\u3066\u3044\u304f\u3053\u3068</p>"},{"location":"langchain/agent/08_react_decompose/#36-agentfinish","title":"3.6. AgentFinish\u3068\u306a\u3063\u305f\u5834\u5408","text":"<p><code>_return</code>\u3067return_values\u3092\u8fd4\u3059</p> <p>AgentExecutor._return</p> <pre><code>def _return(\n    self,\n    output: AgentFinish,\n    intermediate_steps: list,\n    run_manager: Optional[CallbackManagerForChainRun] = None,\n) -&gt; Dict[str, Any]:\n    if run_manager:\n        run_manager.on_agent_finish(output, color=\"green\", verbose=self.verbose)\n    final_output = output.return_values\n    if self.return_intermediate_steps:\n        final_output[\"intermediate_steps\"] = intermediate_steps\n    return final_output\n</code></pre>"},{"location":"langchain/agent/08_react_decompose/#4-run","title":"4. Run","text":"<pre><code>poetry run python src/langchain/react_decompose.py\n</code></pre>"},{"location":"langchain/agent/agent_executor/","title":"AgentExecutor","text":"<p><code>AgentExecutor</code> executes an <code>Agent</code> with a set of <code>Tools</code> and a <code>CallbackManager</code>.</p>"},{"location":"langchain/agent/agent_executor/#how-to-use","title":"How to Use","text":"<pre><code>from langchain.agent import AgentExecutor\n\nagent_executor = AgentExecutor.from_agent_and_tools(\n    agent=agent,\n    tools=tools,\n    callback_manager=None, # default: None (explicitly write None)\n    verbose=True,\n)\n</code></pre>"},{"location":"langchain/agent/agent_executor/#implementation","title":"Implementation","text":"<ol> <li> <p>An agent is called by an AgentExecutor (<code>AgentExcutor</code> -&gt; <code>Chain</code> -&gt; <code>RunnableSerializable</code>) which is a <code>Chain</code> that takes an <code>agent</code> and <code>tools</code> as inputs.</p> <pre><code>agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True, handle_parsing_errors=False)\n</code></pre> <ol> <li><code>memory</code> is optional.</li> <li><code>verbose</code> is <code>True</code> by default.</li> <li><code>handle_parsing_errors</code> is <code>False</code> by default.</li> <li>Invoke the agent with <code>agent_executor.invoke({\"input\": \"Who is Japan's prime minister?\")</code>.</li> <li>invoke is defined in Chain.</li> <li> <p><code>invoke</code> calls an abstractmethod _call.</p> <pre><code>@abstractmethod\ndef _call(\n    self,\n    inputs: Dict[str, Any],\n    run_manager: Optional[CallbackManagerForChainRun] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Execute the chain.\n\n    This is a private method that is not user-facing. It is only called within\n        `Chain.__call__`, which is the user-facing wrapper method that handles\n        callbacks configuration and some input/output processing.\n\n    Args:\n        inputs: A dict of named inputs to the chain. Assumed to contain all inputs\n            specified in `Chain.input_keys`, including any inputs added by memory.\n        run_manager: The callbacks manager that contains the callback handlers for\n            this run of the chain.\n\n    Returns:\n        A dict of named outputs. Should contain all outputs specified in\n            `Chain.output_keys`.\n    \"\"\"\n</code></pre> </li> </ol> </li> <li> <p>The actual implementation of <code>_call</code> for <code>AgentExecutor</code> is here</p> <pre><code>def _call(\n    self,\n    inputs: Dict[str, str],\n    run_manager: Optional[CallbackManagerForChainRun] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Run text through and get agent response.\"\"\"\n    # Construct a mapping of tool name to tool for easy lookup\n    name_to_tool_map = {tool.name: tool for tool in self.tools}\n    # We construct a mapping from each tool to a color, used for logging.\n    color_mapping = get_color_mapping(\n        [tool.name for tool in self.tools], excluded_colors=[\"green\", \"red\"]\n    )\n    intermediate_steps: List[Tuple[AgentAction, str]] = []\n    # Let's start tracking the number of iterations and time elapsed\n    iterations = 0\n    time_elapsed = 0.0\n    start_time = time.time()\n    # We now enter the agent loop (until it returns something).\n    while self._should_continue(iterations, time_elapsed):\n        next_step_output = self._take_next_step(\n            name_to_tool_map,\n            color_mapping,\n            inputs,\n            intermediate_steps,\n            run_manager=run_manager,\n        )\n        if isinstance(next_step_output, AgentFinish):\n            return self._return(\n                next_step_output, intermediate_steps, run_manager=run_manager\n            )\n\n        intermediate_steps.extend(next_step_output)\n        if len(next_step_output) == 1:\n            next_step_action = next_step_output[0]\n            # See if tool should return directly\n            tool_return = self._get_tool_return(next_step_action)\n            if tool_return is not None:\n                return self._return(\n                    tool_return, intermediate_steps, run_manager=run_manager\n                )\n        iterations += 1\n        time_elapsed = time.time() - start_time\n    output = self.agent.return_stopped_response(\n        self.early_stopping_method, intermediate_steps, **inputs\n    )\n    return self._return(output, intermediate_steps, run_manager=run_manager)\n</code></pre> <ol> <li>There's a <code>while</code> loop, which iterates the following logic if <code>_should_continue</code> returns true.</li> <li>Call <code>_take_next_step</code> and get <code>next_step_output</code>, inside which the agent calls <code>plan</code> and the tool is executed.<ol> <li>Plan: Decide which tool to use</li> <li>Execute: Execute the tool</li> </ol> </li> <li>If the <code>next_step_output</code> is an <code>AgentFinish</code>, it returns the output.</li> <li>Otherwise, it extends the <code>intermediate_steps</code> and checks if the tool should return directly.</li> <li>It increments the iterations and time elapsed.</li> <li>If the loop ends, it returns the output.</li> </ol> </li> </ol>"},{"location":"langchain/agent/agent_multiturn/","title":"Multiturn Agent","text":"<p>The content is from https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part29.html.</p>"},{"location":"langchain/agent/agent_multiturn/#run-a-script","title":"Run a script","text":"<pre><code>poetry run python src/examples/agent_multiturn.py\n</code></pre>"},{"location":"langchain/agent/agent_multiturn/#run-in-a-streamlit-app","title":"Run in a Streamlit app","text":"<pre><code>poetry run streamlit run src/projects/apps/streamlit/chat_with_multiturn_agent.py\n</code></pre>"},{"location":"langchain/agent/agent_multiturn/#versions","title":"Versions","text":"<ul> <li>Old syntax: agent_multiturn.py</li> </ul>"},{"location":"langchain/agent/agent_old/","title":"Agent (Old)","text":"<p>Warning</p> <p>Please check Agent for the latest information.</p>"},{"location":"langchain/agent/agent_old/#overview","title":"Overview","text":"<ol> <li><code>Agent</code>\u306f\u57fa\u672c<code>AgentExecutor</code>\u304b\u3089\u5b9f\u884c\u3055\u308c\u308b</li> <li><code>Agent</code>\u306b\u306f<code>Tools</code>, <code>llm</code>, <code>prompt</code>, <code>parser</code>\u3067\u69cb\u6210\u3055\u308c\u308b\u3002<code>initialize_agent</code>\u3067\u521d\u671f\u5316\u3059\u308b\u5834\u5408\u306b\u306f\u3001AgentType\u3092\u6307\u5b9a\u3059\u308b\u3053\u3068\u3067prompt\u3068parser\u306fDefault\u306e\u3082\u306e\u3092\u4f7f\u3046\u3053\u3068\u304c\u3067\u304d\u308b</li> <li><code>Prompt</code>\u306f\u3001<code>initialize_agent</code>\u3067\u521d\u671f\u5316\u3059\u308b\u5834\u5408\u306b\u306f\u3001<code>AgentType</code>\u306b\u3088\u3063\u3066Default Prompt\u304c\u7570\u306a\u3063\u3066\u3044\u308b\u304c\u3001\u81ea\u5206\u3067\u6307\u5b9a\u3059\u308b\u3053\u3068\u3082\u53ef\u80fd\u3002 \u4f8b. <code>SelfAskWithSearch</code>\u3067\u306f\u3001LLM\u306b<code>Follow up: &lt;tool\u306b\u805e\u304f\u8cea\u554f&gt;</code>\u3092\u8fd4\u3057\u3066\u3082\u3089\u3044\u3001<code>The final answer is &lt;\u6700\u7d42\u7684\u306a\u56de\u7b54&gt;</code>\u3092\u8fd4\u3057\u3066\u3082\u3089\u3046\u3088\u3046\u306b\u3057\u3066\u3044\u308b</li> <li><code>Tools</code>\u306f\u3069\u306e\u3088\u3046\u306b\u547c\u3070\u308c\u308b\u304b</li> <li><code>OutputParser</code>\u306f\u3001Prompt\u306b\u5bfe\u5fdc\u3059\u308b\u3082\u306e\u3067\u3001llm\u304b\u3089\u8fd4\u3063\u3066\u304d\u305fOutput\u3092Parse\u3059\u308b\u3082\u306e\u3002 \u4f8b. <code>SelfAskWithSearch</code>\u3067\u306f\u3001\u8fd4\u3063\u3066\u304d\u305foutput\u306e\u6700\u7d42\u884c\u306b<code>Follow up:</code>\u304c\u3042\u308c\u3070\u3001<code>AgentAction</code>\u3092\u8fd4\u3057\u3001 <code>The final answer is</code>\u304c\u3042\u308c\u3070\u3001 <code>AgentFinish</code>\u3092\u8fd4\u3057\u3001\u305d\u308c\u4ee5\u5916\u306f\u3001<code>OutputParserException</code>\u3092\u8fd4\u3059\u3088\u3046\u306b\u3057\u3066\u3044\u308b\u3002</li> </ol>"},{"location":"langchain/agent/agent_old/#implementation-old","title":"Implementation (Old)","text":"<p>Warning</p> <p>Please check Agent for the latest information.</p>"},{"location":"langchain/agent/agent_old/#agentexecutor","title":"AgentExecutor","text":"<p>\u4e00\u3064\u306eAgent\u3068\u8907\u6570\u306eTools\u3092\u3082\u3064</p> <p>\u5b9f\u884c\u3055\u308c\u308b\u3068\u304d\u306f\u3001\u521d\u671f\u5316\u3055\u308c\u305f<code>agent_executor(\"Jira\u304b\u3089\u30d1\u30d5\u30a9\u30fc\u30de\u30f3\u30b9\u30c6\u30b9\u30c8\u306b\u95a2\u3059\u308b\u30c1\u30b1\u30c3\u30c8\u3092\u53d6\u5f97\u3057\u3066\u304f\u3060\u3055\u3044\u3002\")</code> \u3068\u3044\u3046\u5f62\u3067\u547c\u3070\u308c\u3001<code>AgentExecutor</code>\u306e\u7d99\u627f\u5143\u306eChain.__call__() \u304c\u547c\u3070\u308c\u3001AgentExecutor._call\u304c\u305d\u306e\u4e2d\u3067\u547c\u3070\u308c\u308b\u3002\u305d\u3057\u3066\u3001\u3053\u306e\u4e2d\u3067<code>Agent</code>\u306e<code>plan</code>\u3092\u547c\u3076\u3002</p> <p><code>Chain.__call__</code>:</p> <pre><code>    def __call__(\n        self,\n        inputs: Union[Dict[str, Any], Any],\n        return_only_outputs: bool = False,\n        callbacks: Callbacks = None,\n        *,\n        tags: Optional[List[str]] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        run_name: Optional[str] = None,\n        include_run_info: bool = False,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Execute the chain.\n\n        Args:\n            inputs: Dictionary of inputs, or single input if chain expects\n                only one param. Should contain all inputs specified in\n                `Chain.input_keys` except for inputs that will be set by the chain's\n                memory.\n            return_only_outputs: Whether to return only outputs in the\n                response. If True, only new keys generated by this chain will be\n                returned. If False, both input keys and new keys generated by this\n                chain will be returned. Defaults to False.\n            callbacks: Callbacks to use for this chain run. These will be called in\n                addition to callbacks passed to the chain during construction, but only\n                these runtime callbacks will propagate to calls to other objects.\n            tags: List of string tags to pass to all callbacks. These will be passed in\n                addition to tags passed to the chain during construction, but only\n                these runtime tags will propagate to calls to other objects.\n            metadata: Optional metadata associated with the chain. Defaults to None\n            include_run_info: Whether to include run info in the response. Defaults\n                to False.\n\n        Returns:\n            A dict of named outputs. Should contain all outputs specified in\n                `Chain.output_keys`.\n        \"\"\"\n</code></pre> <p>As you can see inputs: Dictionary of inputs, or single input if chain expects only one param. Should contain all inputs specified in <code>Chain.input_keys</code> except for inputs that will be set by the chain's memory. <code>Chain.input_keys</code>\u306f <code>AgentExecutor</code>\u306einput_keys\u3092\u53c2\u7167\u3057\u3066\u3044\u308b\u3002</p> <pre><code>    @property\n    def input_keys(self) -&gt; List[str]:\n        \"\"\"Get the input keys for this chain.\"\"\"\n        return self.agent.input_keys\n</code></pre> <p><code>AgentExecutor._call</code>:</p> <p><code>_call</code> -&gt; <code>self._take_next_step</code> -&gt; <code>self._iter_next_step</code> -&gt; <code>self.agent.plan(intermediate_steps, callbacks, **inputs)</code>\u3068\u547c\u3076</p> <pre><code># Call the LLM to see what to do.\noutput = self.agent.plan(\n    intermediate_steps,\n    callbacks=run_manager.get_child() if run_manager else None,\n    **inputs,\n)\n</code></pre> <p><code>inputs: Dict[str, str]</code>\u306fDict\u3002</p>"},{"location":"langchain/agent/agent_old/#plan","title":"Plan","text":"<p>\u4e0a\u8a18\u306eAgent\u306f<code>Agent</code>\u307e\u305f\u306f<code>BaseSingleActionAgent</code>\u3092\u7d99\u627f\u3057\u3066\u3044\u308b</p> <ol> <li> <p>Agent.plan:</p> <pre><code>full_inputs = self.get_full_inputs(intermediate_steps, **kwargs)\nfull_output = self.llm_chain.predict(callbacks=callbacks, **full_inputs)\nreturn self.output_parser.parse(full_output)\n</code></pre> </li> </ol>"},{"location":"langchain/agent/agent_old/#initialize_agent","title":"initialize_agent","text":"<p>agent\u306bAgentType (e.g.<code>AgentType.ZERO_SHOT_REACT_DESCRIPTION</code>(=<code>zero-shot-react-description</code>))\u3092\u6307\u5b9a\u3057\u3066Agent\u3092\u4f5c\u6210</p> <p>\u8fd4\u308a\u5024\u306f<code>AgentExecutor</code></p> <p>\u4f7f\u3044\u65b9:</p> <p><code>agent</code>\u307e\u305f\u306f<code>agent_path</code>\u3092\u6307\u5b9a\u3057\u3066\u4f5c\u6210</p> <pre><code>agent_executor = initialize_agent(\n    tools,\n    llm,\n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n</code></pre> <p>\u5b9f\u88c5:</p> <ol> <li>Agent Class\u3092\u53d6\u5f97     <pre><code>agent_cls = AGENT_TO_CLASS[agent]\n</code></pre></li> <li>Agent\u306eObject\u3092\u5404Agent\u306e<code>from_llm_and_tools</code>\u30e1\u30bd\u30c3\u30c9\u3092\u4f7f\u3063\u3066\u751f\u6210<ol> <li><code>agent</code>\u306e\u5834\u5408: <code>AgentClass.from_llm_and_tools</code> <pre><code>agent_obj = agent_cls.from_llm_and_tools(\n    llm, tools, callback_manager=callback_manager, **agent_kwargs\n)\n</code></pre>     Agent\u3054\u3068\u306b\u5b9a\u7fa9\u3055\u308c\u3066\u3044\u3066\u3001\u3053\u306e\u4e2d\u3067prompt\u3082\u8a2d\u5b9a\u3055\u308c\u308b (\u4f8b. OpenAIFunctionsAgent.from_llm_and_tools)</li> <li><code>agent_path</code>\u306e\u5834\u5408: <code>load_agent()</code> <pre><code>agent_obj = load_agent(\n    agent_path, llm=llm, tools=tools, callback_manager=callback_manager\n)\n</code></pre></li> </ol> </li> <li>AgentExecutor\u306e\u751f\u6210\u3068Return     <pre><code>AgentExecutor.from_agent_and_tools(\n    agent=agent_obj,\n    tools=tools,\n    callback_manager=callback_manager,\n    tags=tags_,\n    **kwargs,\n)\n</code></pre></li> </ol>"},{"location":"langchain/agent/agent_old/#outputparser","title":"OutputParser","text":"<p>output_parser</p>"},{"location":"langchain/agent/agent_old/#agent-types-old","title":"Agent Types (Old)","text":"<p>Warning</p> <p>Please check Agent for the latest information.</p> Agent Type Plan Prompt OutputParser Link ZeroShotAgent(Agent) <code>Agent(BaseSingleActionAgent)</code>\u306eplan\u3092\u53c2\u7167 ReActDocstoreAgent(Agent) <code>Agent(BaseSingleActionAgent)</code>\u306eplan\u3092\u53c2\u7167 React Docstore SelfAskWithSearchAgent(Agent) <code>Agent(BaseSingleActionAgent)</code>\u306eplan\u3092\u53c2\u7167 Self Ask with Search ConversationalAgent(Agent) <code>Agent(BaseSingleActionAgent)</code>\u306eplan\u3092\u53c2\u7167 ChatAgent(Agent) <code>Agent(BaseSingleActionAgent)</code>\u306eplan\u3092\u53c2\u7167 ConversationalChatAgent(Agent) <code>Agent(BaseSingleActionAgent)</code>\u306eplan\u3092\u53c2\u7167 StructuredChatAgent(Agent) <code>Agent(BaseSingleActionAgent)</code>\u306eplan\u3092\u53c2\u7167 OpenAIFunctionsAgent(BaseSingleActionAgent) plan\u306f<code>OpenAIFunctionsAgent</code>\u306b\u5b9a\u7fa9 OpenAIFunctionsAgentOutputParser._parse_ai_message OpenAIMultiFunctionsAgent(BaseSingleActionAgent) plan\u306f<code>OpenAIMultiFunctionsAgent</code>\u306b\u5b9a\u7fa9 Custom ReAct Agent ReActTestAgent(Agent): <code>Agent(BaseSingleActionAgent)</code>\u306eplan\u3092\u53c2\u7167 Agent, Prompt, OutputParser\u3092\u5b9a\u7fa9\u3059\u308b\u3053\u3068\u3067Custom Agent\u3092\u4f5c\u6210\u3067\u304d\u308b <p>AGENT_TO_CLASS\u304c\u4f5c\u6210\u3067\u304d\u308bAgentType:</p> <pre><code>AGENT_TO_CLASS: Dict[AgentType, AGENT_TYPE] = {\n    AgentType.ZERO_SHOT_REACT_DESCRIPTION: ZeroShotAgent,\n    AgentType.REACT_DOCSTORE: ReActDocstoreAgent,\n    AgentType.SELF_ASK_WITH_SEARCH: SelfAskWithSearchAgent,\n    AgentType.CONVERSATIONAL_REACT_DESCRIPTION: ConversationalAgent,\n    AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION: ChatAgent,\n    AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION: ConversationalChatAgent,\n    AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION: StructuredChatAgent,\n    AgentType.OPENAI_FUNCTIONS: OpenAIFunctionsAgent,\n    AgentType.OPENAI_MULTI_FUNCTIONS: OpenAIMultiFunctionsAgent,\n}\n</code></pre>"},{"location":"langchain/agent/agent_old/#openaifunctionsagent","title":"OpenAIFunctionsAgent","text":"<p>openai\u306efunction calling\u3092\u4f7f\u3046\u3002\u5b9f\u969b\u306bFunction\u3092\u9078\u3093\u3067\u547c\u3076\u90e8\u5206\u306flangchain\u5185\u306b\u306a\u3044\u306e\u3067\u3001\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u306f\u51fa\u6765\u306a\u3044\u3002</p> <p>ref: https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models</p> <p>\u69cb\u6210\u8981\u7d20 1. <code>llm</code>: BaseLanguageModel 1. <code>tools</code>: Sequence[BaseTool] 1. <code>prompt</code>: BasePromptTemplate</p> <p>Agent\u306e\u521d\u671f\u5316</p> <p>OpenAIFunctionsAgent.from_llm_and_tools\u3067Agent\u304c\u521d\u671f\u5316\u3055\u308c\u308b</p> <pre><code>    @classmethod\n    def from_llm_and_tools(\n        cls,\n        llm: BaseLanguageModel,\n        tools: Sequence[BaseTool],\n        callback_manager: Optional[BaseCallbackManager] = None,\n        extra_prompt_messages: Optional[List[BaseMessagePromptTemplate]] = None,\n        system_message: Optional[SystemMessage] = SystemMessage(\n            content=\"You are a helpful AI assistant.\"\n        ),\n        **kwargs: Any,\n    ) -&gt; BaseSingleActionAgent:\n        \"\"\"Construct an agent from an LLM and tools.\"\"\"\n        if not isinstance(llm, ChatOpenAI):\n            raise ValueError(\"Only supported with ChatOpenAI models.\")\n        prompt = cls.create_prompt(\n            extra_prompt_messages=extra_prompt_messages,\n            system_message=system_message,\n        )\n        return cls(\n            llm=llm,\n            prompt=prompt,\n            tools=tools,\n            callback_manager=callback_manager,\n            **kwargs,\n        )\n</code></pre> <p>prompt</p> <pre><code>prompt = cls.create_prompt(\n    extra_prompt_messages=extra_prompt_messages,\n    system_message=system_message,\n)\n</code></pre> <ul> <li>system_message: Message to use as the system message that will be the first in the prompt. default\u3067\u306f<code>You are a helpful AI assistant.</code>\u304c\u6e21\u3055\u308c\u308b</li> <li>extra_prompt_messages: Prompt messages that will be placed between the system message and the new human input.\u3000default\u3067\u306fNone\u304c\u308f\u305f\u3055\u308c\u308b</li> </ul> <pre><code>    @classmethod\n    def create_prompt(\n        cls,\n        system_message: Optional[SystemMessage] = SystemMessage(\n            content=\"You are a helpful AI assistant.\"\n        ),\n        extra_prompt_messages: Optional[List[BaseMessagePromptTemplate]] = None,\n    ) -&gt; BasePromptTemplate:\n        \"\"\"Create prompt for this agent.\n\n        Args:\n            system_message: Message to use as the system message that will be the\n                first in the prompt.\n            extra_prompt_messages: Prompt messages that will be placed between the\n                system message and the new human input.\n\n        Returns:\n            A prompt template to pass into this agent.\n        \"\"\"\n        _prompts = extra_prompt_messages or []\n        messages: List[Union[BaseMessagePromptTemplate, BaseMessage]]\n        if system_message:\n            messages = [system_message]\n        else:\n            messages = []\n\n        messages.extend(\n            [\n                *_prompts,\n                HumanMessagePromptTemplate.from_template(\"{input}\"),\n                MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n            ]\n        )\n        return ChatPromptTemplate(messages=messages)\n</code></pre> <p>Final prompt:</p> <pre><code>ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessage(content='You are a helpful AI assistant.'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])\n</code></pre> <p>\u5b9f\u884c</p> <p>\u521d\u671f\u5316\u3067\u4f5c\u6210\u3055\u308c\u305f\u5171\u901a\u306e\u51e6\u7406\u3067<code>AgentExecutor(input)</code>\u304c\u547c\u3070\u308c\u305f\u3068\u304d\u306b<code>Chain.__call__</code> -&gt; <code>AgentExecutor._call</code> -&gt; <code>self._take_next_step</code> -&gt; <code>self._iter_next_step</code> -&gt; <code>self.agent.plan(intermediate_steps, callbacks, **inputs)</code>\u3068\u547c\u3070\u308c\u308b</p> <p>plan</p> <p>intermediate_steps\u3092\u53d7\u3051\u53d6\u3063\u3066\u3001<code>llm</code>\u306b\u805e\u3044\u3066\u3001AgentAction\u307e\u305f\u306fAgentFinish\u3092\u8fd4\u3059</p> <pre><code>    def plan(\n        self,\n        intermediate_steps: List[Tuple[AgentAction, str]],\n        callbacks: Callbacks = None,\n        with_functions: bool = True,\n        **kwargs: Any,\n    ) -&gt; Union[AgentAction, AgentFinish]:\n</code></pre> <ol> <li>\u5f15\u6570\u306e\u5024<ol> <li><code>intermediate_steps</code>\u306f\u3001<code>[(AgentAction, text), (AgentAction, text), ...]</code>\u306e\u3088\u3046\u306a\u5f62\u5f0f\u3067\u6e21\u3055\u308c\u308b\u304c\u3001\u521d\u671f\u5024\u306f<code>[]</code>\u3067\u3042\u308b</li> <li><code>**kwargs</code>\u306b\u306f<code>**inputs</code>\u304c\u6e21\u3055\u308c\u3066\u3044\u308b\u304c\u3001\u5177\u4f53\u7684\u306b\u306f\u3001<code>{\"input\": inputs}</code>\u3068\u306a\u308b\u3002</li> <li>\u305f\u3069\u308a\u65b9\u306f\u3001<code>agent.plan</code>\u3092\u547c\u3076<code>AgentExecutor._iter_next_step</code>\u306e\u5f15\u6570\u306e<code>inputs</code>\u3067\u3001<code>AgentExecutor._take_next_step</code>\u306e\u5f15\u6570\u306e<code>inputs</code>\u3067\u3001<code>AgentExecutor._call</code>\u306e\u5f15\u6570\u306e<code>inputs</code>\u3067\u3042\u308b\u3002</li> <li>\u66f4\u306b\u305f\u3069\u308b\u3068Chain.call\u306e\u4e2d\u3067<code>inputs = self.prep_inputs(inputs)</code>\u3067\u3001<code>AgentExecutor(inputs)</code>\u304b\u3089\u6e21\u3063\u3066\u304d\u305f<code>inputs</code>\u306e\u5909\u63db\u51e6\u7406\u304c\u884c\u308f\u308c\u3066\u3044\u308b\u3002\u8a73\u7d30\u306f\u7701\u304f\u304c\u3001<code>inputs</code>\u304cstr\u3067\u6e21\u3055\u308c\u305f\u5834\u5408\u306b\u306f\u3001<code>{\"input\": inputs}</code>\u306b\u5909\u63db\u3055\u308c\u308b\u3002</li> <li>\u306a\u305c\u306a\u3089OpenAIFunctionsAgent.input_keys\u306f<code>[\"input\"]</code>\u3067\u3001<code>_input_keys = set(self.input_keys)</code> -&gt; <code>inputs = {list(_input_keys)[0]: inputs}</code>\u3068\u30bb\u30c3\u30c8\u3055\u308c\u3066\u3044\u308b\u304b\u3089\u3067\u3042\u308b\u3002</li> </ol> </li> <li><code>agent_scratchpad = format_to_openai_function_messages(intermediate_steps)</code><ol> <li>format_to_openai_function_messages\u3067<code>agent_scratchpad</code>\u3092\u751f\u6210</li> <li>\u8fd4\u308a\u5024\u306f\u3001<code>List[BaseMessage]</code>\u4eca\u307e\u3067\u306eAgentAction\u3068\u7d50\u679c\u3092\u307e\u3068\u3081\u305f\u3082\u306e</li> <li>intermediate_steps\u304c<code>[]</code>\u306e\u5834\u5408\u306b\u306f\u3001<code>agent_scratchpad</code>\u306f<code>[]</code>\u3068\u306a\u308b</li> </ol> </li> <li><code>selected_inputs = { k: kwargs[k] for k in self.prompt.input_variables if k != \"agent_scratchpad\" }</code> full_inputs\u3092\u4f5c\u6210<ol> <li><code>self.prompt.input_variables</code>\u306f\u3001<code>['agent_scratchpad', 'input']</code>\u306a\u306e\u3067\u3001\u5b9f\u8ceak\u306f<code>input</code>\u306e\u307f\u3002<code>kwargs</code>\u306f<code>{\"input\": inputs}</code>\u306a\u306e\u3067\u3001<code>selected_inputs</code>\u306f<code>{\"input\": inputs}</code>\u3068\u306a\u308b\u3002\u4f8b\u3048\u3070\u3001<code>AgentExecutor(\"\u65e5\u672c\u306e\u4eba\u53e3\u306f\uff1f\")</code>\u306e\u5834\u5408\u306b\u306f\u3001<code>selected_inputs</code>\u306f<code>{\"input\": \"\u65e5\u672c\u306e\u4eba\u53e3\u306f\uff1f\"}</code>\u3068\u306a\u308b\u3002</li> </ol> </li> <li><code>full_inputs = dict(selected_inputs, agent_scratchpad=agent_scratchpad)</code> full_inputs\u3092\u4f5c\u6210<ol> <li><code>full_inputs</code>\u306f\u3001<code>{\"input\": inputs, \"agent_scratchpad\": agent_scratchpad}</code>\u3068\u306a\u308b\u3002\u4f8b\u3048\u3070\u3001<code>AgentExecutor(\"\u65e5\u672c\u306e\u4eba\u53e3\u306f\uff1f\")</code>\u306e\u5834\u5408\u306b\u306f\u3001<code>full_inputs</code>\u306f<code>{\"input\": \"\u65e5\u672c\u306e\u4eba\u53e3\u306f\uff1f\", \"agent_scratchpad\": []}</code>\u3068\u306a\u308b\u3002</li> </ol> </li> <li>input_variables\u3068\u3042\u308f\u305b\u3066 <code>prompt = self.prompt.format_prompt(**full_inputs)</code>\u3067prompt\u306e\u751f\u6210<ol> <li>\u4e0a\u306e\u4f8b\u3060\u3068\u3001prompt\u306f\u3001<code>ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI assistant.'), HumanMessage(content='\u65e5\u672c\u306e\u4eba\u53e3\u306f')])</code>\u3068\u306a\u308b</li> </ol> </li> <li><code>messages = prompt.to_messages()</code> \u3067message\u3092\u751f\u6210<ol> <li>\u4e0a\u306e\u4f8b\u3060\u3068\u3001<code>messages</code>\u306f\u3001<code>[SystemMessage(content='You are a helpful AI assistant.'), HumanMessage(content='\u65e5\u672c\u306e\u4eba\u53e3\u306f')]</code>\u3068\u306a\u308b</li> </ol> </li> <li> <p><code>predicted_message = self.llm.predict_messages()</code>\u3092\u4f7f\u3063\u3066llm\u306b\u805e\u304f</p> <pre><code>if with_functions:\n    predicted_message = self.llm.predict_messages(\n        messages,\n        functions=self.functions,\n        callbacks=callbacks,\n    )\nelse:\n    predicted_message = self.llm.predict_messages(\n        messages,\n        callbacks=callbacks,\n    )\n</code></pre> <pre><code>&gt;&gt;&gt; agent_executor.agent.functions\n[{'name': 'Search', 'description': 'useful for when you need to answer questions about current events. You should ask targeted questions', 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}]\n</code></pre> <p>BaseChatModel.predict_messages (ChatOpenAI\u306e\u5834\u5408)</p> <p><pre><code>def predict_messages(\n    self,\n    messages: List[BaseMessage],\n    *,\n    stop: Optional[Sequence[str]] = None,\n    **kwargs: Any,\n) -&gt; BaseMessage:\n    if stop is None:\n        _stop = None\n    else:\n        _stop = list(stop)\n    return self(messages, stop=_stop, **kwargs)\n</code></pre> 1.  \u5b9f\u969b\u306b\u306f\u3001<code>predict_messages(messages, stop=None, functions=agent_executor.agent.functions)</code>\u304c\u5165\u308c\u3089\u308c\u3066\u547c\u3070\u308c\u308b \u2192 <code>return self(text, stop=_stop, **kwargs)</code>     1. BaseChatModel.__call__     1. BaseChatModel.generate     1. \u5404messages\u306b\u5bfe\u3057\u3066 BaseChatModel._generate_with_cache(m, stop=stop, run_manager=run_managers[0], **kwargs) \u304c\u547c\u3070\u308c\u308b     1. <code>self._generate</code>\u304c\u547c\u3070\u308c\u308b\u304c\u5b9f\u88c5\u306fChatOpenAI._generate         1. <code>message_dicts, params = self._create_message_dicts(messages, stop)</code>: messages \u3068stop\u304b\u3089message_dicts\u3068params\u3092\u751f\u6210             1. <code>message_dicts = [convert_message_to_dict(m) for m in messages]</code>         1. <code>ChatOpenAI.completion_with_retry(messages=message_dicts, run_manager=run_manager, **params)</code>     1. ChatOpenAI.client.create(**kwargs): <code>**kwargs</code>\u306e\u5b9f\u969b\u306e\u5024\u306f<code>messages=message_dicts, **params</code>     1. function-calling \u6700\u7d42\u7684\u306b\u306f openai\u306eclient\u3067Functions\u304c\u547c\u3079\u308b\u3088\u3046\u306b\u306a\u3063\u3066\u3044\u308b         <pre><code>messages = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco, Tokyo, and Paris?\"}]\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n                    },\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\nresponse = client.chat.completions.create(\n    model=\"gpt-3.5-turbo-1106\",\n    messages=messages,\n    tools=tools,\n    tool_choice=\"auto\",  # auto is default, but we'll be explicit\n)\n</code></pre>     \u6700\u7d42\u7684\u306b <code>LLMResult</code>\u304c\u8fd4\u3055\u308c\u3066\u3044\u308b\u3002 1. functions     <pre><code>@property\ndef functions(self) -&gt; List[dict]:\n    return [dict(format_tool_to_openai_function(t)) for t in self.tools]\n</code></pre> 1. \u7c21\u5358\u306a\u4f8b     <pre><code>from langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentType, Tool, initialize_agent\nfrom langchain.tools.render import format_tool_to_openai_function\nfrom langchain_community.utilities import SerpAPIWrapper\nllm = ChatOpenAI()\nsearch = SerpAPIWrapper()\ntools = [\n    Tool(\n        name=\"Search\",\n        func=search.run,\n        description=\"useful for when you need to answer questions about current events. You should ask targeted questions\",\n    ),\n]\nagent_executor = initialize_agent(\n    tools, llm, agent=AgentType.OPENAI_FUNCTIONS, verbose=True\n)\nfunctions = [dict(format_tool_to_openai_function(t)) for t in tools]\nfull_inputs = {'input': \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\", 'agent_scratchpad': []}\nprompt = agent_executor.agent.prompt.format_prompt(**full_inputs)\nmessages = prompt.to_messages() # [SystemMessage(content='You are a helpful AI assistant.'), HumanMessage(content=\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")]\nllm(messages, stop=None, functions=functions) # agent_executor.agent.functions\n# return: AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"__arg1\": \"Leo DiCaprio\\'s girlfriend\"\\n}', 'name': 'Search'}})\n</code></pre> 1. AIMessage\u3092Output Parser\u3067Parse\u3057\u3066\u3001AgentAction\u307e\u305f\u306fAgentFinish\u3092\u8fd4\u3059 <pre><code>agent_decision = OpenAIFunctionsAgentOutputParser._parse_ai_message(\n    predicted_message\n)\n</code></pre> 1. OpenAIFunctionsAgentOutputParser\u3067\u3001<code>_parse_ai_message</code>\u3092\u4f7f\u3063\u3066Output\u3092Parse\u3057\u3066<code>AgentFinish</code>\u307e\u305f\u306f\u3001AgentActionMessageLog(AgentAction)\u3092\u8fd4\u3059 1. \u5177\u4f53\u7684\u306b\u306f\u3001</p> </li> </ol>"},{"location":"langchain/agent/agent_old/#custom-agent-api-integration","title":"Custom Agent (API integration)","text":"<p><code>OpenAIFunctionsAgent</code>\u3092\u4f7f\u3046\u304c\u3001 JQL\u3084Sourcegraph\u306e\u3088\u3046\u306a\u81ea\u7531\u5ea6\u306e\u9ad8\u3044Query\u3092\u4f7f\u3063\u3066\u691c\u7d22\u3067\u304d\u308b\u5916\u90e8API\u306b\u95a2\u3057\u3066\u306f\u3001API\u306eReference\u3082Input\u306b\u3057\u3066\u3042\u3052\u305f\u65b9\u304c\u3044\u3044\u3002</p> <ol> <li>\u57fa\u672c\u306f<code>OpenAIFunctionsAgent</code> or <code>STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION</code> (?)\u3092\u4f7f\u3046 (\u3069\u306eAgentType\u304c\u4e00\u756a\u3044\u3044\u306e\u304b\u3082\u691c\u8a3c)</li> <li>\u30a8\u30e9\u30fc\u51e6\u7406\u306e\u6bb5\u968e\u3067\u3001API Reference\u3092\u4f7f\u3063\u3066\u3001\u6b63\u3057\u3044\u5165\u529b\u304c\u3067\u304d\u308b\u3088\u3046\u306b\u3059\u308b (API reference\u306f\u983b\u7e41\u306b\u5909\u308f\u3089\u306a\u3044\u306e\u3067Cache\u306a\u3069\u306b\u5165\u308c\u3066\u7f6e\u3051\u308b\u3068\u306a\u304a\u826f\u3044)</li> </ol> <p>\u89e3\u6c7a\u3057\u305f\u3044\u8ab2\u984c: 1. \u5916\u90e8API\u3092\u53e9\u304f\u5834\u5408\u306bReference\u3092\u53c2\u7167\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u305f\u3044</p> <p>\u4f8b. LangChain\u30ab\u30b9\u30bf\u30e0\u30c4\u30fc\u30eb\u3092\u4f5c\u3063\u3066AI Agent\u306b\u6e21\u3057\u3066\u307f\u305f &lt;- \u3053\u3061\u3089\u304c\u7d20\u6674\u3089\u3057\u3059\u304e\u305f\u306e\u3067\u3001\u3053\u308c\u3092\u53c2\u8003\u306b\u3059\u308b</p> <pre><code>poetry run python src/langchain/agent_custom.py -e search\n</code></pre> <p>\u3053\u3061\u3089\u306e\u4f8b\u3067\u306f\u3001 Google Search -&gt; VectorStore -&gt; \u691c\u7d22\u53ef\u80fd \u3068\u3044\u3046\u304b\u306a\u308a\u62e1\u5f35\u6027\u306e\u9ad8\u3044Agent\u3092\u4f5c\u6210\u3057\u3066\u3044\u308b\u3002</p>"},{"location":"langchain/agent/agent_old/#faq","title":"FAQ","text":"<ol> <li>\u3044\u3064\u3001\u3069\u306eAgentType\u3092\u4f7f\u3046\u306e\u304c\u3044\u3044\u306e\u304b</li> <li>Agent\u3092\u4f7f\u3046\u306e\u306f\u3069\u3046\u3044\u3046\u3068\u304d\u304b</li> <li>ReAct\u3068Agent\u306e\u9055\u3044\u306f\u306a\u306b\u304b</li> <li>LCEL\u3068Agent\u306e\u9055\u3044\u306f\u306a\u306b\u304b</li> <li> <p><code>value is not a valid list (type=type_error.list)</code></p> <pre><code>  File \"/Users/m.naka/repos/nakamasato/gpt-training/python/search/libs/multiturn_agent.py\", line 640, in &lt;module&gt;\n    ai = agent.run(input=user)\n         ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/m.naka/repos/nakamasato/gpt-training/.venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 512, in run\n    return self(kwargs, callbacks=callbacks, tags=tags, metadata=metadata)[\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/m.naka/repos/nakamasato/gpt-training/.venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 314, in __call__\n    final_outputs: Dict[str, Any] = self.prep_outputs(\n                                    ^^^^^^^^^^^^^^^^^^\n  File \"/Users/m.naka/repos/nakamasato/gpt-training/.venv/lib/python3.11/site-packages/langchain/chains/base.py\", line 410, in prep_outputs\n    self.memory.save_context(inputs, outputs)\n  File \"/Users/m.naka/repos/nakamasato/gpt-training/.venv/lib/python3.11/site-packages/langchain/memory/chat_memory.py\", line 39, in save_context\n    self.chat_memory.add_ai_message(output_str)\n  File \"/Users/m.naka/repos/nakamasato/gpt-training/.venv/lib/python3.11/site-packages/langchain_core/chat_history.py\", line 59, in add_ai_message\n    self.add_message(AIMessage(content=message))\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/m.naka/repos/nakamasato/gpt-training/.venv/lib/python3.11/site-packages/langchain_core/load/serializable.py\", line 97, in __init__\n    super().__init__(**kwargs)\n  File \"/Users/m.naka/repos/nakamasato/gpt-training/.venv/lib/python3.11/site-packages/pydantic/v1/main.py\", line 341, in __init__\n    raise validation_error\npydantic.v1.error_wrappers.ValidationError: 2 validation errors for AIMessage\ncontent\n  str type expected (type=type_error.str)\ncontent\n  value is not a valid list (type=type_error.list)\n</code></pre> </li> </ol>"},{"location":"langchain/agent/examples/","title":"create_pandas_dataframe_agent","text":"<p>https://api.python.langchain.com/en/latest/agents/langchain_experimental.agents.agent_toolkits.pandas.base.create_pandas_dataframe_agent.html</p> <ol> <li><code>df</code>: can ba a list of DataFrames or a single DataFrame</li> <li><code>tools</code>: <code>[PythonAstREPLTool(locals=df_locals)]</code> and you can add <code>extra_tools</code></li> <li>agent_type<ol> <li><code>openai-tools</code></li> <li><code>openai-functions</code></li> <li><code>zero-shot-react-description</code>: default</li> </ol> </li> <li>prompt<ol> <li>zero-shot-react-description:<code>_get_prompt</code></li> <li>openai-tools or openap-functions: <code>_get_functions_prompt</code></li> </ol> </li> </ol> <pre><code>from langchain_openai import ChatOpenAI\nfrom langchain_experimental.agents import create_pandas_dataframe_agent\nfrom langchain.agents.agent_types import AgentType\nfrom langchain_experimental.tools.python.tool import PythonAstREPLTool\nimport pandas as pd\ndf = pd.read_csv(\"data/titanic.csv\")\n\nagent = create_pandas_dataframe_agent(\n    ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\"),\n    df,\n    verbose=True,\n    agent_type=AgentType.OPENAI_FUNCTIONS,\n)\ntools = [PythonAstREPLTool(locals=df)] # default\n</code></pre>"},{"location":"langchain/agent/examples/#prompt","title":"Prompt","text":"<p>mrkl/prompt.py</p> <pre><code>FORMAT_INSTRUCTIONS = \"\"\"Use the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\"\"\"\n</code></pre>"},{"location":"langchain/agent/examples/#one_shot_react_description-single-df","title":"ONE_SHOT_REACT_DESCRIPTION + Single DF","text":"<p>_get_single_prompt</p> <pre><code>PREFIX = \"\"\"\nYou are working with a pandas dataframe in Python. The name of the dataframe is `df`.\nYou should use the tools below to answer the question posed of you:\"\"\"\n\nSUFFIX_NO_DF = \"\"\"\nBegin!\nQuestion: {input}\n{agent_scratchpad}\"\"\"\n\nSUFFIX_WITH_DF = \"\"\"\nThis is the result of `print(df.head())`:\n{df_head}\n\nBegin!\nQuestion: {input}\n{agent_scratchpad}\"\"\"\n</code></pre> <pre><code>template = \"\\n\\n\".join([prefix, \"{tools}\", FORMAT_INSTRUCTIONS, suffix_to_use])\nprompt = PromptTemplate.from_template(template)\n</code></pre> <pre><code>print(\"\\n\\n\".join([PREFIX, \"{tools}\", FORMAT_INSTRUCTIONS, SUFFIX_WITH_DF]))\n</code></pre> <pre><code>You are working with a pandas dataframe in Python. The name of the dataframe is `df`.\nYou should use the tools below to answer the question posed of you:\n\n{tools}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\n\nThis is the result of `print(df.head())`:\n{df_head}\n\nBegin!\nQuestion: {input}\n{agent_scratchpad}\n</code></pre>"},{"location":"langchain/agent/examples/#one_shot_react_description-multi-df","title":"ONE_SHOT_REACT_DESCRIPTION + Multi DF","text":"<pre><code>MULTI_DF_PREFIX = \"\"\"\nYou are working with {num_dfs} pandas dataframes in Python named df1, df2, etc. You\nshould use the tools below to answer the question posed of you:\"\"\"\n\nSUFFIX_WITH_MULTI_DF = \"\"\"\nThis is the result of `print(df.head())` for each dataframe:\n{dfs_head}\n\nBegin!\nQuestion: {input}\n{agent_scratchpad}\"\"\"\n</code></pre> <pre><code>print(\"\\n\\n\".join([MULTI_DF_PREFIX, \"{tools}\", FORMAT_INSTRUCTIONS, SUFFIX_WITH_MULTI_DF]))\n</code></pre> <pre><code>You are working with {num_dfs} pandas dataframes in Python named df1, df2, etc. You\nshould use the tools below to answer the question posed of you:\n\n{tools}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\n\nThis is the result of `print(df.head())` for each dataframe:\n{dfs_head}\n\nBegin!\nQuestion: {input}\n{agent_scratchpad}\n</code></pre>"},{"location":"langchain/agent/examples/#openai_functions-single-df","title":"OPENAI_FUNCTIONS + Single DF","text":"<pre><code>PREFIX_FUNCTIONS = \"\"\"\nYou are working with a pandas dataframe in Python. The name of the dataframe is `df`.\"\"\"\nFUNCTIONS_WITH_DF = \"\"\"\nThis is the result of `print(df.head())`:\n{df_head}\"\"\"\n</code></pre> <pre><code>from langchain_core.messages import SystemMessage\nnumber_of_head_rows: int = 5\ndf_head = str(df.head(number_of_head_rows).to_markdown())\nsuffix = FUNCTIONS_WITH_DF.format(df_head=df_head)\nPREFIX_FUNCTIONS + suffix\n</code></pre> <pre><code>You are working with a pandas dataframe in Python. The name of the dataframe is `df`.\nThis is the result of `print(df.head())`:\n|    |   PassengerId |   Survived |   Pclass | Name                                                | Sex    |   Age |   SibSp |   Parch | Ticket           |    Fare | Cabin   | Embarked   |\n|---:|--------------:|-----------:|---------:|:----------------------------------------------------|:-------|------:|--------:|--------:|:-----------------|--------:|:--------|:-----------|\n|  0 |             1 |          0 |        3 | Braund, Mr. Owen Harris                             | male   |    22 |       1 |       0 | A/5 21171        |  7.25   | nan     | S          |\n|  1 |             2 |          1 |        1 | Cumings, Mrs. John Bradley (Florence Briggs Thayer) | female |    38 |       1 |       0 | PC 17599         | 71.2833 | C85     | C          |\n|  2 |             3 |          1 |        3 | Heikkinen, Miss. Laina                              | female |    26 |       0 |       0 | STON/O2. 3101282 |  7.925  | nan     | S          |\n|  3 |             4 |          1 |        1 | Futrelle, Mrs. Jacques Heath (Lily May Peel)        | female |    35 |       1 |       0 | 113803           | 53.1    | C123    | S          |\n|  4 |             5 |          0 |        3 | Allen, Mr. William Henry                            | male   |    35 |       0 |       0 | 373450           |  8.05   | nan     | S          |\n</code></pre>"},{"location":"langchain/agent/examples/#openai_functions-multi-df","title":"OPENAI_FUNCTIONS + Multi DF","text":"<p>prefix:</p> <pre><code>MULTI_DF_PREFIX_FUNCTIONS = \"\"\"\nYou are working with {num_dfs} pandas dataframes in Python named df1, df2, etc.\"\"\"\n\nFUNCTIONS_WITH_MULTI_DF = \"\"\"\nThis is the result of `print(df.head())` for each dataframe:\n{dfs_head}\"\"\"\n</code></pre> <p>almost same as one for single df</p>"},{"location":"langchain/agent/examples/#create_csv_agent","title":"create_csv_agent","text":"<p>Exactly same as <code>create_pandas_dataframe_agent</code>.</p> <pre><code>return create_pandas_dataframe_agent(llm, df, **kwargs)\n</code></pre> <p>Run:</p> <pre><code>poetry run python src/examples/agent_toolkit_csv.py\n</code></pre> <pre><code>&gt; Entering new AgentExecutor chain...\n\nInvoking: `python_repl_ast` with `{'query': 'df.shape[0]'}`\n\n\n891There are 891 rows in the dataframe.\n\n&gt; Finished chain.\n{'input': 'how many rows are there?', 'output': 'There are 891 rows in the dataframe.'}\n</code></pre>"},{"location":"langchain/agent/prompt/","title":"Agent Prompt","text":""},{"location":"langchain/agent/prompt/#hwchase17react-chat","title":"hwchase17/react-chat","text":"<pre><code>Assistant is a large language model trained by OpenAI.\n\nAssistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n\nAssistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n\nOverall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n\nTOOLS:\n------\n\nAssistant has access to the following tools:\n\n{tools}\n\nTo use a tool, please use the following format:\n\n```\nThought: Do I need to use a tool? Yes\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n```\n\nWhen you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n\n```\nThought: Do I need to use a tool? No\nFinal Answer: [your response here]\n```\n\nBegin!\n\nPrevious conversation history:\n{chat_history}\n\nNew input: {input}\n{agent_scratchpad}\n</code></pre>"},{"location":"langchain/agent/prompt/#hwchase17react","title":"hwchase17/react","text":"<pre><code>Answer the following questions as best you can. You have access to the following tools:\n\n{tools}\n\nUse the following format:\n\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAction: the action to take, should be one of [{tool_names}]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\n\nBegin!\n\nQuestion: {input}\nThought:{agent_scratchpad}\n</code></pre>"},{"location":"langchain/agent/prompt/#hwchase17structured-chat-agent","title":"hwchase17/structured-chat-agent","text":"<pre><code>Respond to the human as helpfully and accurately as possible. You have access to the following tools:\n\n{tools}\n\nUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n\nValid \"action\" values: \"Final Answer\" or {tool_names}\n\nProvide only ONE action per $JSON_BLOB, as shown:\n\n```\n{{\n  \"action\": $TOOL_NAME,\n  \"action_input\": $INPUT\n}}\n```\n\nFollow this format:\n\nQuestion: input question to answer\nThought: consider previous and subsequent steps\nAction:\n```\n$JSON_BLOB\n```\nObservation: action result\n... (repeat Thought/Action/Observation N times)\nThought: I know what to respond\nAction:\n```\n{{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"Final response to human\"\n}}\n\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\n</code></pre>"},{"location":"langchain/agent/simple_example/","title":"Simple Example (LangChain 0.1)","text":""},{"location":"langchain/agent/simple_example/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>langchain</code> 0.1</li> <li><code>TAVILY_API_KEY</code></li> </ul>"},{"location":"langchain/agent/simple_example/#steps","title":"Steps","text":""},{"location":"langchain/agent/simple_example/#tool1","title":"Tool1","text":"<pre><code>from langchain_community.tools.tavily_search import TavilySearchResults\n\nsearch = TavilySearchResults()\n\nsearch.invoke(\"what is the weather in SF\")\n</code></pre> <pre><code>&gt;&gt;&gt; search.invoke(\"what is the weather in SF\")\n[{'url': 'https://www.weatherapi.com/', 'content': \"{'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.78, 'lon': -122.42, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1716417732, 'localtime': '2024-05-22 15:42'}, 'current': {'last_updated_epoch': 1716417000, 'last_updated': '2024-05-22 15:30', 'temp_c': 19.4, 'temp_f': 66.9, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 15.0, 'wind_kph': 24.1, 'wind_degree': 260, 'wind_dir': 'W', 'pressure_mb': 1014.0, 'pressure_in': 29.95, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 61, 'cloud': 0, 'feelslike_c': 19.4, 'feelslike_f': 66.9, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 15.6, 'gust_kph': 25.1}}\"}, {'url': 'https://www.yahoo.com/news/may-22-2024-san-francisco-121934270.html', 'content': 'KRON San Francisco. May 22, 2024 San Francisco Bay Area weather forecast. KRON San Francisco. May 22, 2024 at 8:19 AM. Read full article. KRON4 Meteorologist John Shrable has the latest weather ...'}, {'url': 'https://weatherspark.com/h/m/557/2024/5/Historical-Weather-in-May-2024-in-San-Francisco-California-United-States', 'content': 'San Francisco Temperature History May 2024. The daily range of reported temperatures (gray bars) and 24-hour highs (red ticks) and lows (blue ticks), placed over the daily average high (faint red line) and low (faint blue line) temperature, with 25th to 75th and 10th to 90th percentile bands.'}, {'url': 'https://www.wunderground.com/hourly/us/ca/san-francisco/94130/date/2024-05-22', 'content': 'San Francisco Weather Forecasts. Weather Underground provides local &amp; long-range weather forecasts, weatherreports, maps &amp; tropical weather conditions for the San Francisco area. ... Wednesday 05/ ...'}, {'url': 'https://www.weathertab.com/en/c/e/05/united-states/california/san-francisco/', 'content': 'Explore comprehensive May 2024 weather forecasts for San Francisco, including daily high and low temperatures, precipitation risks, and monthly temperature trends. Featuring detailed day-by-day forecasts, dynamic graphs of daily rain probabilities, and temperature trends to help you plan ahead. ... 22 64\u00b0F 50\u00b0F 18\u00b0C 10\u00b0C 11% 23 64\u00b0F 51\u00b0F ...'}]\n</code></pre>"},{"location":"langchain/agent/simple_example/#tool2","title":"Tool2","text":"<p>https://python.langchain.com/v0.1/docs/modules/tools/custom_tools/</p> <pre><code>from langchain.tools import tool\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n</code></pre> <pre><code>&gt;&gt;&gt; multiply\nStructuredTool(name='multiply', description='multiply(a: int, b: int) -&gt; int - Multiply two numbers.', args_schema=&lt;class 'pydantic.v1.main.multiplySchema'&gt;, func=&lt;function multiply at 0x10d809da0&gt;)\n</code></pre> <pre><code>&gt;&gt;&gt; multiply.invoke({\"a\": 1, \"b\": 2})\n2\n</code></pre>"},{"location":"langchain/agent/simple_example/#set-tools","title":"Set tools","text":"<pre><code>tools = [search, multiply]\n</code></pre>"},{"location":"langchain/agent/simple_example/#chat","title":"Chat","text":"<pre><code>from langchain_openai import ChatOpenAI\nllm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n</code></pre>"},{"location":"langchain/agent/simple_example/#prompts","title":"Prompts","text":"<pre><code>from langchain import hub\n\n# Get the prompt to use - you can modify this!\nprompt = hub.pull(\"hwchase17/openai-functions-agent\")\nprompt.messages\n</code></pre> <p>langchainhub\u3092\u3084\u3081\u305f</p> <pre><code>&gt;&gt;&gt; prompt.messages\n[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')]\n</code></pre>"},{"location":"langchain/agent/simple_example/#agent","title":"Agent","text":"<pre><code>from langchain.agents.tool_calling_agent.base import create_tool_calling_agent\nagent = create_tool_calling_agent(llm, tools, prompt)\n</code></pre> <pre><code>agent.invoke({\"input\": \"hi\", \"intermediate_steps\": []})\n</code></pre> <pre><code>&gt;&gt;&gt; agent.invoke({\"input\": \"hi\", \"intermediate_steps\": []})\nAgentFinish(return_values={'output': 'Hello! How can I assist you today?'}, log='Hello! How can I assist you today?')\n</code></pre>"},{"location":"langchain/agent/simple_example/#agentexecutor","title":"AgentExecutor","text":"<pre><code>from langchain.agents import AgentExecutor\n\nagent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n</code></pre> <p>Example1:</p> <pre><code>agent_executor.invoke({\"input\": \"What's the weather in SF?\"})\n</code></pre> <p>Example2:</p> <pre><code>agent_executor.invoke({\"input\": \"What's 2 times 3?\"})\n</code></pre>"},{"location":"langchain/agent/simple_example/#details","title":"Details","text":""},{"location":"langchain/agent/simple_example/#agentsteps0","title":"Agent.steps[0]","text":"<pre><code>&gt;&gt;&gt; agent.steps[0]\nRunnableAssign(mapper={\n  agent_scratchpad: RunnableLambda(lambda x: format_to_tool_messages(x['intermediate_steps']))\n})\n</code></pre> <p>format_to_tool_messages: Convert (AgentAction, tool output) tuples into FunctionMessages.</p> <pre><code>from langchain.agents.format_scratchpad.tools import (\n    format_to_tool_messages,\n)\n</code></pre> <pre><code>def format_to_tool_messages(\n    intermediate_steps: Sequence[Tuple[AgentAction, str]],\n) -&gt; List[BaseMessage]:\n    \"\"\"Convert (AgentAction, tool output) tuples into FunctionMessages.\n\n    Args:\n        intermediate_steps: Steps the LLM has taken to date, along with observations\n\n    Returns:\n        list of messages to send to the LLM for the next prediction\n\n    \"\"\"\n    messages = []\n    for agent_action, observation in intermediate_steps:\n        if isinstance(agent_action, ToolAgentAction):\n            new_messages = list(agent_action.message_log) + [\n                _create_tool_message(agent_action, observation)\n            ]\n            messages.extend([new for new in new_messages if new not in messages])\n        else:\n            messages.append(AIMessage(content=agent_action.log))\n    return messages\n</code></pre>"},{"location":"langchain/agent/simple_example/#agentsteps1prompt","title":"Agent.steps[1].prompt","text":"<pre><code>&gt;&gt;&gt; agent.steps[1].invoke({\"input\": \"hi,\", \"agent_scratchpad\": []})\nChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='hi,')])\n&gt;&gt;&gt; agent.steps[1].input_types['agent_scratchpad']\ntyping.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]\n&gt;&gt;&gt; import langchain_core\n&gt;&gt;&gt; agent.steps[1].invoke({\"input\": \"hi,\", \"agent_scratchpad\": [langchain_core.messages.ai.AIMessage(content='a')]})\nChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='hi,'), AIMessage(content='a')])\n</code></pre> <p><code>agent_scratchpad</code>: is set by step[0]</p>"},{"location":"langchain/langgraph/","title":"LangGraph","text":""},{"location":"langchain/langgraph/#overview","title":"Overview","text":"<p>LangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of (and intended to be used with) LangChain.</p> <p>When</p> <p>The main use is for adding cycles to your LLM application. examples:</p> <ol> <li>if there's an order to execute agents (e.g. get information and then draw a chart)</li> </ol> <ol> <li>AgentState: Defined by <code>TypedDict</code> with</li> <li>StateGraph:</li> <li>Node: Agent (call model) or Tool (call tool). Each node returns operations to update that state. These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute.</li> <li>Edge:<ol> <li>A conditional edge can be defined by <code>add_conditional_edge</code></li> <li>An edge can be defined by <code>add_edge</code></li> </ol> </li> </ol>"},{"location":"langchain/langgraph/#getting-started","title":"Getting Started","text":"<ol> <li> <p>Create a tool and <code>ToolExecutor</code></p> <pre><code>from langgraph.prebuilt import ToolExecutor\ntool_executor = ToolExecutor(tools)\n</code></pre> </li> <li> <p>Set up a model and bind tools</p> <pre><code>model = ChatOpenAI(temperature=0, streaming=True)\n\nfrom langchain.tools.render import format_tool_to_openai_function\n\nfunctions = [format_tool_to_openai_function(t) for t in tools]\nmodel = model.bind_functions(functions)\n</code></pre> </li> <li> <p>Define <code>AgentState</code></p> <pre><code>from typing import TypedDict, Annotated, Sequence\nimport operator\nfrom langchain_core.messages import BaseMessage\n\n\nclass AgentState(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n</code></pre> </li> <li> <p>Define node</p> <p>Note</p> <p>node returns <code>{\"messages\": [response]}</code>, which is used to update <code>AgentState</code>.</p> <ol> <li> <p>Agent: <code>call_model</code></p> <pre><code>def call_model(state):\n    messages = state[\"messages\"]\n    response = model.invoke(messages)\n    # We return a list, because this will get added to the existing list\n    return {\"messages\": [response]}\n</code></pre> </li> <li> <p>Tool <code>action</code>: <code>call_tool</code></p> <pre><code>def call_tool(state):\n    ...\n    action = ToolInvocation(\n        tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n        tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"]),\n    )\n    response = tool_executor.invoke(action)\n    function_message = FunctionMessage(content=str(response), name=action.tool)\n    return {\"messages\": [function_message]}\n</code></pre> </li> </ol> </li> <li> <p>Define graph</p> <ol> <li> <p>Create workflow</p> <pre><code>workflow = StateGraph(AgentState)\n</code></pre> </li> <li> <p>Add nodes</p> <pre><code>workflow.add_node(\"agent\", call_model)\nworkflow.add_node(\"action\", call_tool)\n</code></pre> </li> <li> <p>Add entrypoint</p> <pre><code>workflow.set_entry_point(\"agent\")\n</code></pre> </li> <li> <p>Add Edge</p> <pre><code>workflow.add_conditional_edges(\"agent\", should_continue, { \"continue\": \"action\", \"end\": END} # conditional edge\nworkflow.add_edge(\"action\", \"agent\") # normal edge\n</code></pre> </li> <li> <p>Compile</p> <pre><code>app = workflow.compile()\n</code></pre> </li> </ol> </li> <li> <p>Use it.</p> <pre><code>inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\nprint(app.invoke(inputs))\n</code></pre> </li> <li> <p>Run</p> <pre><code>poetry run python src/examples/langgraph_sample.py\n</code></pre> <p><code>{'messages': [HumanMessage(content='what is the weather in sf'), AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"__arg1\": \"weather in San Francisco\"\\n}', 'name': 'google-search'}}), FunctionMessage(content=\"[{'title': '10-Day Weather Forecast for San Francisco, CA - The Weather ...', 'link': 'https://weather.com/weather/tenday/l/San+Francisco+CA+USCA0987:1:US', 'snippet': 'Be prepared with the most accurate 10-day forecast for San Francisco, CA with highs, lows, chance of precipitation from The Weather Channel and Weather.com.'}, {'title': 'San Francisco, CA Weather Forecast | AccuWeather', 'link': 'https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629', 'snippet': 'San Francisco, CA Weather Forecast, with current conditions, wind, air quality, and what to expect for the next 3 days.'}, {'title': '10-Day Weather Forecast for Inverness, CA - The Weather Channel ...', 'link': 'https://weather.com/weather/tenday/l/Inverness+CA?canonicalCityId=61b2ebcaa5e78eebca92d21eaff7a0439eb081e8e60287fca37af4186f8242b7', 'snippet': 'Sun 11 | Night. 40\u00b0. 9%. NE 6 mph. Partly cloudy early followed by cloudy skies overnight. Low near 40F. Winds NE at 5 to 10 mph. Humidity97%.'}, {'title': 'San Francisco, CA Weather Conditions | Weather Underground', 'link': 'https://www.wunderground.com/weather/us/ca/san-francisco', 'snippet': 'San Francisco Weather Forecasts. Weather Underground provides local &amp; long-range weather forecasts, weatherreports, maps &amp; tropical weather conditions for\\\\xa0...'}, {'title': 'Hourly Weather Forecast for San Francisco, CA - The Weather ...', 'link': 'https://weather.com/weather/hourbyhour/l/54f9d8baac32496f6b5497b4bf7a277c3e2e6cc5625de69680e6169e7e38e9a8', 'snippet': 'Hourly Local Weather Forecast, weather conditions, precipitation, dew point, humidity, wind from Weather.com and The Weather Channel.'}]\", name='google-search'), AIMessage(content=\"The weather in San Francisco can vary, but you can check the 10-day weather forecast on The Weather Channel's website [here](https://weather.com/weather/tenday/l/San+Francisco+CA+USCA0987:1:US). You can also find the current weather conditions and forecast on AccuWeather's website [here](https://www.accuweather.com/en/us/san-francisco/94103/weather-forecast/347629).\")]}</code></p> </li> </ol>"},{"location":"langchain/langgraph/#examples","title":"Examples","text":""},{"location":"langchain/langgraph/#basic-multi-agent-collaboration","title":"Basic Multi-agent Collaboration","text":"<p>Notes</p> <p>Route next action based on the output of the agent <code>Chart Generator</code> &lt;-&gt; <code>Researcher</code></p> <p>Instead of using <code>PythonREPL</code> and <code>TavilySearchResults</code> as tools of a single agent, we use a separate agent for each tool and control the next action with conditional edges.</p> <p>Inspired by https://github.com/microsoft/autogen</p> <p>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</p> <ol> <li>Nodes:<ol> <li>Agent <code>Chart Generator</code> (tool: <code>PythonREPL</code>): Call LLM to get the arguments.</li> <li>Agent <code>Researcher</code> (tool: <code>TavilySearchResults</code>): Call LLM to get the arguments.</li> <li>Tool <code>call_tool</code>: invoke the tool specified in the last message</li> </ol> </li> <li>Edges: Conditional Edge<ol> <li><code>router</code><ol> <li>last_message.additional_kwargs contains <code>function_call</code> -&gt;<code>call_tool</code></li> <li><code>FINAL_ANSWER</code> -&gt; <code>end</code></li> <li>Otherwise, <code>continue</code></li> </ol> </li> <li><code>Researcher</code> -&gt; <code>{\"continue\": \"Chart Generator\", \"call_tool\": \"call_tool\", \"end\": END}</code></li> <li><code>Chart Generator</code> -&gt; <code>{\"continue\": \"Researcher\", \"call_tool\": \"call_tool\", \"end\": END}</code></li> </ol> </li> </ol>"},{"location":"langchain/langgraph/#agent-supervisor","title":"Agent Supervisor","text":"<p>Example:</p> <p></p> <pre><code>poetry run python src/examples/langgraph_agent_supervisor.py\n{'supervisor': {'next': 'Coder'}}\n----\nPython REPL can execute arbitrary code. Use with caution.\n{'Coder': {'messages': [HumanMessage(content='The code `print(\\'Hello, World!\\')` has been executed, and it printed \"Hello, World!\" to the terminal.', name='Coder')]}}\n----\n{'supervisor': {'next': 'FINISH'}}\n----\n</code></pre> <ol> <li>Coder: https://app.tavily.com/home</li> </ol> <p>Notes</p> <p>Let LLM to decide which agent to use.</p> <p>We consider it as 'agent as a tool'. This example is pretty similar to normal Agent with Tools.</p> <ul> <li>Supervisor agent &lt;-&gt; Agent</li> <li>Member agents &lt;-&gt; Tools</li> </ul> langgraph_sample.py<pre><code># https://github.com/langchain-ai/langgraph/blob/main/examples/multi_agent/agent_supervisor.ipynb\n\n\n# For this example, you will make an agent to do web research with a search engine, and one agent to create plots. Define the tools they'll use below:\n\nimport functools\nimport operator\nfrom typing import Annotated, Sequence, TypedDict\n\nfrom langchain.agents import AgentExecutor, create_openai_tools_agent\nfrom langchain_community.tools.tavily_search import TavilySearchResults\nfrom langchain_core.messages import BaseMessage, HumanMessage\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_experimental.tools import PythonREPLTool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, StateGraph\nfrom pydantic import BaseModel\nfrom langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n\n\nclass MemberAgentConfig(BaseModel):\n    name: str\n    tools: list\n    system_prompt: str\n\n\ndef create_member_agents(llm, agent_configs: list[MemberAgentConfig]):\n    \"\"\"Use this function to generate a dictionary of member agents.\n    If you have AgentExecutor objects, you can use them directly.\n    \"\"\"\n    agents = {}\n    for config in agent_configs:\n        agent = create_agent(llm, config.tools, config.system_prompt)\n        agents[config.name] = agent\n    return agents\n\n\ndef create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n    \"\"\"Define a helper function below, which make it easier to add new agent worker nodes.\"\"\"\n    # Each worker node will be given a name and some tools.\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\n                \"system\",\n                system_prompt,\n            ),\n            MessagesPlaceholder(variable_name=\"messages\"),\n            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n        ]\n    )\n    agent = create_openai_tools_agent(llm, tools, prompt)\n    executor = AgentExecutor(agent=agent, tools=tools)\n    return executor\n\n\ndef agent_node(state, agent, name):\n    \"\"\"a function that we will use to be the nodes in the graph\n    - it takes care of converting the agent response to a human message.\n    This is important because that is how we will add it the global state of the graph\"\"\"\n    result = agent.invoke(state)\n    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}\n\n\nDEFAULT_SUPERVISOR_SYSTEM_PROMPT = \"\"\"You are a supervisor tasked with managing a conversation between the\n following workers:  {members}. Given the following user request,\n respond with the worker to act next. Each worker will perform a\n task and respond with their results and status. When finished,\n respond with FINISH.\"\"\"\n\n\ndef construct_supervisor(llm, agent_member_names: list[str], system_prompt: str = DEFAULT_SUPERVISOR_SYSTEM_PROMPT):\n    # Our team supervisor is an LLM node. It just picks the next agent to process\n    # and decides when the work is completed\n    options = [\"FINISH\"] + agent_member_names\n    # Using openai function calling can make output parsing easier for us\n    function_def = {\n        \"name\": \"route\",\n        \"description\": \"Select the next role.\",\n        \"parameters\": {\n            \"title\": \"routeSchema\",\n            \"type\": \"object\",\n            \"properties\": {\n                \"next\": {\n                    \"title\": \"Next\",\n                    \"anyOf\": [\n                        {\"enum\": options},\n                    ],\n                }\n            },\n            \"required\": [\"next\"],\n        },\n    }\n    prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", system_prompt),\n            MessagesPlaceholder(variable_name=\"messages\"),\n            (\n                \"system\",\n                \"Given the conversation above, who should act next?\" \" Or should we FINISH? Select one of: {options}\",\n            ),\n        ]\n    ).partial(options=str(options), members=\", \".join(agent_member_names))\n\n    supervisor_chain = prompt | llm.bind_functions(functions=[function_def], function_call=\"route\") | JsonOutputFunctionsParser()\n    return supervisor_chain\n\n\n# The agent state is the input to each node in the graph\nclass AgentState(TypedDict):\n    # The annotation tells the graph that new messages will always\n    # be added to the current states\n    messages: Annotated[Sequence[BaseMessage], operator.add]\n    # The 'next' field indicates where to route to next\n    next: str\n\n\ndef construct_graph(supervisor_chain, agents: dict[str, AgentExecutor]):\n    workflow = StateGraph(AgentState)\n\n    # supervisor\n    workflow.add_node(\"supervisor\", supervisor_chain)\n\n    # agent members\n    nodes = {}\n    for name, agent in agents.items():\n        node = functools.partial(agent_node, agent=agent, name=name)\n        nodes[name] = node\n\n    # add edges (need to add edges after adding nodes)\n    for name in agents.keys():\n        workflow.add_node(name, nodes[name])\n        # We want our workers to ALWAYS \"report back\" to the supervisor when done\n        workflow.add_edge(name, \"supervisor\")\n\n    # The supervisor populates the \"next\" field in the graph state\n    # which routes to a node or finishes\n    conditional_map = {name: name for name in agents.keys()}\n    conditional_map[\"FINISH\"] = END\n    workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n    # Finally, add entrypoint\n    workflow.set_entry_point(\"supervisor\")\n\n    graph = workflow.compile()\n\n    return graph\n\n\nif __name__ == \"__main__\":\n    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n\n    AGENT_MEMBERS = [\n        MemberAgentConfig(\n            name=\"Researcher\",\n            tools=[TavilySearchResults(max_results=5)],\n            system_prompt=\"You are a web researcher.\",\n        ),\n        MemberAgentConfig(\n            name=\"Coder\",\n            tools=[PythonREPLTool()],\n            system_prompt=\"You may generate safe python code to analyze data and generate charts using matplotlib.\",\n        ),\n    ]\n\n    supervisor_chain = construct_supervisor(llm, [m.name for m in AGENT_MEMBERS])\n    agents = create_member_agents(llm, AGENT_MEMBERS)\n    graph = construct_graph(supervisor_chain, agents)\n\n    questions = [\n        # \"Code hello world and print it to the terminal\",\n        \"\u30d0\u30a4\u30c7\u30f3\u5927\u7d71\u9818\u306e\u5e74\u9f62\u306f?\",\n        # \"Python\u3067\u30c7\u30fc\u30bf\u3092\u5206\u6790\u3057\u3001matplotlib\u3067\u30d7\u30ed\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\",\n    ]\n    for q in questions:\n        for s in graph.stream({\"messages\": [HumanMessage(content=q)]}):\n            if \"__end__\" not in s:\n                print(s)\n                print(\"----\")\n        print(\"finished!\")\n</code></pre>"},{"location":"langchain/langgraph/#hierarchical-agent-teams","title":"Hierarchical Agent Teams","text":"<p>Notes</p> <p>Advanced version of Agent supuervisor with multiple layers of group of agents.</p> <ul> <li>Supervisor</li> <li>Teams</li> <li>Agents</li> </ul> <p>Inspired by https://github.com/microsoft/autogen</p> <p>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation</p>"},{"location":"langchain/langgraph/#sample-codes","title":"Sample Codes","text":"<pre><code>poetry run python src/examples/langgraph_sample.py\n</code></pre> langgraph_sample.py<pre><code>import operator\nfrom typing import Annotated, Sequence, TypedDict\n\nfrom langchain_core.messages import BaseMessage, HumanMessage\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.prebuilt import ToolNode\nfrom src.libs.tools import TOOL_GOOGLE\n\n\ndef create_example(llm=ChatOpenAI(temperature=0, streaming=True)):\n    # 1. Set up tools\n    tools = [\n        TOOL_GOOGLE,\n    ]\n    tool_node = ToolNode(tools=tools)\n\n    # 3. Bind the tools to the model\n    llm = llm.bind_tools(tools)\n\n    # 4. Define AgentState\n    class AgentState(TypedDict):\n        messages: Annotated[Sequence[BaseMessage], operator.add]\n\n    # 5. Define nodes\n    # node: function or runnable (e.g. agent, tool)\n\n    # Define the function that determines whether to continue or not\n    def should_continue(state):\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n        if last_message.tool_calls:\n            return \"continue\"\n        else:\n            return \"end\"\n\n    # Define the function that calls the model\n    def call_model(state):\n        messages = state[\"messages\"]\n        response = llm.invoke(messages)\n        # We return a list, because this will get added to the existing list\n        return {\"messages\": [response]}\n\n    # 6. Define the graph\n\n    # Define a new graph\n    workflow = StateGraph(AgentState)\n\n    # Define the two nodes we will cycle between\n    workflow.add_node(\"agent\", call_model)\n    workflow.add_node(\"action\", tool_node)\n\n    # Set the entrypoint as `agent`\n    # This means that this node is the first one called\n    workflow.set_entry_point(\"agent\")\n\n    # We now add a conditional edge\n    workflow.add_conditional_edges(\n        # First, we define the start node. We use `agent`.\n        # This means these are the edges taken after the `agent` node is called.\n        \"agent\",\n        # Next, we pass in the function that will determine which node is called next.\n        should_continue,\n        # Finally we pass in a mapping.\n        # The keys are strings, and the values are other nodes.\n        # END is a special node marking that the graph should finish.\n        # What will happen is we will call `should_continue`, and then the output of that\n        # will be matched against the keys in this mapping.\n        # Based on which one it matches, that node will then be called.\n        {\n            # If `tools`, then we call the tool node.\n            \"continue\": \"action\",\n            # Otherwise we finish.\n            \"end\": END,\n        },\n    )\n\n    # We now add a normal edge from `tools` to `agent`.\n    # This means that after `tools` is called, `agent` node is called next.\n    workflow.add_edge(\"action\", \"agent\")\n\n    # Finally, we compile it!\n    # This compiles it into a LangChain Runnable,\n    # meaning you can use it as you would any other runnable\n    app = workflow.compile()\n    return app\n\n\nif __name__ == \"__main__\":\n    app = create_example()\n    # 7. Use\n    inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]}\n    print(app.invoke(inputs))\n</code></pre>"},{"location":"langchain/langgraph/#faq","title":"FAQ","text":"<ol> <li>When to use <code>LangGraph</code>?</li> <li>How to determine <code>AgentState</code>?</li> <li>llm.bind_functions\u3068\u306f?</li> <li> <p>LangGraph \u3068StreamlitCallbackHandler\u304c incompatible? https://github.com/langchain-ai/langgraph/issues/101</p> <pre><code>2024-03-02 20:48:03.039 Thread 'ThreadPoolExecutor-54_0': missing ScriptRunContext\nError in StreamlitCallbackHandler.on_llm_start callback: NoSessionContext()\nError in StreamlitCallbackHandler.on_llm_new_token callback: RuntimeError('Current LLMThought is unexpectedly None!')\n...\nError in StreamlitCallbackHandler.on_llm_new_token callback: RuntimeError('Current LLMThought is unexpectedly None!')\nError in StreamlitCallbackHandler.on_llm_new_token callback: RuntimeError('Current LLMThought is unexpectedly None!')\nError in StreamlitCallbackHandler.on_llm_new_token callback: RuntimeError('Current LLMThought is unexpectedly None!')\nError in StreamlitCallbackHandler.on_llm_end callback: RuntimeError('Current LLMThought is unexpectedly None!')\n2024-03-02 20:48:03.659 Thread 'ThreadPoolExecutor-54_0': missing ScriptRunContext\nError in StreamlitCallbackHandler.on_llm_start callback: NoSessionContext()\nError in StreamlitCallbackHandler.on_llm_end callback: RuntimeError('Current LLMThought is unexpectedly None!')\n</code></pre> </li> </ol>"},{"location":"langchain/langgraph/#ref","title":"Ref","text":"<ol> <li>2024-01-17 LangGraph<ol> <li>human in the loop</li> </ol> </li> <li>2024-01-23 LangGraph: Multi-Agent Workflows</li> <li>2024-02-21 Reflection Agents</li> <li>2024-03-19 LangGraph \u3092\u7528\u3044\u305f LLM \u30a8\u30fc\u30b8\u30a7\u30f3\u30c8\u3001Plan-and-Execute Agents \u306e\u5b9f\u88c5\u89e3\u8aac</li> </ol>"},{"location":"langchain/langgraph/test/","title":"Write Unit Test for LangGraph","text":""},{"location":"langchain/langgraph/test/#langgraph-code","title":"LangGraph code","text":"<p>https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/</p> <pre><code>from typing import List\n\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langgraph.graph import END, START, StateGraph\nfrom pydantic import BaseModel, Field\nfrom typing_extensions import TypedDict\n\n\nretrieve_grader_system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n\ngenerate_prompt_template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: {question}\nContext: {context}\nAnswer:\"\"\"  # noqa E501\n\n\nhallucination_system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\nGive a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"  # noqa E501\n\nanswer_grader_system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\nGive a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n\n\nquestion_rewriter_system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\nfor vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"  # noqa E501\n\n\nclass GradeDocuments(BaseModel):\n    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n\n    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n\n\nclass GradeHallucinations(BaseModel):\n    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n\n    binary_score: str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n\n\nclass GradeAnswer(BaseModel):\n    \"\"\"Binary score to assess answer addresses question.\"\"\"\n\n    binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n\n\nclass GraphState(TypedDict):\n    \"\"\"\n    Represents the state of our graph.\n\n    Attributes:\n        question: question\n        generation: LLM generation\n        documents: list of documents\n    \"\"\"\n\n    question: str\n    generation: str\n    documents: List[str]\n\n\ndef create_graph(chat, retriever):\n    # Retrieval Grader\n    structured_llm_grader = chat.with_structured_output(GradeDocuments)\n\n    grade_prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", retrieve_grader_system),\n            (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n        ]\n    )\n\n    retrieval_grader = grade_prompt | structured_llm_grader\n\n    # Generate\n    generate_prompt = ChatPromptTemplate.from_template(generate_prompt_template)\n    rag_chain = generate_prompt | chat | StrOutputParser()\n\n    # Hallucination Grader\n    structured_llm_grader = chat.with_structured_output(GradeHallucinations)\n    hallucination_prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", hallucination_system),\n            (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n        ]\n    )\n    hallucination_grader = hallucination_prompt | structured_llm_grader\n\n    # Answer Grader\n    structured_llm_grader = chat.with_structured_output(GradeAnswer)\n    answer_prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", answer_grader_system),\n            (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n        ]\n    )\n\n    answer_grader = answer_prompt | structured_llm_grader\n\n    # Question Re-writer\n    re_write_prompt = ChatPromptTemplate.from_messages(\n        [\n            (\"system\", question_rewriter_system),\n            (\n                \"human\",\n                \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n            ),\n        ]\n    )\n\n    question_rewriter = re_write_prompt | chat | StrOutputParser()\n\n    def retrieve(state):\n        \"\"\"\n        Retrieve documents\n\n        Args:\n            state (dict): The current graph state\n\n        Returns:\n            state (dict): New key added to state, documents, that contains retrieved documents\n        \"\"\"\n        print(\"---RETRIEVE---\")\n        question = state[\"question\"]\n\n        # Retrieval\n        documents = retriever.invoke(question)\n        print(f\"{len(documents)} documents retrieved\")\n        return {\"documents\": documents, \"question\": question}\n\n    def generate(state):\n        \"\"\"\n        Generate answer\n\n        Args:\n            state (dict): The current graph state\n\n        Returns:\n            state (dict): New key added to state, generation, that contains LLM generation\n        \"\"\"\n        print(\"---GENERATE---\")\n        question = state[\"question\"]\n        documents = state[\"documents\"]\n\n        # RAG generation\n        generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n        return {\"documents\": documents, \"question\": question, \"generation\": generation}\n\n    def grade_documents(state):\n        \"\"\"\n        Determines whether the retrieved documents are relevant to the question.\n\n        Args:\n            state (dict): The current graph state\n\n        Returns:\n            state (dict): Updates documents key with only filtered relevant documents\n        \"\"\"\n\n        print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n        question = state[\"question\"]\n        documents = state[\"documents\"]\n\n        # Score each doc\n        filtered_docs = []\n        for d in documents:\n            score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n            grade = score.binary_score\n            if grade == \"yes\":\n                print(\"---GRADE: DOCUMENT RELEVANT---\")\n                filtered_docs.append(d)\n            else:\n                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n        print(f\"filtered documents ({len(documents)} -&gt; {len(filtered_docs)})\")\n        return {\"documents\": filtered_docs, \"question\": question}\n\n    def transform_query(state):\n        \"\"\"\n        Transform the query to produce a better question.\n\n        Args:\n            state (dict): The current graph state\n\n        Returns:\n            state (dict): Updates question key with a re-phrased question\n        \"\"\"\n\n        print(\"---TRANSFORM QUERY---\")\n        question = state[\"question\"]\n        documents = state[\"documents\"]\n\n        # Re-write question\n        better_question = question_rewriter.invoke({\"question\": question})\n        return {\"documents\": documents, \"question\": better_question}\n\n    ### Edges ###\n\n    def decide_to_generate(state):\n        \"\"\"\n        Determines whether to generate an answer, or re-generate a question.\n\n        Args:\n            state (dict): The current graph state\n\n        Returns:\n            str: Binary decision for next node to call\n        \"\"\"\n\n        print(\"---ASSESS GRADED DOCUMENTS---\")\n        state[\"question\"]\n        filtered_documents = state[\"documents\"]\n\n        if not filtered_documents:\n            # All documents have been filtered check_relevance\n            # We will re-generate a new query\n            print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\")\n            return \"transform_query\"\n        else:\n            # We have relevant documents, so generate answer\n            print(\"---DECISION: GENERATE---\")\n            return \"generate\"\n\n    def grade_generation_v_documents_and_question(state):\n        \"\"\"\n        Determines whether the generation is grounded in the document and answers question.\n\n        Args:\n            state (dict): The current graph state\n\n        Returns:\n            str: Decision for next node to call\n        \"\"\"\n\n        print(\"---CHECK HALLUCINATIONS---\")\n        question = state[\"question\"]\n        documents = state[\"documents\"]\n        generation = state[\"generation\"]\n\n        score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n        grade = score.binary_score\n\n        # Check hallucination\n        if grade == \"yes\":\n            print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n            # Check question-answering\n            print(\"---GRADE GENERATION vs QUESTION---\")\n            score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n            grade = score.binary_score\n            if grade == \"yes\":\n                print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n                return \"useful\"\n            else:\n                print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n                return \"not useful\"\n        else:\n            print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n            return \"not supported\"\n\n    workflow = StateGraph(GraphState)\n\n    # Define the nodes\n    workflow.add_node(\"retrieve\", retrieve)  # retrieve\n    workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n    workflow.add_node(\"generate\", generate)  # generatae\n    workflow.add_node(\"transform_query\", transform_query)  # transform_query\n\n    # Build graph\n    workflow.add_edge(START, \"retrieve\")\n    workflow.add_edge(\"retrieve\", \"grade_documents\")\n    workflow.add_conditional_edges(\n        \"grade_documents\",\n        decide_to_generate,\n        {\n            \"transform_query\": \"transform_query\",\n            \"generate\": \"generate\",\n        },\n    )\n    workflow.add_edge(\"transform_query\", \"retrieve\")\n    workflow.add_conditional_edges(\n        \"generate\",\n        grade_generation_v_documents_and_question,\n        {\n            \"not supported\": \"generate\",\n            \"useful\": END,\n            \"not useful\": \"transform_query\",\n        },\n    )\n\n    # Compile\n    app = workflow.compile()\n\n    return app\n</code></pre>"},{"location":"langchain/langgraph/test/#write-unit-test-for-the-code","title":"Write Unit Test for the code","text":"<pre><code>from unittest.mock import MagicMock\nfrom langchain.schema import Document\nfrom langchain_core.messages import AIMessage\n\nfrom src.examples.langgraph_self_reflection import create_graph, GradeDocuments, GradeHallucinations, GradeAnswer\n\n\ndef test_graph():\n    retriever = MagicMock()\n    retriever.invoke.side_effect = [\n        [\n            Document(metadata={\"title\": \"Page 1\", \"source\": \"http://example.com/page1\"}, page_content=\"\"),\n            Document(metadata={\"title\": \"Page 2\", \"source\": \"http://example.com/page2\"}, page_content=\"\"),\n        ],  # retrieve\n    ]\n\n    chat_mock = MagicMock()\n    chat_mock.side_effect = [\n        AIMessage(\n            content=\"\"\"\u4ee5\u4e0b\u306e\u30da\u30fc\u30b8\u3092\u898b\u3064\u3051\u307e\u3057\u305f:\n1. &lt;http://example.com/page1|Page 1&gt;\n2. &lt;http://example.com/page2|Page 2&gt;\"\"\"\n        ),  # rag_chain generate\n    ]\n\n    chat_mock.with_structured_output.return_value.side_effect = [\n        GradeDocuments(binary_score=\"yes\"),  # retrieval_grader for the first document\n        GradeDocuments(binary_score=\"no\"),  # retrieval_grader for the second document\n        GradeHallucinations(binary_score=\"yes\"),  # hallucination_grader\n        GradeAnswer(binary_score=\"yes\"),  # answer_grader\n    ]\n\n    graph = create_graph(chat=chat_mock, retriever=retriever)\n\n    res = graph.invoke({\"question\": \"What's the origin of the name 'Sofia'?\"})\n\n    # Verify the expected behavior\n    expected_output = \"\"\"\u4ee5\u4e0b\u306e\u30da\u30fc\u30b8\u3092\u898b\u3064\u3051\u307e\u3057\u305f:\n1. &lt;http://example.com/page1|Page 1&gt;\n2. &lt;http://example.com/page2|Page 2&gt;\"\"\"\n\n    assert res == {\n        \"question\": \"What's the origin of the name 'Sofia'?\",\n        \"generation\": expected_output,\n        \"documents\": [\n            Document(metadata={\"title\": \"Page 1\", \"source\": \"http://example.com/page1\"}, page_content=\"\"),\n        ],  # the second document is filtered out by the retrieval_grader\n    }\n</code></pre>"},{"location":"langchain/lcel/","title":"LCEL (LangChain Expression Language)","text":""},{"location":"langchain/lcel/#why-use-lcel","title":"Why use LCEL?","text":"<p>LCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.</p> <p>But it seems that not all cases are suitable for LCEL.</p>"},{"location":"langchain/lcel/#basics","title":"Basics","text":"<p>The syntax of LCEL is implemented by <code>__or__</code> and <code>__call__</code> methods.</p> <pre><code>class Runnable:\n    def __init__(self, func):\n        self.func = func\n\n    def __or__(self, other):\n        def chained_func(*args, **kwargs):\n            # the other func consumes the result of this func\n            return other(self.func(*args, **kwargs))\n\n        return Runnable(chained_func)\n\n    def __call__(self, *args, **kwargs):\n        return self.func(*args, **kwargs)\n\n\ndef add_five(x):\n    return x + 5\n\n\ndef multiply_by_two(x):\n    return x * 2\n\n\n# wrap the functions with Runnable\nadd_five = Runnable(add_five)\nmultiply_by_two = Runnable(multiply_by_two)\n\n# run them using the object approach\nchain = add_five | multiply_by_two\nprint(chain(3))\n</code></pre> <p>Ref: https://www.pinecone.io/learn/series/langchain/langchain-expression-language/</p>"},{"location":"langchain/lcel/#example","title":"Example","text":"<pre><code>chain1 = prompt1 | model | StrOutputParser()\nchain2 = prompt2 | model | StrOutputParser()\n</code></pre> <ol> <li><code>Generate an argument about: {input}</code> -&gt; base_response     <pre><code>planner = (\n    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n    | ChatOpenAI()\n    | StrOutputParser()\n    | {\"base_response\": RunnablePassthrough()}\n)\n</code></pre></li> <li><code>List the pros or positive aspects of {base_response}</code> <pre><code>arguments_for = (\n    ChatPromptTemplate.from_template(\n        \"List the pros or positive aspects of {base_response}\"\n    )\n    | ChatOpenAI()\n    | StrOutputParser()\n)\n</code></pre></li> <li><code>List the cons or negative aspects of {base_response}</code> <pre><code>arguments_against = (\n    ChatPromptTemplate.from_template(\n        \"List the cons or negative aspects of {base_response}\"\n    )\n    | ChatOpenAI()\n    | StrOutputParser()\n)\n</code></pre></li> <li> <p><code>Generate a final response given the critique</code></p> <p><pre><code>final_responder = (\n    ChatPromptTemplate.from_messages(\n        [\n            (\"ai\", \"{original_response}\"),\n            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n            (\"system\", \"Generate a final response given the critique\"),\n        ]\n    )\n    | ChatOpenAI()\n    | StrOutputParser()\n)\n</code></pre> 1. Combine</p> <pre><code>chain = (\n    planner\n    | {\n        \"results_1\": arguments_for,\n        \"results_2\": arguments_against,\n        \"original_response\": itemgetter(\"base_response\"),\n    }\n    | final_responder\n)\n</code></pre> </li> <li> <p>Run</p> <pre><code>chain.invoke({\"input\": \"scrum\"})\n</code></pre> </li> </ol>"},{"location":"langchain/lcel/#implementation","title":"Implementation","text":""},{"location":"langchain/lcel/#runnable","title":"Runnable","text":"<ul> <li>invoke/ainvoke: Transforms a single input into an output.</li> <li>batch/abatch: Efficiently transforms multiple inputs into outputs.</li> <li>stream/astream: Streams output from a single input as it's produced.</li> <li>astream_log: Streams output and selected intermediate results from an input.</li> </ul> <pre><code>class Runnable(Generic[Input, Output], ABC):\n</code></pre> <pre><code>    def bind(self, **kwargs: Any) -&gt; Runnable[Input, Output]:\n        \"\"\"\n        Bind arguments to a Runnable, returning a new Runnable.\n        \"\"\"\n        return RunnableBinding(bound=self, kwargs=kwargs, config={})\n</code></pre>"},{"location":"langchain/lcel/#runnablebinding","title":"RunnableBinding","text":"<p>example:</p> <pre><code># Create a runnable binding that invokes the ChatModel with the\n# additional kwarg `stop=['-']` when running it.\nfrom langchain_community.chat_models import ChatOpenAI\nmodel = ChatOpenAI()\nmodel.invoke('Say \"Parrot-MAGIC\"', stop=['-']) # Should return `Parrot`\n# Using it the easy way via `bind` method which returns a new\n# RunnableBinding\nrunnable_binding = model.bind(stop=['-'])\nrunnable_binding.invoke('Say \"Parrot-MAGIC\"') # Should return `Parrot`\n</code></pre>"},{"location":"langchain/lcel/#idea","title":"Idea","text":"<p>\u4eba\u9593\u306e\u60c5\u5831\u691c\u7d22\u306e\u7121\u610f\u8b58\u306a\u601d\u8003\u3092\u53cd\u6620\u3067\u304d\u308b\u306e\u3067\u306f\u306a\u3044\u304b\u3002</p> <p>\u4f8b. 1. \u3069\u3046\u3084\u3063\u3066\u8a2d\u5b9a\u3059\u308b\u3093\u3060\u308d\u3046 1. Document\u306f\u3069\u3053\u3060\uff1f\u2192\u4eba\u306b\u805e\u304f\u304b\u3082 1. Document\u3092\u8aad\u3080 -&gt; \u3042\u308b\u7a0b\u5ea6\u3067\u304d\u308b 1. \u554f\u984c\u306b\u76f4\u9762\u3059\u308b -&gt; \u4eba\u306b\u805e\u304f</p> <p>\u4f8b. 1. \u4eba\u306b\u805e\u304f 1. \u56de\u7b54\u3092\u3082\u3089\u3046 1. \u81ea\u5206\u306e\u7406\u89e3\u3057\u3066\u3044\u306a\u3044\u3068\u3053\u308d\u3092\u3055\u3089\u306b\u805e\u304f 1. \u76f8\u624b\u304b\u3089\u56de\u7b54\u304c\u5f97\u3089\u308c\u308b 1. \u554f\u984c\u304c\u3042\u308b\u3053\u3068\u306b\u6c17\u3065\u304f 1. \u30c1\u30b1\u30c3\u30c8\u306b\u3059\u308b</p> <p>\u81ea\u5206\u306e\u4f5c\u696d\u3000\u2194 \u4eba\u3068\u306e\u3084\u308a\u3068\u308a</p>"},{"location":"langchain/lcel/#ref","title":"Ref","text":"<ol> <li>https://medium.com/@twjjosiah/chain-loops-in-langchain-expression-language-lcel-a38894db0cee</li> </ol>"},{"location":"langchain/lcel/examples/","title":"LCEL examples","text":""},{"location":"langchain/lcel/examples/#runnablebranch","title":"RunnableBranch","text":"<p>You can use <code>RunnableBranch</code> to create a conditional chain.</p> <ul> <li>if <code>message</code> is not empty, then run <code>message_summary_chain</code></li> <li>if <code>message</code> is empty, then return <code>\"no message found\"</code></li> </ul> <pre><code>def example_runnable_branch():\n    # branch\n    extract_message_chain = RunnableLambda(lambda x: {\"message\": [\"message\"] * randint(0, 2)})  # randomize message\n    message_summary_chain = RunnableLambda(lambda x: f\"this is the summary of {len(x['message'])} messages\")  # emulate chain with LLM\n\n    # chain\n    chain_with_branch = extract_message_chain | RunnableBranch(\n        (lambda x: bool(x[\"message\"]), message_summary_chain),\n        lambda x: \"no message found\",  # fixed result when empty\n    )\n    print(chain_with_branch.batch([{}, {}, {}]))\n</code></pre>"},{"location":"langchain/lcel/examples/#more-examples","title":"More examples","text":"<pre><code>from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda, RunnableBranch\nimport pandas as pd\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom operator import itemgetter\nfrom random import randint\n\ndf = pd.read_csv(\"data/titanic.csv\")\nprint(df)\n\n\ndef get_survived(df):\n    return df[df[\"Survived\"] == 1]\n\n\ndef simple_example():\n    # retrieval = RunnableParallel({\"df\": df, \"question\": RunnablePassthrough()})\n    # df -&gt; user_id \u3092\u6e21\u3057\u3066\u3057\u307c\u308b\n    # input = RunnablePassthrough()\n    model = ChatOpenAI()\n\n    print(get_survived(df))\n    prompt = ChatPromptTemplate.from_template(\"df {df} \u306f\u4f55\u884c\u3042\u308a\u307e\u3059\u304b\")\n    chain = {\"df\": itemgetter(\"df\") | RunnableLambda(get_survived)} | prompt | model\n\n    print(chain.invoke({\"df\": df}))\n\n\ndef example_runnable_parallel():\n    # mapper\n    runnable = RunnableParallel(\n        passed=RunnablePassthrough(),  # \u305d\u306e\u307e\u307e\u6e21\u3059 {\"df\": df}\n        extra=RunnablePassthrough.assign(survived=lambda x: get_survived(x[\"df\"]).shape[0]),  # {'extra': {\"df\": df, \"survived\": 342}}\n        count=lambda x: {\"survived\": get_survived(x[\"df\"]).shape[0], \"total\": x[\"df\"].shape[0]},  # {'count': {'survived': 342, 'total': 891}}\n    )\n    print(runnable.invoke({\"df\": df}))\n\n\ndef example_runnable_lambda():\n    TEXT_KEY = \"text\"\n\n    def summarize(x):\n        return x[TEXT_KEY].replace(\"text\", \"summary\")\n\n    texts = [{\"id\": 1, TEXT_KEY: \"text1\"}, {\"id\": 2, TEXT_KEY: \"text2\"}, {\"id\": 3, TEXT_KEY: \"text3\"}]\n\n    summarizer = RunnableParallel(\n        id=RunnableLambda(lambda x: x[\"id\"]),  # id\n        summary=RunnableLambda(summarize),  # summary\n    )\n\n    summaries = summarizer.batch(texts)\n    print(summaries)\n\n    prompt = ChatPromptTemplate.from_template(\"\u4ee5\u4e0b\u306e\u6587\u7ae0\u306eSummary\u30923\u884c\u3067\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044:\\n{context}\")\n    llm = ChatOpenAI()\n    final_summarizer = prompt | llm\n    res = final_summarizer.invoke({\"context\": \"\\n\".join([f\"- {s['id']}: {s['summary']}\" for s in summaries])})\n    print(res)\n\n\ndef example_runnable_assign():\n    # assign\n\n    create_sql_query = RunnableLambda(lambda x: f\"SELECT * FROM {x['table']} WHERE {x['column']} = {x['value']}\")\n    create_graph = RunnableLambda(lambda x: f\"[{x['title']}] graph based on {x['sql']} and original data ({x['graph_table']} and {x['column']})\")\n\n    runnable = RunnablePassthrough.assign(sql=create_sql_query, graph_table=lambda x: x[\"table\"]) | create_graph\n    print(runnable.invoke({\"table\": \"titanic\", \"column\": \"Survived\", \"value\": 1, \"title\": \"Graph Title\"}))\n\n\ndef example_runnable_branch():\n    # branch\n    extract_message_chain = RunnableLambda(lambda x: {\"message\": [\"message\"] * randint(0, 2)})  # randomize message\n    message_summary_chain = RunnableLambda(lambda x: f\"this is the summary of {len(x['message'])} messages\")  # emulate chain with LLM\n\n    # chain\n    chain_with_branch = extract_message_chain | RunnableBranch(\n        (lambda x: bool(x[\"message\"]), message_summary_chain),\n        lambda x: \"no message found\",  # fixed result when empty\n    )\n    print(chain_with_branch.batch([{}, {}, {}]))\n\n\nexample_runnable_branch()\n</code></pre>"},{"location":"streamlit/","title":"streamlit","text":""},{"location":"streamlit/#basic-chat-app-with-memory","title":"basic chat app with memory","text":"<pre><code># https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history\n\nfrom langchain_community.chat_message_histories import (\n    StreamlitChatMessageHistory,\n)\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain_core.runnables.history import RunnableWithMessageHistory\nfrom langchain_openai import ChatOpenAI\nimport streamlit as st\n\n\nmsgs = StreamlitChatMessageHistory(key=\"langchain_messages\")\n\nif len(msgs.messages) == 0:\n    msgs.add_ai_message(\"How can I help you?\")\n\nprompt = ChatPromptTemplate.from_messages(\n    [\n        (\"system\", \"You are an AI chatbot having a conversation with a human.\"),\n        MessagesPlaceholder(variable_name=\"history\"),\n        (\"human\", \"{question}\"),\n    ]\n)\n\nchain = prompt | ChatOpenAI()\nchain_with_history = RunnableWithMessageHistory(\n    chain,\n    lambda session_id: msgs,  # Always return the instance created earlier\n    input_messages_key=\"question\",\n    history_messages_key=\"history\",\n)\n\nfor msg in msgs.messages:\n    st.chat_message(msg.type).write(msg.content)\n\nif prompt := st.chat_input():\n    st.chat_message(\"human\").write(prompt)\n\n    # As usual, new messages are added to StreamlitChatMessageHistory when the Chain is called.\n    config = {\"configurable\": {\"session_id\": \"any\"}}\n    response = chain_with_history.invoke({\"question\": prompt}, config)\n    st.chat_message(\"ai\").write(response.content)\n</code></pre> <pre><code>poetry run streamlit run src/projects/apps/streamlit/chat_with_history.py\n</code></pre> <p>Note</p> <p><code>langchain_community</code> had a bug that was fixed in this PR. I confirmed it works with the latest version of <code>langchain_community</code> (0.0.28).</p> <ul> <li>https://python.langchain.com/docs/integrations/memory/streamlit_chat_message_history</li> <li>https://github.com/streamlit/streamlit/issues/7290</li> </ul>"},{"location":"streamlit/#ref","title":"Ref","text":"<ol> <li>\u793e\u54e11000\u4eba\u4ee5\u4e0a\u304c\u4f7f\u3046\u3001Streamlit in Google Cloud\u306e\u30b5\u30fc\u30d0\u30ec\u30b9\u30d7\u30e9\u30c3\u30c8\u30d5\u30a9\u30fc\u30e0\u3092\u5b8c\u5168\u5185\u88fd\u3057\u3066\u307f\u305f</li> </ol>"}]}